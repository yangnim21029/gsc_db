#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¿«é€Ÿ Sitemap å†—é¤˜åˆ†æå·¥å…·

å°ˆç‚ºå¿«é€Ÿã€å®Œæ•´åœ°åˆ†æ sitemap URL å†—é¤˜æƒ…æ³è€Œè¨­è¨ˆï¼Œå„ªåŒ–äº†è³‡æ–™åº«æŸ¥è©¢å’Œçµæœå‘ˆç¾ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ™ºèƒ½ Sitemap ç™¼ç¾ï¼ˆå¯é¸ï¼Œæˆ–è‡ªå‹•æª¢æ¸¬ï¼‰ã€‚
2. é«˜æ•ˆç²å– Sitemap ä¸­çš„æ‰€æœ‰ URLï¼ˆæ”¯æŒ sitemap ç´¢å¼•ï¼Œä¸¦ç™¼è™•ç†ï¼‰ã€‚
3. ä¸€æ¬¡æ€§æŸ¥è©¢è³‡æ–™åº«ä¸­æ‰€æœ‰ç›¸é—œé é¢ï¼Œé¿å…å¤šæ¬¡æŸ¥è©¢ã€‚
4. æ¸…æ™°çš„æ•¸æ“šè¦†è“‹æƒ…æ³å’Œå†—é¤˜åˆ†æå ±å‘Šã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
# è‡ªå‹•ç™¼ç¾ sitemap ä¸¦é€²è¡Œåˆ†æï¼ˆé è¨­è¼¸å‡º Excel åˆ° data/ è³‡æ–™å¤¾ï¼ŒæŸ¥è©¢å…¨éƒ¨æ™‚é–“ï¼‰
poetry run python scripts/sitemap_redundancy_analyzer.py --site-id 14

# æ‰‹å‹•æŒ‡å®š sitemap URL
poetry run python scripts/sitemap_redundancy_analyzer.py --site-id 14 \
    --sitemap-url "https://example.com/sitemap.xml" --days 30

# æŒ‡å®šè‡ªè¨‚è¼¸å‡ºè·¯å¾‘ï¼ˆExcelæ ¼å¼ï¼‰
poetry run python scripts/sitemap_redundancy_analyzer.py --site-id 14 \
    --output-csv "reports/analysis.xlsx" --days 30

# æŒ‡å®šCSVæ ¼å¼ï¼ˆåƒ…å†—é¤˜URLï¼‰
poetry run python scripts/sitemap_redundancy_analyzer.py --site-id 14 \
    --output-csv "reports/redundant_urls.csv" --days 30
"""

import argparse
import csv
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import quote, urljoin

import pandas as pd
import requests
from lxml import etree
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, track
from rich.table import Table

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„æ·»åŠ åˆ° sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ruff: noqa: E402
from src.containers import Container
from src.services.database import Database

console = Console()

SITEMAP_NS = {"ns": "http://www.sitemaps.org/schemas/sitemap/0.9"}
COMMON_SITEMAP_PATHS = ["/sitemap.xml", "/sitemap_index.xml", "/sitemaps.xml"]
REQUEST_TIMEOUT_SHORT = 5
REQUEST_TIMEOUT_LONG = 30
MAX_WORKERS = 10  # ä¸¦ç™¼ä¸‹è¼‰çš„ç·šç¨‹æ•¸


class SitemapAnalyzer:
    """å¿«é€Ÿå†—é¤˜åˆ†æå™¨"""

    def __init__(self, db: Database):
        self.db = db
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (compatible; SitemapAnalyzer/1.0; +https://github.com/user/repo)"
            }
        )

    def discover_sitemaps(self, domain: str) -> List[str]:
        """å¾ robots.txt å’Œå¸¸è¦‹è·¯å¾‘ç™¼ç¾æ‰€æœ‰æœ‰æ•ˆçš„ sitemaps"""
        if not domain.startswith(("http://", "https://")):
            domain = f"https://{domain}"

        console.print(Panel.fit(f"[bold blue]æ™ºèƒ½ Sitemap ç™¼ç¾[/bold blue]\nç›®æ¨™åŸŸå: {domain}"))

        # 1. å¾ robots.txt ç™¼ç¾
        robots_url = urljoin(domain, "/robots.txt")
        sitemaps = []
        try:
            response = self.session.get(robots_url, timeout=REQUEST_TIMEOUT_SHORT)
            if response.status_code == 200:
                sitemap_pattern = re.compile(r"sitemap:\s*(.+)", re.IGNORECASE)
                sitemaps.extend([match.strip() for match in sitemap_pattern.findall(response.text)])
        except requests.RequestException as e:
            console.log(f"[yellow]ç„¡æ³•è®€å– {robots_url}: {e}[/yellow]")

        # 2. æª¢æŸ¥å¸¸è¦‹è·¯å¾‘
        for path in COMMON_SITEMAP_PATHS:
            sitemaps.append(urljoin(domain, path))

        # 3. é©—è­‰æ‰€æœ‰ sitemaps
        validated_sitemaps = []
        for url in track(set(sitemaps), description="é©—è­‰ Sitemap..."):
            try:
                response = self.session.head(
                    url, timeout=REQUEST_TIMEOUT_SHORT, allow_redirects=True
                )
                if (
                    response.status_code == 200
                    and "xml" in response.headers.get("content-type", "").lower()
                ):
                    validated_sitemaps.append(str(response.url))  # ä½¿ç”¨é‡å®šå‘å¾Œçš„æœ€çµ‚ URL
                    console.print(f"âœ… æ‰¾åˆ°æœ‰æ•ˆ Sitemap: {response.url}")
            except requests.RequestException:
                continue

        if not validated_sitemaps:
            console.print("[bold red]âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ sitemap[/bold red]")
            return []

        console.print(
            f"[bold green]ç™¼ç¾ {len(validated_sitemaps)} å€‹æœ‰æ•ˆçš„ Sitemapsï¼Œå°‡å…¨éƒ¨ä½¿ç”¨[/bold green]"
        )
        return validated_sitemaps

    def discover_and_select_sitemap(self, domain: str, auto_select: bool = True) -> Optional[str]:
        """ä¿æŒå‘å¾Œå…¼å®¹æ€§çš„æ–¹æ³•ï¼Œè¿”å›ç¬¬ä¸€å€‹ç™¼ç¾çš„ sitemap"""
        all_sitemaps = self.discover_sitemaps(domain)
        return all_sitemaps[0] if all_sitemaps else None

    def _fetch_and_parse_single_sitemap(self, url: str) -> List[str]:
        """ç²å–ä¸¦è§£æå–®å€‹ sitemap æˆ– sitemap ç´¢å¼•ï¼Œè¿”å› URL åˆ—è¡¨"""
        try:
            response = self.session.get(url, timeout=REQUEST_TIMEOUT_LONG)
            response.raise_for_status()  # æ‹‹å‡º HTTP éŒ¯èª¤
            root = etree.fromstring(response.content, parser=etree.XMLParser(recover=True))

            # æª¢æŸ¥æ˜¯ sitemap ç´¢å¼•é‚„æ˜¯ url é›†åˆ
            if root.tag.endswith("sitemapindex"):
                xpath_expr = "//ns:sitemap/ns:loc"
            elif root.tag.endswith("urlset"):
                xpath_expr = "//ns:url/ns:loc"
            else:
                # å¯èƒ½æ˜¯æ²’æœ‰ namespace çš„ sitemap
                if root.tag == "sitemapindex":
                    xpath_expr = "//sitemap/loc"
                else:
                    xpath_expr = "//url/loc"

            # æå–æ–‡æœ¬ä¸¦éæ¿¾æ‰ None æˆ–ç©ºå­—ä¸²
            return [
                elem.text for elem in root.xpath(xpath_expr, namespaces=SITEMAP_NS) if elem.text
            ]

        except requests.HTTPError as e:
            console.log(f"[red]HTTP éŒ¯èª¤ {e.response.status_code} æ–¼ {url}[/red]")
        except Exception as e:
            console.log(f"[red]è™•ç† {url} å¤±æ•—: {e}[/red]")
        return []

    def fetch_sitemap_urls(self, sitemap_url: str) -> List[str]:
        """é«˜æ•ˆç²å–ä¸¦è§£æ Sitemapï¼Œæå–æ‰€æœ‰ URLï¼Œæ”¯æŒä¸¦ç™¼è™•ç†ç´¢å¼•"""
        console.print(f"\nğŸ” æ­£åœ¨ç²å– Sitemap: {sitemap_url}")

        initial_urls = self._fetch_and_parse_single_sitemap(sitemap_url)
        if not initial_urls:
            return []

        # åˆ¤æ–·æ˜¯ sitemap ç´¢å¼•é‚„æ˜¯æ™®é€šçš„ sitemap
        if initial_urls[0].endswith(".xml"):
            console.print(
                f"ğŸ“„ æª¢æ¸¬åˆ° Sitemap ç´¢å¼•ï¼ŒåŒ…å« {len(initial_urls)} å€‹å­ sitemapï¼Œé–‹å§‹ä¸¦ç™¼è™•ç†..."
            )
            all_urls = []
            with Progress(console=console) as progress:
                task = progress.add_task("[cyan]è§£æå­ Sitemap...", total=len(initial_urls))
                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    future_to_url = {
                        executor.submit(self._fetch_and_parse_single_sitemap, url): url
                        for url in initial_urls
                    }
                    for future in as_completed(future_to_url):
                        try:
                            urls_from_sub = future.result()
                            all_urls.extend(urls_from_sub)
                        except Exception as e:
                            url = future_to_url[future]
                            console.log(f"[red]ç²å–å­ sitemap {url} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}[/red]")
                        progress.update(task, advance=1)
            urls = all_urls
        else:
            urls = initial_urls

        console.print("\nğŸ“Š [bold]Sitemap URL æå–çµæœ[/bold]")
        console.print(f"   ğŸ¯ Sitemap ç¸½ URL æ•¸: {len(urls):,} å€‹")
        console.print(f"   ğŸ“„ ä¾†æº: {sitemap_url}")
        return urls

    def get_db_pages_and_coverage(self, site_id: int, days: Optional[int]) -> Tuple[Set[str], dict]:
        """ä¸€æ¬¡æ€§ç²å–è³‡æ–™åº«ä¸­çš„ä¸é‡è¤‡é é¢åŠæ•¸æ“šè¦†è“‹æƒ…æ³"""

        console.print("\nğŸ” æ­£åœ¨æŸ¥è©¢è³‡æ–™åº«...")

        date_clause = ""
        time_range_text = "å…¨éƒ¨æ™‚é–“"

        if days:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            date_clause = "AND date BETWEEN ? AND ?"
            params = [site_id, start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")]
            time_range_text = f"æœ€è¿‘ {days} å¤©"
        else:
            params = [site_id]

        # æŸ¥è©¢æ•¸æ“šè¦†è“‹æƒ…æ³
        coverage_query = f"""
            SELECT date, COUNT(*) FROM gsc_performance_data
            WHERE site_id = ? {date_clause}
            GROUP BY date ORDER BY date DESC
        """
        with self.db._lock:
            coverage_results = self.db._connection.execute(coverage_query, params).fetchall()

        actual_days = len(coverage_results)
        total_records = sum(row[1] for row in coverage_results)

        console.print(f"ğŸ“Š [bold]è³‡æ–™åº«æ•¸æ“šè¦†è“‹æƒ…æ³[/bold] (æŸ¥è©¢ç¯„åœ: {time_range_text})")
        if days and days > 0:
            console.print(
                f"   âœ… å¯¦éš›æœ‰æ•¸æ“šå¤©æ•¸: {actual_days} å¤© ({actual_days / days * 100:.1f}%)"
            )
        else:
            console.print(f"   âœ… å¯¦éš›æœ‰æ•¸æ“šå¤©æ•¸: {actual_days} å¤©")

        if actual_days > 0:
            console.print(
                f"   ğŸ“… æ•¸æ“šæ—¥æœŸç¯„åœ: {coverage_results[-1][0]} åˆ° {coverage_results[0][0]}"
            )
        console.print(f"   ğŸ“ˆ ç¸½æ•¸æ“šé»è¨˜éŒ„æ•¸: {total_records:,} ç­†")

        # æŸ¥è©¢ä¸é‡è¤‡é é¢
        pages_query = (
            f"SELECT DISTINCT page FROM gsc_performance_data WHERE site_id = ? {date_clause}"
        )
        with self.db._lock:
            page_results = self.db._connection.execute(pages_query, params).fetchall()

        db_pages = {row[0] for row in page_results}
        console.print(f"   ğŸ¯ æœ‰æ•¸æ“šçš„ç¨ç«‹é é¢URLæ•¸: {len(db_pages):,} å€‹")

        coverage_info = {
            "queried_days": time_range_text,
            "actual_days": actual_days,
            "db_unique_pages": len(db_pages),
        }
        return db_pages, coverage_info

    def analyze_and_display(
        self,
        sitemap_urls: List[str],
        db_pages: Set[str],
        site_info: Dict,
        coverage_info: Dict,
        output_csv_path: Optional[str] = None,
        site_id: Optional[int] = None,
        days: Optional[int] = None,
    ):
        """åˆ†æå†—é¤˜ä¸¦é¡¯ç¤ºçµæœ"""
        console.print("\nğŸ” æ­£åœ¨é€²è¡Œå†—é¤˜åˆ†æ...")

        sitemap_set = set(sitemap_urls)

        urls_in_db = sitemap_set.intersection(db_pages)
        urls_not_in_db = sitemap_set - db_pages

        # å‰µå»ºçµæœé¢æ¿
        title = f"[bold]Sitemap å†—é¤˜åˆ†æå ±å‘Š for {site_info.get('name', 'æœªçŸ¥')}[/bold]"
        table = Table(title=title, show_header=False, box=None, padding=(0, 2))
        table.add_column(style="cyan")
        table.add_column(style="magenta")

        table.add_row("\n[bold]Sitemap çµ±è¨ˆ[/bold]", "")
        table.add_row("Sitemap ç¸½ URL æ•¸", f"{len(sitemap_urls):,}")
        table.add_row("å»é‡å¾Œç¨ç«‹ URL æ•¸", f"{len(sitemap_set):,}")

        table.add_row("\n[bold]è³‡æ–™åº«çµ±è¨ˆ[/bold]", f"({coverage_info['queried_days']})")
        table.add_row("æœ‰æ•¸æ“šçš„ç¨ç«‹ URL æ•¸", f"{coverage_info['db_unique_pages']:,}")

        table.add_row("\n[bold]å†—é¤˜åˆ†æ (Sitemap vs è³‡æ–™åº«)[/bold]", "")
        table.add_row("âœ… æœ‰æ•¸æ“šçš„ Sitemap URL", f"{len(urls_in_db):,}")
        table.add_row("âŒ ç„¡æ•¸æ“šçš„ Sitemap URL", f"{len(urls_not_in_db):,}")

        if len(sitemap_set) > 0:
            redundancy_rate = len(urls_not_in_db) / len(sitemap_set) * 100
            coverage_rate = len(urls_in_db) / len(sitemap_set) * 100

            # æ ¹æ“šå†—é¤˜ç‡é¡¯ç¤ºä¸åŒé¡è‰²çš„çµæœ
            color = "green"
            if redundancy_rate > 50:
                color = "red"
            elif redundancy_rate > 20:
                color = "yellow"

            table.add_row(
                f"[{color}]å†—é¤˜ç‡ (ç„¡æ•¸æ“šçš„ URL ä½”æ¯”)[/{color}]",
                f"[{color}]{redundancy_rate:.1f}%[/{color}]",
            )
            table.add_row(
                "[green]è¦†è“‹ç‡ (æœ‰æ•¸æ“šçš„ URL ä½”æ¯”)[/green]", f"[green]{coverage_rate:.1f}%[/green]"
            )

        console.print(Panel(table, expand=False))

        # å¦‚æœæä¾›äº†è¼¸å‡ºè·¯å¾‘ï¼Œå‰‡å°‡è©³ç´°åˆ†æçµæœå¯«å…¥ Excel
        if output_csv_path:
            output_path = Path(output_csv_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            try:
                # åˆ¤æ–·è¼¸å‡ºæ ¼å¼ï¼šå¦‚æœæª”æ¡ˆå‰¯æª”åæ˜¯ .xlsxï¼Œå‰‡è¼¸å‡º Excelï¼›å¦å‰‡è¼¸å‡º CSV
                if output_path.suffix.lower() == ".xlsx":
                    if site_id is not None:
                        self._export_to_excel(
                            output_path,
                            sitemap_urls,
                            urls_in_db,
                            urls_not_in_db,
                            site_info,
                            coverage_info,
                            redundancy_rate if len(sitemap_set) > 0 else 0,
                            coverage_rate if len(sitemap_set) > 0 else 0,
                            site_id,
                            days,
                        )
                    else:
                        console.print(
                            "[bold red]éŒ¯èª¤ï¼šç„¡æ³•ç”ŸæˆExcelæª”æ¡ˆï¼Œç¼ºå°‘site_idåƒæ•¸[/bold red]"
                        )
                else:
                    # ä¿æŒåŸæœ‰çš„ CSV æ ¼å¼ï¼ˆåªæœ‰ç„¡æ•¸æ“šçš„ URLï¼‰
                    with open(output_path, "w", newline="", encoding="utf-8") as csvfile:
                        writer = csv.writer(csvfile)
                        writer.writerow(["URL"])  # å¯«å…¥æ¨™é ­
                        for url in sorted(list(urls_not_in_db)):
                            writer.writerow([url])
                    console.print(
                        f"\nğŸ’¾ [bold green]ç„¡æ•¸æ“šçš„å†—é¤˜ URL åˆ—è¡¨å·²å„²å­˜è‡³: "
                        f"{output_path}[/bold green]"
                    )
            except Exception as e:
                console.print(f"\n[bold red]éŒ¯èª¤ï¼šç„¡æ³•å¯«å…¥æª”æ¡ˆ {output_path}: {e}[/bold red]")

    def _export_to_excel(
        self,
        output_path: Path,
        sitemap_urls: List[str],
        urls_in_db: Set[str],
        urls_not_in_db: Set[str],
        site_info: Dict,
        coverage_info: Dict,
        redundancy_rate: float,
        coverage_rate: float,
        site_id: int,
        days: Optional[int],
    ):
        """å°‡åˆ†æçµæœå°å‡ºç‚º Excel æª”æ¡ˆï¼ŒåŒ…å«å¤šå€‹å·¥ä½œè¡¨"""

        with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
            # å·¥ä½œè¡¨1ï¼šåˆ†æå ±å‘Šæ‘˜è¦
            summary_data = {
                "é …ç›®": [
                    "ç¶²ç«™åç¨±",
                    "ç¶²ç«™ID",
                    "Sitemap ç¸½ URL æ•¸",
                    "Sitemap å»é‡å¾Œç¨ç«‹ URL æ•¸",
                    "GSC performace ä¸­çš„ç¨ç«‹ URL æ•¸",
                    "âœ… æ“æœ‰ GSC performace çš„ URLï¼ˆSitemapï¼‰",
                    "âŒ æ²’æœ‰ GSC performace æ•¸æ“šçš„  URLï¼ˆSitemapï¼‰",
                    "å†—é¤˜ç‡ (%)",
                    "è¦†è“‹ç‡ (%)",
                    "æŸ¥è©¢æ™‚é–“ç¯„åœ",
                    "å¯¦éš›æœ‰æ•¸æ“šå¤©æ•¸",
                ],
                "æ•¸å€¼": [
                    site_info.get("name", "æœªçŸ¥"),
                    site_info.get("id", "æœªçŸ¥"),
                    len(sitemap_urls),
                    len(set(sitemap_urls)),
                    coverage_info["db_unique_pages"],
                    len(urls_in_db),
                    len(urls_not_in_db),
                    f"{redundancy_rate:.1f}%",
                    f"{coverage_rate:.1f}%",
                    coverage_info["queried_days"],
                    coverage_info["actual_days"],
                ],
            }
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name="åˆ†æå ±å‘Š", index=False)

            # å·¥ä½œè¡¨2ï¼šæœ‰æ•¸æ“šçš„URLåˆ—è¡¨
            if urls_in_db:
                # å° URL é€²è¡Œç·¨ç¢¼
                encoded_urls = [
                    quote(url, safe=":/?#[]@!$&'()*+,;=") for url in sorted(list(urls_in_db))
                ]
                urls_with_data_df = pd.DataFrame(
                    {"URL": encoded_urls, "ç‹€æ…‹": ["æœ‰æ•¸æ“š"] * len(urls_in_db)}
                )
                urls_with_data_df.to_excel(
                    writer, sheet_name="æœ‰ GSC performace çš„ URL", index=False
                )

            # å·¥ä½œè¡¨3ï¼šç„¡æ•¸æ“šçš„URLåˆ—è¡¨ï¼ˆå†—é¤˜ï¼‰
            if urls_not_in_db:
                # å° URL é€²è¡Œç·¨ç¢¼
                encoded_urls = [
                    quote(url, safe=":/?#[]@!$&'()*+,;=") for url in sorted(list(urls_not_in_db))
                ]
                urls_without_data_df = pd.DataFrame(
                    {"URL": encoded_urls, "ç‹€æ…‹": ["ç„¡æ•¸æ“š"] * len(urls_not_in_db)}
                )
                urls_without_data_df.to_excel(
                    writer, sheet_name="ç„¡ GSC performace URL", index=False
                )

            # å·¥ä½œè¡¨4ï¼šæ¯æœˆå¹³å‡è¡¨ç¾è¡¨
            if urls_in_db:
                monthly_performance = self._get_monthly_performance(site_id, list(urls_in_db), days)
                if monthly_performance:
                    monthly_df = pd.DataFrame(monthly_performance)
                    monthly_df.to_excel(writer, sheet_name="æ¯æœˆå¹³å‡è¡¨ç¾è¡¨", index=False)

                    # è¨­ç½® Excel æ ¼å¼ï¼Œè®“é—œéµå­—æ¬„ä½æ”¯æŒæ›è¡Œé¡¯ç¤º
                    worksheet = writer.sheets["æ¯æœˆå¹³å‡è¡¨ç¾è¡¨"]

                    # æ‰¾åˆ°é—œéµå­—æ¬„ä½çš„ç´¢å¼•
                    if monthly_performance:
                        columns = list(monthly_performance[0].keys())
                        if "é—œéµå­—" in columns:
                            keyword_col_idx = columns.index("é—œéµå­—") + 1  # Excel åˆ—ç´¢å¼•å¾1é–‹å§‹

                            # è¨­ç½®é—œéµå­—æ¬„ä½çš„æ ¼å¼
                            for row_idx in range(
                                2, len(monthly_performance) + 2
                            ):  # å¾ç¬¬2è¡Œé–‹å§‹ï¼ˆè·³éæ¨™é¡Œè¡Œï¼‰
                                cell = worksheet.cell(row=row_idx, column=keyword_col_idx)
                                cell.alignment = cell.alignment.copy(wrapText=True)

                            # è¨­ç½®æ¬„ä½å¯¬åº¦
                            worksheet.column_dimensions[
                                chr(ord("A") + keyword_col_idx - 1)
                            ].width = 50

            # æ‡‰ç”¨æ¨£å¼
            self._apply_excel_styles(writer)

        console.print(f"\nğŸ’¾ [bold green]è©³ç´°åˆ†æå ±å‘Šå·²å„²å­˜è‡³: {output_path}[/bold green]")
        console.print("ğŸ“Š Excel æª”æ¡ˆåŒ…å«ä»¥ä¸‹å·¥ä½œè¡¨ï¼š")
        console.print("   â€¢ åˆ†æå ±å‘Š - æ‘˜è¦çµ±è¨ˆ")
        console.print("   â€¢ æœ‰ GSC performace çš„ URL - åœ¨GSCè³‡æ–™åº«ä¸­æœ‰æ•¸æ“šçš„URL")
        console.print("   â€¢ ç„¡ GSC performace URL - åœ¨Sitemapä¸­ä½†GSCè³‡æ–™åº«ç„¡æ•¸æ“šçš„URL")
        console.print("   â€¢ æ¯æœˆå¹³å‡è¡¨ç¾è¡¨ - æœ‰æ•¸æ“šURLçš„æœˆåº¦è¡¨ç¾çµ±è¨ˆ")

    def _apply_excel_styles(self, writer):
        """ç‚º Excel æª”æ¡ˆæ‡‰ç”¨æ¨£å¼"""
        from openpyxl.styles import Alignment, Font, PatternFill

        # å®šç¾©æ¨£å¼
        header_font = Font(bold=True, color="FFFFFF")  # ç²—é«”ç™½å­—
        header_fill = PatternFill(
            start_color="000000", end_color="000000", fill_type="solid"
        )  # é»‘åº•

        highlight_font = Font(color="000000")  # é»‘å­—
        highlight_fill = PatternFill(
            start_color="FFFF00", end_color="FFFF00", fill_type="solid"
        )  # è¢å…‰ç­†èƒŒæ™¯ï¼ˆé»ƒè‰²ï¼‰

        # æ‡‰ç”¨æ¨£å¼åˆ°æ‰€æœ‰å·¥ä½œè¡¨
        for sheet_name in writer.sheets:
            worksheet = writer.sheets[sheet_name]

            # ç‚º header è¡Œæ·»åŠ æ¨£å¼ï¼ˆç¬¬ä¸€è¡Œï¼‰
            for cell in worksheet[1]:
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = Alignment(horizontal="center", vertical="center")

            # ç‚ºåˆ†æå ±å‘Šä¸­çš„ç‰¹å®šè¡Œæ·»åŠ è¢å…‰ç­†æ¨£å¼
            if sheet_name == "åˆ†æå ±å‘Š":
                for row in worksheet.iter_rows(min_row=2, max_row=worksheet.max_row):
                    item_cell = row[0]  # é …ç›®æ¬„ä½
                    value_cell = row[1]  # æ•¸å€¼æ¬„ä½

                    # æª¢æŸ¥æ˜¯å¦ç‚ºå†—é¤˜ç‡æˆ–è¦†è“‹ç‡
                    if item_cell.value and (
                        "å†—é¤˜ç‡" in str(item_cell.value) or "è¦†è“‹ç‡" in str(item_cell.value)
                    ):
                        item_cell.font = highlight_font
                        item_cell.fill = highlight_fill
                        value_cell.font = highlight_font
                        value_cell.fill = highlight_fill

            # èª¿æ•´åˆ—å¯¬
            for column in worksheet.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except (TypeError, AttributeError):
                        pass
                adjusted_width = min(max_length + 2, 50)  # æœ€å¤§å¯¬åº¦é™åˆ¶ç‚º50
                worksheet.column_dimensions[column_letter].width = adjusted_width

    def _get_monthly_performance(
        self, site_id: int, urls_with_data: List[str], days: Optional[int]
    ) -> List[Dict]:
        """ç²å–æœ‰æ•¸æ“šURLçš„æ¯æœˆå¹³å‡è¡¨ç¾"""
        console.print("\nğŸ“ˆ æ­£åœ¨ç²å–æ¯æœˆå¹³å‡è¡¨ç¾æ•¸æ“š...")

        # æ§‹å»ºæ—¥æœŸç¯©é¸æ¢ä»¶
        date_clause = ""
        params: List[str] = [str(site_id)]

        if days and days > 0:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            date_clause = "AND date >= ?"
            params.append(start_date.strftime("%Y-%m-%d"))

        # å‰µå»ºURLåƒæ•¸ä½”ä½ç¬¦
        url_placeholders = ",".join("?" * len(urls_with_data))
        params.extend(urls_with_data)

        # æŸ¥è©¢æ¯æœˆå¹³å‡è¡¨ç¾ï¼ˆä¿®æ”¹ç‚ºåŠ ç¸½é»æ“Šæ•¸å’Œæ›å…‰æ•¸ï¼Œä¸¦æ–°å¢é—œéµå­—ç›¸é—œæ¬„ä½ï¼‰
        query = f"""
        SELECT
            page,
            strftime('%Y-%m', date) as month,
            SUM(clicks) as total_clicks,
            SUM(impressions) as total_impressions,
            AVG(ctr) as avg_ctr,
            AVG(position) as avg_position,
            COUNT(*) as record_count,
            REPLACE(GROUP_CONCAT(DISTINCT query), ',', CHAR(10)) as keywords,
            COUNT(DISTINCT query) as keyword_count
        FROM gsc_performance_data
        WHERE site_id = ? {date_clause}
        AND page IN ({url_placeholders})
        GROUP BY page, strftime('%Y-%m', date)
        ORDER BY page, month
        """

        try:
            with self.db._lock:
                results = self.db._connection.execute(query, params).fetchall()

            performance_data = []
            for row in results:
                # å° URL é€²è¡Œç·¨ç¢¼
                encoded_url = quote(row[0], safe=":/?#[]@!$&'()*+,;=")

                performance_data.append(
                    {
                        "URL": encoded_url,
                        "æœˆä»½": row[1],
                        "ç¸½é»æ“Šæ•¸": int(row[2] or 0),
                        "ç¸½æ›å…‰æ•¸": int(row[3] or 0),
                        "å¹³å‡é»æ“Šç‡": round((row[4] or 0) * 100, 3),  # è½‰æ›ç‚ºç™¾åˆ†æ¯”
                        "å¹³å‡æ’å": round(row[5] or 0, 2),
                        "è¨˜éŒ„æ•¸": row[6],
                        "é—œéµå­—": row[7] or "",
                        "é—œéµå­—æ•¸": row[8] or 0,
                    }
                )

            console.print(f"   âœ… ç²å– {len(performance_data)} æ¢æœˆåº¦è¡¨ç¾è¨˜éŒ„")
            return performance_data

        except Exception as e:
            console.print(f"[red]ç²å–æœˆåº¦è¡¨ç¾æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}[/red]")
            return []


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description="å¿«é€Ÿ Sitemap å†—é¤˜åˆ†æå·¥å…·")
    parser.add_argument("--site-id", type=int, required=True, help="è¦åˆ†æçš„ç¶²ç«™ ID")
    parser.add_argument(
        "--sitemap-url", type=str, action="append", help="æ‰‹å‹•æŒ‡å®š Sitemap URLï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰"
    )
    parser.add_argument("--days", type=int, help="æŸ¥è©¢å¤©æ•¸ç¯„åœï¼ˆå¯é¸ï¼Œé è¨­æŸ¥è©¢å…¨éƒ¨æ™‚é–“ï¼‰")
    parser.add_argument(
        "--interactive-discovery", action="store_true", help="å¼·åˆ¶é€²è¡Œäº¤äº’å¼ Sitemap é¸æ“‡"
    )
    parser.add_argument(
        "--single-sitemap", action="store_true", help="åªä½¿ç”¨ç¬¬ä¸€å€‹ç™¼ç¾çš„ sitemapï¼ˆèˆŠè¡Œç‚ºï¼‰"
    )
    parser.add_argument(
        "--output-csv",
        type=str,
        help="å°‡åˆ†æçµæœå°å‡ºåˆ°æŒ‡å®šæª”æ¡ˆ (.xlsx=Excelå¤šå·¥ä½œè¡¨, .csv=åƒ…å†—é¤˜URL, "
        "é è¨­è¼¸å‡ºExcelåˆ°data/è³‡æ–™å¤¾)",
    )
    parser.add_argument(
        "--no-smart-discovery",
        action="store_true",
        help="æš«åœæ™ºèƒ½ Sitemap ç™¼ç¾åŠŸèƒ½ï¼Œéœ€è¦æ‰‹å‹•æŒ‡å®š --sitemap-url",
    )

    args = parser.parse_args()

    console.print(Panel.fit("[bold blue]å¿«é€Ÿ Sitemap å†—é¤˜åˆ†æå·¥å…·[/bold blue]"))

    try:
        container = Container()
        db = container.database()
        console.print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
    except Exception as e:
        console.print(f"[bold red]éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥è³‡æ–™åº«: {e}[/bold red]")
        sys.exit(1)

    site_info = db.get_site_by_id(args.site_id)
    if not site_info:
        console.print(f"[bold red]éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° ID ç‚º {args.site_id} çš„ç«™é»[/bold red]")
        sys.exit(1)

    analyzer = SitemapAnalyzer(db)

    # ç²å– Sitemap URL
    sitemap_urls_to_fetch = args.sitemap_url
    if not sitemap_urls_to_fetch:
        if args.no_smart_discovery:
            console.print(
                "[bold yellow]æ™ºèƒ½ Sitemap ç™¼ç¾åŠŸèƒ½å·²æš«åœï¼Œè«‹ä½¿ç”¨ --sitemap-url "
                "æ‰‹å‹•æŒ‡å®š Sitemap URL[/bold yellow]"
            )
            console.print("ç¯„ä¾‹ï¼š--sitemap-url 'https://example.com/sitemap.xml'")
            sys.exit(1)

        domain = site_info["domain"].replace("sc-domain:", "")

        if args.single_sitemap:
            # ä½¿ç”¨èˆŠè¡Œç‚ºï¼Œåªé¸æ“‡ä¸€å€‹ sitemap
            discovered_url = analyzer.discover_and_select_sitemap(
                domain, auto_select=not args.interactive_discovery
            )
            if discovered_url:
                sitemap_urls_to_fetch = [discovered_url]
            else:
                sys.exit(1)
        else:
            # æ–°è¡Œç‚ºï¼šä½¿ç”¨æ‰€æœ‰ç™¼ç¾çš„ sitemaps
            discovered_urls = analyzer.discover_sitemaps(domain)
            if discovered_urls:
                sitemap_urls_to_fetch = discovered_urls
            else:
                sys.exit(1)

    # çµ±è¨ˆç¸½ URL æ•¸
    all_sitemap_urls = []
    total_urls_from_all_sources = 0

    console.print(f"\nğŸ” [bold]é–‹å§‹è™•ç† {len(sitemap_urls_to_fetch)} å€‹ Sitemap ä¾†æº[/bold]")

    for i, url in enumerate(sitemap_urls_to_fetch, 1):
        console.print(
            f"\nğŸ“„ [bold cyan]è™•ç† Sitemap {i}/{len(sitemap_urls_to_fetch)}:[/bold cyan] {url}"
        )
        urls_from_this_source = analyzer.fetch_sitemap_urls(url)
        if urls_from_this_source:
            all_sitemap_urls.extend(urls_from_this_source)
            total_urls_from_all_sources += len(urls_from_this_source)
            console.print(f"   âœ… å¾æ­¤ä¾†æºç²å–: {len(urls_from_this_source):,} å€‹ URL")
        else:
            console.print("   âŒ æ­¤ä¾†æºç„¡æ³•ç²å–ä»»ä½• URL")

    # å»é‡çµ±è¨ˆ
    unique_urls = set(all_sitemap_urls)
    console.print("\nğŸ“Š [bold green]æ‰€æœ‰ Sitemap åˆä½µçµ±è¨ˆ:[/bold green]")
    console.print(f"   ğŸ¯ ç¸½ URL æ•¸: {total_urls_from_all_sources:,} å€‹")
    console.print(f"   ğŸ”„ å»é‡å¾Œ: {len(unique_urls):,} å€‹")
    if total_urls_from_all_sources > 0:
        duplicate_rate = (
            (total_urls_from_all_sources - len(unique_urls)) / total_urls_from_all_sources * 100
        )
        console.print(f"   ğŸ“‰ é‡è¤‡ç‡: {duplicate_rate:.1f}%")
    else:
        console.print("   ğŸ“‰ é‡è¤‡ç‡: 0%")

    if not all_sitemap_urls:
        console.print("[bold yellow]æœªèƒ½å¾æŒ‡å®šçš„ Sitemap ä¸­æå–ä»»ä½• URLã€‚[/bold yellow]")
        sys.exit(0)

    db_pages, coverage_info = analyzer.get_db_pages_and_coverage(args.site_id, args.days)

    # å¦‚æœæ²’æœ‰æŒ‡å®šè¼¸å‡ºè·¯å¾‘ï¼Œå‰‡é è¨­è¼¸å‡ºåˆ° data è³‡æ–™å¤¾ï¼ˆExcelæ ¼å¼ï¼‰
    output_csv_path = args.output_csv
    if not output_csv_path:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        site_name = site_info.get("name", f"site_{args.site_id}").replace(" ", "_")
        filename = f"sitemap_redundancy_{site_name}_{timestamp}.xlsx"
        output_csv_path = f"data/{filename}"

    analyzer.analyze_and_display(
        all_sitemap_urls,
        db_pages,
        site_info,
        coverage_info,
        output_csv_path,
        args.site_id,
        args.days,
    )


if __name__ == "__main__":
    main()
