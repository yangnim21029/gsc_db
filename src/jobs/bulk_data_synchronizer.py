#!/usr/bin/env python3
"""
GSC æ•¸æ“šåŒæ­¥ä½œæ¥­
- å·²é‡æ§‹ç‚ºå¯ç›´æ¥èª¿ç”¨çš„å‡½æ•¸ã€‚
- åŒ…å«é€²åº¦æ¢ã€é‡è©¦ã€æ–·é»çºŒå‚³å’Œå¤šç¨®åŒæ­¥æ¨¡å¼ã€‚
"""

import concurrent.futures
import logging
import traceback
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)
from rich.table import Table
from tenacity import (
    RetryError,
    before_sleep_log,
    retry,
    stop_after_attempt,
    wait_exponential,
)

from ..config import settings
from ..services.database import Database, SyncMode
from ..services.gsc_client import GSCClient
from ..utils.rich_console import console

logger = logging.getLogger(__name__)


def _check_existing_data(db: Database, site_id: int, date: str) -> bool:
    """æª¢æŸ¥æŒ‡å®šç«™é»å’Œæ—¥æœŸæ˜¯å¦å·²æœ‰æ•¸æ“š"""
    with db._lock:
        result = db._connection.execute(
            "SELECT COUNT(*) as count FROM gsc_performance_data WHERE site_id = ? AND date = ?",
            (site_id, date),
        ).fetchone()
        return bool(result and result["count"] > 0)


@retry(
    wait=wait_exponential(
        multiplier=1,
        min=settings.retry.wait_min_seconds,
        max=settings.retry.wait_max_seconds,
    ),
    stop=stop_after_attempt(settings.retry.attempts),
    before_sleep=before_sleep_log(logger, logging.WARNING),
)
def _sync_single_day(
    db: Database,
    client: GSCClient,
    site: Dict[str, Any],
    date: str,
    sync_mode: SyncMode,
) -> Dict[str, int]:
    """åŒæ­¥å–®æ—¥æ•¸æ“šï¼Œä½¿ç”¨ tenacity é€²è¡Œé‡è©¦ã€‚"""
    site_name = site["name"]

    if sync_mode == SyncMode.OVERWRITE:
        db.delete_performance_data_for_day(site["id"], date)

    day_stats = {"inserted": 0, "updated": 0, "skipped": 0}
    data_stream = client.stream_site_data(site_url=site["domain"], start_date=date, end_date=date)

    for device, search_type, chunk in data_stream:
        chunk_stats = db.save_data_chunk(
            chunk=chunk,
            site_id=site["id"],
            sync_mode=sync_mode.value,
            date_str=date,
            device=device,
            search_type=search_type,
        )
        for key in day_stats:
            day_stats[key] += chunk_stats.get(key, 0)

    logger.info(
        f"ç«™é» [bold cyan]{site_name}[/bold cyan] æ—¥æœŸ [bold]{date}[/bold] - "
        f"[green]åŒæ­¥æˆåŠŸ[/green]"
    )
    return day_stats


def _print_final_summary(total_stats: Dict[str, int]):
    """æ‰“å°æœ€çµ‚çš„åŒæ­¥çµæœæ‘˜è¦ã€‚"""
    table = Table(title="ğŸ“Š åŒæ­¥çµæœæ‘˜è¦", show_header=True, header_style="bold magenta")
    table.add_column("é …ç›®", style="dim")
    table.add_column("æ•¸é‡", justify="right")

    table.add_row("æ’å…¥è¨˜éŒ„", f"[green]{total_stats.get('inserted', 0):,}[/green]")
    table.add_row("æ›´æ–°è¨˜éŒ„", f"[blue]{total_stats.get('updated', 0):,}[/blue]")
    table.add_row("è·³éè¨˜éŒ„", f"[yellow]{total_stats.get('skipped', 0):,}[/yellow]")
    table.add_row("å¤±æ•—ä»»å‹™", f"[red]{total_stats.get('failed', 0):,}[/red]")

    console.print(table)


def run_sync(
    db: Database,
    client: GSCClient,
    all_sites: bool,
    site_id: Optional[int],
    start_date: Optional[str],
    end_date: Optional[str],
    days: int,
    sync_mode: SyncMode,
    max_workers: int,
):
    """åŸ·è¡Œ GSC æ•¸æ“šçš„æ¯æ—¥åŒæ­¥ä½œæ¥­ï¼Œæ¡ç”¨è¨ˆåŠƒ-åŸ·è¡Œæ¨¡å¼ã€‚"""
    # ================= 1. æº–å‚™éšæ®µ =================
    logger.info("æ­£åœ¨æº–å‚™åŒæ­¥ä½œæ¥­...")

    # ç²å–è¦åŒæ­¥çš„ç¶²ç«™åˆ—è¡¨
    sites_to_sync: List[Dict[str, Any]] = []
    if all_sites:
        sites_to_sync = db.get_sites(active_only=True)
    elif site_id:
        site = db.get_site_by_id(site_id)
        if site:
            sites_to_sync.append(site)

    if not sites_to_sync:
        logger.warning("æ‰¾ä¸åˆ°ä»»ä½•è¦åŒæ­¥çš„æ´»å‹•ç«™é»ã€‚")
        return

    # ç¢ºå®šæ—¥æœŸç¯„åœ
    if start_date and end_date:
        s_date = datetime.strptime(start_date, "%Y-%m-%d").date()
        e_date = datetime.strptime(end_date, "%Y-%m-%d").date()
    else:
        today = datetime.now().date()
        e_date = today - timedelta(days=2)  # GSCæ•¸æ“šå»¶é²ç´„2å¤©
        s_date = e_date - timedelta(days=days - 1)

    date_list = [
        (s_date + timedelta(days=i)).strftime("%Y-%m-%d") for i in range((e_date - s_date).days + 1)
    ]

    # ================= 2. è¨ˆåŠƒéšæ®µ =================
    all_possible_tasks = [(site, date) for site in sites_to_sync for date in date_list]
    tasks_to_run = all_possible_tasks
    all_possible_task_count = len(sites_to_sync) * len(date_list)

    if sync_mode == SyncMode.SKIP:
        logger.info("SKIP æ¨¡å¼ï¼šæ­£åœ¨æª¢æŸ¥æ•¸æ“šåº«ä¸­å·²å­˜åœ¨çš„æ•¸æ“š...")
        site_ids = [s["id"] for s in sites_to_sync]
        existing_data_days = db.get_existing_data_days_for_sites(
            site_ids, date_list[0], date_list[-1]
        )
        tasks_to_run = [
            (site, date)
            for site, date in all_possible_tasks
            if (site["id"], date) not in existing_data_days
        ]

    logger.info(
        f"è¨ˆåŠƒåŒæ­¥ {len(sites_to_sync)} å€‹ç«™é»ï¼Œ {len(date_list)} å¤©ã€‚"
        f"å°‡è¦åŸ·è¡Œ: {len(tasks_to_run)} å€‹åŒæ­¥ä»»å‹™ã€‚"
    )

    if not tasks_to_run:
        logger.info("æ‰€æœ‰è¨ˆåŠƒçš„æ•¸æ“šéƒ½å·²å­˜åœ¨ï¼Œç„¡éœ€åŒæ­¥ã€‚")
        _print_final_summary(
            {"inserted": 0, "updated": 0, "skipped": all_possible_task_count, "failed": 0}
        )
        return

    # ================= 3. åŸ·è¡Œéšæ®µ =================
    total_stats = {
        "inserted": 0,
        "updated": 0,
        "skipped": all_possible_task_count - len(tasks_to_run),
        "failed": 0,
    }

    progress_columns = [
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TextColumn("({task.completed}/{task.total})"),
        TimeElapsedColumn(),
        TimeRemainingColumn(),
    ]

    with Progress(*progress_columns, console=console) as progress:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            task_id = progress.add_task("[bold green]ä¸¦è¡ŒåŒæ­¥ä¸­...", total=len(tasks_to_run))

            future_to_task = {
                executor.submit(_sync_single_day, db, client, site, date, sync_mode): (site, date)
                for site, date in tasks_to_run
            }

            for future in concurrent.futures.as_completed(future_to_task):
                site, date = future_to_task[future]
                try:
                    day_stats = future.result()
                    for key in total_stats:
                        if key in day_stats:
                            total_stats[key] += day_stats[key]
                except RetryError as e:
                    logger.error(
                        f"åŒæ­¥å¤±æ•— [ç«™é»: {site['name']}, æ—¥æœŸ: {date}]: "
                        f"å¤šæ¬¡å˜—è©¦å¾Œä»ç„¶å¤±æ•—ã€‚æœ€å¾Œä¸€æ¬¡éŒ¯èª¤: {e.last_attempt.exception()}"
                    )
                    total_stats["failed"] += 1
                except Exception as exc:
                    logger.error(f"ä»»å‹™ {site['name']}-{date} ç”¢ç”Ÿæœªé æœŸçš„ä¾‹å¤–: {exc}")
                    logger.debug(traceback.format_exc())
                    total_stats["failed"] += 1

                progress.update(task_id, advance=1)

    _print_final_summary(total_stats)


class BulkDataSynchronizer:
    """Bulk data synchronizer wrapper class for dependency injection."""

    def __init__(self, db: Database, gsc_client: GSCClient):
        self.db = db
        self.gsc_client = gsc_client

    def run_sync(
        self,
        all_sites: bool = False,
        site_id: Optional[int] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        days: int = 2,
        sync_mode: SyncMode = SyncMode.OVERWRITE,
        max_workers: int = 4,
    ):
        """Run the synchronization process."""
        return run_sync(
            db=self.db,
            client=self.gsc_client,
            all_sites=all_sites,
            site_id=site_id,
            start_date=start_date,
            end_date=end_date,
            days=days,
            sync_mode=sync_mode,
            max_workers=max_workers,
        )
