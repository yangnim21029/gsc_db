#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¸€æ¬¡æ€§è…³æœ¬ï¼šå¾ç«™é»çš„ Sitemap ä¸­æå– URLï¼ŒæŸ¥è©¢å…¶åœ¨ GSC DB ä¸­çš„è¡¨ç¾æ•¸æ“šï¼Œä¸¦å°å‡ºç‚º CSVã€‚

æ¥­å‹™éœ€æ±‚ï¼šç‚ºç‰¹å®šç«™é»çš„ Sitemap URL åˆ—è¡¨æä¾›ä¸€ä»½è©³ç´°çš„ SEO è¡¨ç¾å ±å‘Šã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹åŸ·è¡Œï¼š
poetry run python scripts/sitemap_url_performance_exporter.py \
    --site-id YOUR_SITE_ID \
    --sitemap-url "https://your-domain.com/sitemap.xml" \
    --output-file "scripts/reports/sitemap_performance.csv"

æˆ–è€…ä½¿ç”¨ç«™é» URL è‡ªå‹•æŸ¥æ‰¾ï¼š
poetry run python scripts/sitemap_url_performance_exporter.py \
    --site-url "https://your-domain.com" \
    --output-file "scripts/reports/sitemap_performance.csv"
"""

import argparse
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd
import requests
from lxml import etree
from rich.console import Console
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
    track,
)
from rich.table import Table

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„æ·»åŠ åˆ° sys.pathï¼Œä»¥ä¾¿å°å…¥ src æ¨¡çµ„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ruff: noqa: E402
from src.containers import Container
from src.services.database import Database

console = Console()

# Sitemap XML å‘½åç©ºé–“
SITEMAP_NS = {"ns": "http://www.sitemaps.org/schemas/sitemap/0.9"}


def fetch_sitemap_urls(sitemap_url: str) -> List[str]:
    """
    éæ­¸ç²å–ä¸¦è§£æ Sitemapï¼Œæå–æ‰€æœ‰ URLã€‚
    æ”¯æŒ Sitemap ç´¢å¼•æ–‡ä»¶ã€‚
    """
    urls = []
    try:
        console.log(f"æ­£åœ¨ç²å– Sitemap: {sitemap_url}")
        response = requests.get(sitemap_url, timeout=30)
        response.raise_for_status()
        xml_content = response.content
        root = etree.fromstring(xml_content, parser=etree.XMLParser(recover=True))

        # æª¢æŸ¥æ˜¯ Sitemap ç´¢å¼•é‚„æ˜¯ URL é›†åˆ
        if root.tag.endswith("sitemapindex"):
            console.log("æª¢æ¸¬åˆ° Sitemap ç´¢å¼•ï¼Œæ­£åœ¨è§£æå­ Sitemap...")
            sitemaps = root.xpath("//ns:sitemap/ns:loc", namespaces=SITEMAP_NS)
            for sitemap_loc in track(sitemaps, description="è§£æå­ Sitemap..."):
                urls.extend(fetch_sitemap_urls(sitemap_loc.text))
        elif root.tag.endswith("urlset"):
            locations = root.xpath("//ns:url/ns:loc", namespaces=SITEMAP_NS)
            urls.extend([loc.text for loc in locations])
            console.log(f"å¾ Sitemap ä¸­æå–äº† {len(locations)} å€‹ URL")
        else:
            console.log(f"[yellow]è­¦å‘Šï¼šæœªçŸ¥çš„ Sitemap æ ¹æ¨™ç±¤: {root.tag}[/yellow]")

    except requests.RequestException as e:
        console.log(f"[red]éŒ¯èª¤ï¼šç„¡æ³•ç²å– Sitemap {sitemap_url}: {e}[/red]")
    except etree.XMLSyntaxError as e:
        console.log(f"[red]éŒ¯èª¤ï¼šç„¡æ³•è§£æ XML {sitemap_url}: {e}[/red]")
    except Exception as e:
        console.log(f"[red]éŒ¯èª¤ï¼šè™•ç† Sitemap æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}[/red]")

    return urls


def get_performance_for_pages(
    db: Database, site_id: int, page_urls: List[str], days: int = 30
) -> Optional[List[Dict]]:
    """
    å¾è³‡æ–™åº«ä¸­ç‚ºæŒ‡å®šçš„ URL åˆ—è¡¨æŸ¥è©¢åŒ¯ç¸½çš„æ€§èƒ½æ•¸æ“šã€‚
    """
    if not page_urls:
        return None

    # è¨ˆç®—æ—¥æœŸç¯„åœ
    end_date = datetime.now().strftime("%Y-%m-%d")
    start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")

    # å°‡ URL åˆ—è¡¨åˆ†å¡Šä»¥é¿å… SQL æŸ¥è©¢éé•·
    chunk_size = 500
    all_results = []

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeElapsedColumn(),
        console=console,
    ) as progress:
        task = progress.add_task("æŸ¥è©¢è³‡æ–™åº«...", total=len(page_urls))

        for i in range(0, len(page_urls), chunk_size):
            chunk = page_urls[i : i + chunk_size]
            placeholders = ", ".join("?" for _ in chunk)
            query = f"""
                SELECT
                    page,
                    SUM(clicks) as total_clicks,
                    SUM(impressions) as total_impressions,
                    AVG(ctr) as average_ctr,
                    AVG(position) as average_position,
                    COUNT(DISTINCT query) as unique_queries,
                    COUNT(DISTINCT date) as data_days,
                    MIN(date) as earliest_date,
                    MAX(date) as latest_date
                FROM
                    gsc_performance_data
                WHERE
                    site_id = ? AND page IN ({placeholders})
                    AND date BETWEEN ? AND ?
                GROUP BY
                    page
            """
            params = [site_id] + chunk + [start_date, end_date]

            try:
                with db._lock:
                    cursor = db._connection.execute(query, params)
                    results = cursor.fetchall()
                    if results:
                        all_results.extend([dict(row) for row in results])
                progress.update(task, advance=len(chunk))
            except Exception as e:
                console.log(f"[red]è³‡æ–™åº«æŸ¥è©¢éŒ¯èª¤: {e}[/red]")
                return None

    return all_results


def get_site_by_url_or_id(
    db: Database, site_url: Optional[str] = None, site_id: Optional[int] = None
) -> Optional[Dict]:
    """
    æ ¹æ“š URL æˆ– ID æŸ¥æ‰¾ç«™é»
    """
    if site_id:
        return db.get_site_by_id(site_id)
    elif site_url:
        return db.get_site_by_domain(site_url)
    return None


def create_performance_report(
    sitemap_urls: List[str], performance_data: List[Dict], site_info: Dict, output_file: str
) -> None:
    """
    å‰µå»ºæ€§èƒ½å ±å‘Šä¸¦ä¿å­˜ç‚º CSV
    """
    console.print("[bold blue]Step 3: æ­£åœ¨ç”Ÿæˆå ±å‘Š...[/bold blue]")

    # å‰µå»º URL åˆ°æ€§èƒ½æ•¸æ“šçš„æ˜ å°„
    performance_dict = {row["page"]: row for row in performance_data}

    # æº–å‚™å ±å‘Šæ•¸æ“š
    report_data = []
    for url in sitemap_urls:
        perf = performance_dict.get(url, {})
        report_data.append(
            {
                "URL": url,
                "ç¸½é»æ“Šé‡": perf.get("total_clicks", 0),
                "ç¸½æ›å…‰é‡": perf.get("total_impressions", 0),
                "å¹³å‡é»é–±ç‡(%)": round(perf.get("average_ctr", 0) * 100, 2)
                if perf.get("average_ctr")
                else 0,
                "å¹³å‡æ’å": round(perf.get("average_position", 0), 2)
                if perf.get("average_position")
                else 0,
                "ç¨ç‰¹æŸ¥è©¢æ•¸": perf.get("unique_queries", 0),
                "æ•¸æ“šå¤©æ•¸": perf.get("data_days", 0),
                "æœ€æ—©æ—¥æœŸ": perf.get("earliest_date", ""),
                "æœ€æ–°æ—¥æœŸ": perf.get("latest_date", ""),
                "åœ¨è³‡æ–™åº«ä¸­": "æ˜¯" if url in performance_dict else "å¦",
            }
        )

    # å‰µå»º DataFrame ä¸¦æ’åº
    df = pd.DataFrame(report_data)
    df = df.sort_values(["ç¸½é»æ“Šé‡", "ç¸½æ›å…‰é‡"], ascending=[False, False])

    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜ CSV
    df.to_csv(output_path, index=False, encoding="utf-8-sig")

    # é¡¯ç¤ºæ‘˜è¦çµ±è¨ˆ
    display_summary_stats(df, sitemap_urls, performance_data, site_info, output_path)


def display_summary_stats(
    df: pd.DataFrame,
    sitemap_urls: List[str],
    performance_data: List[Dict],
    site_info: Dict,
    output_path: Path,
) -> None:
    """
    é¡¯ç¤ºæ‘˜è¦çµ±è¨ˆä¿¡æ¯
    """
    # å‰µå»ºæ‘˜è¦è¡¨æ ¼
    table = Table(title="Sitemap URL æˆæ•ˆåˆ†ææ‘˜è¦")
    table.add_column("é …ç›®", style="cyan")
    table.add_column("æ•¸å€¼", style="magenta")

    total_sitemap_urls = len(sitemap_urls)
    performance_pages = {row["page"] for row in performance_data}
    urls_with_data = len([url for url in sitemap_urls if url in performance_pages])
    coverage_rate = (urls_with_data / total_sitemap_urls * 100) if total_sitemap_urls > 0 else 0

    table.add_row("ç«™é»åç¨±", site_info.get("name", "æœªçŸ¥"))
    table.add_row("ç«™é»åŸŸå", site_info.get("domain", "æœªçŸ¥"))
    table.add_row("Sitemap URL ç¸½æ•¸", str(total_sitemap_urls))
    table.add_row("è³‡æ–™åº«ä¸­æœ‰æ•¸æ“šçš„ URL æ•¸", str(urls_with_data))
    table.add_row("æ•¸æ“šè¦†è“‹ç‡", f"{coverage_rate:.1f}%")

    if urls_with_data > 0:
        total_clicks = df["ç¸½é»æ“Šé‡"].sum()
        total_impressions = df["ç¸½æ›å…‰é‡"].sum()
        avg_ctr = df[df["ç¸½æ›å…‰é‡"] > 0]["å¹³å‡é»é–±ç‡(%)"].mean()
        avg_position = df[df["å¹³å‡æ’å"] > 0]["å¹³å‡æ’å"].mean()

        table.add_row("ç¸½é»æ“Šé‡", str(total_clicks))
        table.add_row("ç¸½æ›å…‰é‡", str(total_impressions))
        table.add_row("æ•´é«”å¹³å‡é»é–±ç‡(%)", f"{avg_ctr:.2f}" if pd.notna(avg_ctr) else "N/A")
        table.add_row("æ•´é«”å¹³å‡æ’å", f"{avg_position:.2f}" if pd.notna(avg_position) else "N/A")

    console.print(table)
    console.print(f"[bold green]ğŸš€ å ±å‘Šå·²æˆåŠŸç”Ÿæˆ: {output_path}[/bold green]")


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description="å¾ Sitemap æå– URL ä¸¦æŸ¥è©¢å…¶ GSC è¡¨ç¾æ•¸æ“šã€‚")
    parser.add_argument("--site-id", type=int, help="è¦æŸ¥è©¢çš„ç¶²ç«™çš„æœ¬åœ°è³‡æ–™åº« IDã€‚")
    parser.add_argument("--site-url", type=str, help="è¦æŸ¥è©¢çš„ç¶²ç«™ URLï¼ˆè‡ªå‹•æŸ¥æ‰¾ç«™é» IDï¼‰ã€‚")
    parser.add_argument(
        "--sitemap-url", type=str, help="è¦è§£æçš„ Sitemap çš„å®Œæ•´ URLã€‚å¦‚æœæœªæä¾›ï¼Œå°‡å˜—è©¦è‡ªå‹•ç™¼ç¾ã€‚"
    )
    parser.add_argument("--output-file", type=str, required=True, help="å°å‡ºçš„ CSV æª”æ¡ˆçš„è·¯å¾‘ã€‚")
    parser.add_argument("--days", type=int, default=30, help="æŸ¥è©¢éå»å¤šå°‘å¤©çš„æ•¸æ“šï¼ˆé è¨­ï¼š30å¤©ï¼‰ã€‚")

    args = parser.parse_args()

    # é©—è­‰åƒæ•¸
    if not args.site_id and not args.site_url:
        console.print("[bold red]éŒ¯èª¤ï¼šå¿…é ˆæä¾› --site-id æˆ– --site-url å…¶ä¸­ä¹‹ä¸€ã€‚[/bold red]")
        return

    console.print(
        Panel.fit(
            f"[bold blue]Sitemap URL æˆæ•ˆåˆ†æå·¥å…·[/bold blue]\n"
            f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        )
    )

    # åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
    try:
        container = Container()
        db = container.database()
        console.print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
    except Exception as e:
        console.print(f"[bold red]éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥è³‡æ–™åº«: {e}[/bold red]")
        return

    # æŸ¥æ‰¾ç«™é»ä¿¡æ¯
    site_info = get_site_by_url_or_id(db, args.site_url, args.site_id)
    if not site_info:
        console.print("[bold red]éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„ç«™é»ã€‚[/bold red]")
        return

    site_id = site_info["id"]
    console.print(f"âœ… æ‰¾åˆ°ç«™é»ï¼š{site_info['name']} (ID: {site_id})")

    # ç¢ºå®š Sitemap URL
    sitemap_url = args.sitemap_url
    if not sitemap_url:
        # å˜—è©¦è‡ªå‹•ç™¼ç¾ Sitemap
        domain = site_info["domain"]
        if domain.startswith("sc-domain:"):
            domain = domain.replace("sc-domain:", "")
            sitemap_url = f"https://{domain}/sitemap.xml"
        else:
            sitemap_url = f"{domain.rstrip('/')}/sitemap.xml"
        console.print(f"è‡ªå‹•ç™¼ç¾ Sitemap URL: {sitemap_url}")

    # Step 1: ç²å– Sitemap URL
    console.print(f"[bold blue]Step 1: æ­£åœ¨å¾ {sitemap_url} ç²å– URL...[/bold blue]")
    sitemap_urls = fetch_sitemap_urls(sitemap_url)
    if not sitemap_urls:
        console.print("[bold red]æœªèƒ½å¾ Sitemap ä¸­ç²å–ä»»ä½• URLã€‚è…³æœ¬çµ‚æ­¢ã€‚[/bold red]")
        return
    console.print(f"âœ… æˆåŠŸå¾ Sitemap ä¸­æ‰¾åˆ° {len(sitemap_urls)} å€‹ URLã€‚")

    # Step 2: æŸ¥è©¢æ€§èƒ½æ•¸æ“š
    console.print(f"[bold blue]Step 2: æ­£åœ¨æŸ¥è©¢éå» {args.days} å¤©çš„æ€§èƒ½æ•¸æ“š...[/bold blue]")
    performance_data = get_performance_for_pages(db, site_id, sitemap_urls, args.days)

    if not performance_data:
        console.print("[yellow]åœ¨è³‡æ–™åº«ä¸­æœªæ‰¾åˆ°èˆ‡ Sitemap URL åŒ¹é…çš„æ€§èƒ½æ•¸æ“šã€‚[/yellow]")
        # ä»ç„¶å‰µå»ºå ±å‘Šï¼Œä½†æ‰€æœ‰æ•¸æ“šéƒ½æ˜¯ 0
        performance_data = []
    else:
        console.print(f"âœ… æˆåŠŸæŸ¥è©¢åˆ° {len(performance_data)} æ¢ URL çš„æ€§èƒ½æ•¸æ“šã€‚")

    # Step 3: ç”Ÿæˆå ±å‘Š
    create_performance_report(sitemap_urls, performance_data, site_info, args.output_file)


if __name__ == "__main__":
    main()
