#!/usr/bin/env python3
"""
GSC æ•¸æ“šåŒæ­¥ä½œæ¥­
- å·²é‡æ§‹ç‚ºå¯ç›´æ¥èª¿ç”¨çš„å‡½æ•¸ã€‚
- åŒ…å«é€²åº¦æ¢ã€é‡è©¦ã€æ–·é»çºŒå‚³å’Œå¤šç¨®åŒæ­¥æ¨¡å¼ã€‚
"""

import logging
import ssl
import traceback
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import requests
from googleapiclient.errors import HttpError
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)
from rich.table import Table
from tenacity import (
    RetryError,
    before_sleep_log,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from ..config import settings
from ..services.database import Database, SyncMode
from ..services.gsc_client import GSCClient
from ..utils.console_utils import (
    format_error_message,
    format_success_message,
    format_warning_message,
)
from ..utils.rich_console import console

logger = logging.getLogger(__name__)


def _check_existing_data(db: Database, site_id: int, date: str) -> bool:
    """æª¢æŸ¥æŒ‡å®šç«™é»å’Œæ—¥æœŸæ˜¯å¦å·²æœ‰æ•¸æ“š"""
    with db._lock:
        result = db._connection.execute(
            "SELECT COUNT(*) as count FROM gsc_performance_data WHERE site_id = ? AND date = ?",
            (site_id, date),
        ).fetchone()
        return bool(result and result["count"] > 0)


def _is_retryable_error(exception: Exception) -> bool:
    """åˆ¤æ–·æ˜¯å¦ç‚ºå¯é‡è©¦çš„éŒ¯èª¤"""
    # SSL éŒ¯èª¤
    if isinstance(exception, ssl.SSLError):
        return True

    # ç¶²çµ¡é€£æ¥éŒ¯èª¤
    if isinstance(
        exception,
        (
            requests.exceptions.ConnectionError,
            requests.exceptions.Timeout,
            requests.exceptions.RequestException,
        ),
    ):
        return True

    # HTTP éŒ¯èª¤ä¸­çš„è‡¨æ™‚éŒ¯èª¤
    if isinstance(exception, HttpError):
        # 5xx ä¼ºæœå™¨éŒ¯èª¤å’Œ 429 é™æµéŒ¯èª¤é€šå¸¸å¯ä»¥é‡è©¦
        if exception.resp.status >= 500 or exception.resp.status == 429:
            return True
        # 403 å¯èƒ½æ˜¯è‡¨æ™‚çš„é…é¡é™åˆ¶
        if exception.resp.status == 403:
            return True

    # å…¶ä»–ç¶²çµ¡ç›¸é—œéŒ¯èª¤
    if "SSLError" in str(exception) or "record layer failure" in str(exception):
        return True

    if "length mismatch" in str(exception) or "SSL" in str(exception):
        return True

    # Connection cleanup errors (Google API client)
    if "'NoneType' object has no attribute 'close'" in str(exception):
        return True

    return False


@retry(
    wait=wait_exponential(
        multiplier=1,
        min=settings.retry.wait_min_seconds,
        max=settings.retry.wait_max_seconds,
    ),
    stop=stop_after_attempt(settings.retry.attempts),
    retry=retry_if_exception_type((ssl.SSLError, requests.exceptions.RequestException, HttpError)),
    before_sleep=before_sleep_log(logger, logging.WARNING),
)
def _sync_single_day(
    db: Database,
    client: GSCClient,
    site: Dict[str, Any],
    date: str,
    sync_mode: SyncMode,
) -> Dict[str, int]:
    """åŒæ­¥å–®æ—¥æ•¸æ“šï¼Œä½¿ç”¨ tenacity é€²è¡Œé‡è©¦ã€‚"""
    # --- å„ªåŒ–ï¼šåœ¨åŸ·è¡Œ API è«‹æ±‚å‰ï¼Œå…ˆé€²è¡Œæœ€çµ‚æª¢æŸ¥ ---
    # å³ä½¿ run_sync å·²ç¶“éæ¿¾ï¼Œé€™è£¡ä½œç‚ºé›™é‡ä¿éšªï¼Œç¢ºä¿ä»»å‹™çš„ç¨ç«‹æ€§å’Œå¥å£¯æ€§ã€‚
    if sync_mode == SyncMode.SKIP and _check_existing_data(db, site["id"], date):
        logger.info(f"æ•¸æ“šå·²å­˜åœ¨ï¼Œè·³é API åŒæ­¥ [ç«™é»: {site['name']}, æ—¥æœŸ: {date}]")
        return {"inserted": 0, "updated": 0, "skipped": 0, "tasks_skipped_runtime": 1}

    site_name = site["name"]

    if sync_mode == SyncMode.OVERWRITE:
        db.delete_performance_data_for_day(site["id"], date)

    day_stats = {"inserted": 0, "updated": 0, "skipped": 0}

    try:
        logger.info(f"é–‹å§‹å‘¼å« GSC APIï¼š{site['name']} - {date}")
        data_stream = client.stream_site_data(
            site_url=site["domain"], start_date=date, end_date=date
        )
        logger.info(f"GSC API å›æ‡‰æˆåŠŸï¼š{site['name']} - {date}")

        for device, search_type, chunk in data_stream:
            chunk_stats = db.save_data_chunk(
                chunk=chunk,
                site_id=site["id"],
                sync_mode=sync_mode.value,
                date_str=date,
                device=device,
                search_type=search_type,
            )
            for key in day_stats:
                day_stats[key] += chunk_stats.get(key, 0)

        logger.info(
            f"ç«™é» [bold cyan]{site_name}[/bold cyan] æ—¥æœŸ [bold]{date}[/bold] - "
            f"[green]åŒæ­¥æˆåŠŸ[/green]"
        )
        return day_stats

    except ssl.SSLError as e:
        logger.warning(f"SSL éŒ¯èª¤ï¼Œå°‡é‡è©¦: {e}")
        raise
    except requests.exceptions.RequestException as e:
        logger.warning(f"ç¶²çµ¡è«‹æ±‚éŒ¯èª¤ï¼Œå°‡é‡è©¦: {e}")
        raise
    except HttpError as e:
        if e.resp.status >= 500 or e.resp.status in [429, 403]:
            logger.warning(f"HTTP éŒ¯èª¤ {e.resp.status}ï¼Œå°‡é‡è©¦: {e}")
            raise
        else:
            logger.error(f"ä¸å¯é‡è©¦çš„ HTTP éŒ¯èª¤ {e.resp.status}: {e}")
            raise
    except Exception as e:
        # Handle specific NoneType close() errors
        if "'NoneType' object has no attribute 'close'" in str(e):
            logger.warning(f"Connection cleanup error (will retry): {e}")
            raise  # This is retryable
        elif _is_retryable_error(e):
            logger.warning(f"å¯é‡è©¦çš„éŒ¯èª¤: {e}")
            raise
        else:
            logger.error(f"ä¸å¯é‡è©¦çš„éŒ¯èª¤: {e}")
            raise


def _print_final_summary(total_stats: Dict[str, int]):
    """æ‰“å°æœ€çµ‚çš„åŒæ­¥çµæœæ‘˜è¦ã€‚"""
    table = Table(title="ğŸ“Š åŒæ­¥ä½œæ¥­æ‘˜è¦", show_header=True, header_style="bold magenta")
    table.add_column("é …ç›®", style="dim")
    table.add_column("æ•¸é‡", justify="right")

    table.add_row(
        "è¨ˆåŠƒå‰è·³éçš„ä»»å‹™", f"[yellow]{total_stats.get('tasks_pre_skipped', 0):,}[/yellow]"
    )
    table.add_row(
        "åŸ·è¡Œä¸­è·³éçš„ä»»å‹™", f"[yellow]{total_stats.get('tasks_skipped_runtime', 0):,}[/yellow]"
    )
    table.add_row("æ’å…¥è¨˜éŒ„", f"[green]{total_stats.get('inserted', 0):,}[/green]")
    table.add_row("æ›´æ–°è¨˜éŒ„", f"[blue]{total_stats.get('updated', 0):,}[/blue]")
    # 'skipped' ç¾åœ¨åªä»£è¡¨åœ¨ä¿å­˜æ•¸æ“šå¡Šæ™‚è·³éçš„è¨˜éŒ„ï¼Œé€šå¸¸ç‚º 0
    # table.add_row("è·³éè¨˜éŒ„ (å„²å­˜å±¤)", f"[yellow]{total_stats.get('skipped', 0):,}[/yellow]")
    table.add_row("å¤±æ•—ä»»å‹™", f"[red]{total_stats.get('failed', 0):,}[/red]")

    console.print(table)


def run_sync(
    db: Database,
    client: GSCClient,
    all_sites: bool,
    site_id: Optional[int],
    start_date: Optional[str],
    end_date: Optional[str],
    days: int,
    sync_mode: SyncMode,
    max_workers: int,
):
    """åŸ·è¡Œ GSC æ•¸æ“šçš„æ¯æ—¥åŒæ­¥ä½œæ¥­ï¼Œæ¡ç”¨è¨ˆåŠƒ-åŸ·è¡Œæ¨¡å¼ã€‚"""
    # ================= 1. æº–å‚™éšæ®µ =================
    logger.info("æ­£åœ¨æº–å‚™åŒæ­¥ä½œæ¥­...")

    # ç²å–è¦åŒæ­¥çš„ç¶²ç«™åˆ—è¡¨
    sites_to_sync: List[Dict[str, Any]] = []
    if all_sites:
        sites_to_sync = db.get_sites(active_only=True)
    elif site_id:
        site = db.get_site_by_id(site_id)
        if site:
            sites_to_sync.append(site)

    if not sites_to_sync:
        logger.warning("æ‰¾ä¸åˆ°ä»»ä½•è¦åŒæ­¥çš„æ´»å‹•ç«™é»ã€‚")
        return

    # ç¢ºå®šæ—¥æœŸç¯„åœ
    if start_date and end_date:
        s_date = datetime.strptime(start_date, "%Y-%m-%d").date()
        e_date = datetime.strptime(end_date, "%Y-%m-%d").date()
    else:
        today = datetime.now().date()
        e_date = today - timedelta(days=2)  # GSCæ•¸æ“šå»¶é²ç´„2å¤©
        s_date = e_date - timedelta(days=days - 1)

    date_list = [
        (s_date + timedelta(days=i)).strftime("%Y-%m-%d") for i in range((e_date - s_date).days + 1)
    ]

    # ================= 2. è¨ˆåŠƒéšæ®µ =================
    all_possible_tasks = [(site, date) for site in sites_to_sync for date in date_list]
    tasks_to_run = all_possible_tasks
    all_possible_task_count = len(sites_to_sync) * len(date_list)

    if sync_mode == SyncMode.SKIP:
        logger.info("SKIP æ¨¡å¼ï¼šæ­£åœ¨æª¢æŸ¥æ•¸æ“šåº«ä¸­å·²å­˜åœ¨çš„æ•¸æ“š...")
        site_ids = [s["id"] for s in sites_to_sync]
        existing_data_days = db.get_existing_data_days_for_sites(
            site_ids, date_list[0], date_list[-1]
        )
        tasks_to_run = [
            (site, date)
            for site, date in all_possible_tasks
            if (site["id"], date) not in existing_data_days
        ]

    tasks_pre_skipped = all_possible_task_count - len(tasks_to_run)
    logger.info(
        f"è¨ˆåŠƒåŒæ­¥ {len(sites_to_sync)} å€‹ç«™é»ï¼Œ {len(date_list)} å¤©ã€‚"
        f"ç¸½ä»»å‹™æ•¸: {all_possible_task_count}, å·²è·³é: {tasks_pre_skipped}, å°‡åŸ·è¡Œ: {len(tasks_to_run)}ã€‚"
    )

    if not tasks_to_run:
        logger.info("æ‰€æœ‰è¨ˆåŠƒçš„æ•¸æ“šéƒ½å·²å­˜åœ¨ï¼Œç„¡éœ€åŒæ­¥ã€‚")
        _print_final_summary(
            {
                "tasks_pre_skipped": all_possible_task_count,
                "tasks_skipped_runtime": 0,
                "inserted": 0,
                "updated": 0,
                "failed": 0,
            }
        )
        return

    # ================= 3. åŸ·è¡Œéšæ®µ =================
    # æ ¹æ“š GSC API æœ€ä½³å¯¦è¸ï¼Œä½¿ç”¨é †åºè™•ç†é¿å…ä¸¦è¡Œå•é¡Œ
    # API æ–‡ä»¶å»ºè­°é¿å…éåº¦ä½µç™¼ä»¥é˜²æ­¢ SSL å’Œé€Ÿç‡é™åˆ¶å•é¡Œ
    logger.info("ä½¿ç”¨é †åºè™•ç†æ¨¡å¼ï¼Œç¢ºä¿ GSC API èª¿ç”¨çš„ç©©å®šæ€§")

    total_stats = {
        "inserted": 0,
        "updated": 0,
        "skipped": 0,  # ç”¨æ–¼è¨˜éŒ„å„²å­˜å±¤çš„è·³éï¼Œé€šå¸¸ç‚º 0
        "failed": 0,
        "tasks_pre_skipped": tasks_pre_skipped,
        "tasks_skipped_runtime": 0,
    }

    progress_columns = [
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TextColumn("({task.completed}/{task.total})"),
        TimeElapsedColumn(),
        TimeRemainingColumn(),
    ]

    with Progress(*progress_columns, console=console) as progress:
        task_id = progress.add_task("[bold green]é †åºåŒæ­¥ä¸­...", total=len(tasks_to_run))

        # é †åºè™•ç†æ¯å€‹ä»»å‹™ï¼Œé¿å…ä¸¦è¡Œå°è‡´çš„ API é™åˆ¶å’Œ SSL éŒ¯èª¤
        for idx, (site, date) in enumerate(tasks_to_run):
            # æ›´æ–°ä»»å‹™æè¿°ï¼Œé¡¯ç¤ºç•¶å‰æ­£åœ¨è™•ç†çš„ç¶²ç«™å’Œæ—¥æœŸ
            progress.update(
                task_id,
                description=f"[bold green]åŒæ­¥ä¸­... [cyan]{site['name']}[/cyan] - [yellow]{date}[/yellow]",
            )

            try:
                # åœ¨æ§åˆ¶å°é¡¯ç¤ºå³æ™‚ç‹€æ…‹
                console.print(
                    f"[dim]é–‹å§‹åŒæ­¥:[/dim] {site['name']} ({site['domain']}) - {date}",
                    highlight=False,
                )

                day_stats = _sync_single_day(db, client, site, date, sync_mode)
                for key in total_stats:
                    if key in day_stats:
                        total_stats[key] += day_stats[key]

                # é¡¯ç¤ºæœ¬æ¬¡åŒæ­¥çµæœ
                if day_stats.get("inserted", 0) > 0 or day_stats.get("updated", 0) > 0:
                    console.print(
                        format_success_message(
                            f"å®Œæˆ: æ–°å¢ {day_stats.get('inserted', 0)} ç­†, "
                            f"æ›´æ–° {day_stats.get('updated', 0)} ç­†"
                        ),
                        highlight=False,
                    )
                else:
                    console.print(format_warning_message("å·²å­˜åœ¨æˆ–ç„¡æ•¸æ“š"), highlight=False)

            except RetryError as e:
                logger.error(
                    f"åŒæ­¥å¤±æ•— [ç«™é»: {site['name']}, æ—¥æœŸ: {date}]: "
                    f"å¤šæ¬¡å˜—è©¦å¾Œä»ç„¶å¤±æ•—ã€‚æœ€å¾Œä¸€æ¬¡éŒ¯èª¤: {e.last_attempt.exception()}"
                )
                console.print(
                    format_error_message(f"å¤±æ•—: {str(e.last_attempt.exception())}"),
                    highlight=False,
                )
                total_stats["failed"] += 1
            except Exception as exc:
                logger.error(f"ä»»å‹™ {site['name']}-{date} ç”¢ç”Ÿæœªé æœŸçš„ä¾‹å¤–: {exc}")
                logger.debug(traceback.format_exc())
                console.print(format_error_message(f"éŒ¯èª¤: {str(exc)}"), highlight=False)
                total_stats["failed"] += 1

            progress.update(task_id, advance=1)

    _print_final_summary(total_stats)


class BulkDataSynchronizer:
    """Bulk data synchronizer wrapper class for dependency injection."""

    def __init__(self, db: Database, gsc_client: GSCClient):
        self.db = db
        self.gsc_client = gsc_client

    def run_sync(
        self,
        all_sites: bool = False,
        site_id: Optional[int] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        days: int = 2,
        sync_mode: SyncMode = SyncMode.OVERWRITE,
        max_workers: Optional[int] = None,  # ä¿ç•™åƒæ•¸ä»¥ä¿æŒå‘å¾Œå…¼å®¹æ€§ï¼Œä½†ä¸å†ä½¿ç”¨
    ):
        """Run the synchronization process."""
        return run_sync(
            db=self.db,
            client=self.gsc_client,
            all_sites=all_sites,
            site_id=site_id,
            start_date=start_date,
            end_date=end_date,
            days=days,
            sync_mode=sync_mode,
            max_workers=1,  # å¼·åˆ¶ä½¿ç”¨é †åºè™•ç†
        )
