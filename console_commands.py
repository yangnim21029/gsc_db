#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
GSC æ•¸æ“šå°å‡ºå·¥å…· (ç°¡åŒ–ç‰ˆ)
ç´”å‘½ä»¤è¡Œæ“ä½œï¼Œæ”¯æ´æ•¸æ“šåŒæ­¥ã€æ¯å°æ™‚æ•¸æ“šç­‰åŠŸèƒ½
"""

import argparse
import logging
from datetime import datetime, timedelta
from typing import Optional

# æ­£ç¢ºçš„æ¨¡å¡Šå°å…¥
from services.gsc_client import GSCClient
from services.database import Database

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('gsc_simple.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description='GSC æ•¸æ“šå°å‡ºå·¥å…· (CLI)')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # èªè­‰
    subparsers.add_parser('auth', help='é€²è¡Œ GSC API èªè­‰')

    # ç«™é»ç®¡ç†
    subparsers.add_parser('sites', help='åˆ—å‡ºæ‰€æœ‰ç«™é»')

    add_site_parser = subparsers.add_parser('add-site', help='æ·»åŠ ç«™é»åˆ°æ•¸æ“šåº«')
    add_site_parser.add_argument('site_url', nargs='?', help='ç«™é» URL')

    # æ•¸æ“šè¦†è“‹
    coverage_parser = subparsers.add_parser('coverage', help='é¡¯ç¤ºæ•¸æ“šè¦†è“‹æƒ…æ³')
    coverage_parser.add_argument('--site-id', type=int, help='ç«™é» ID')

    # æ•¸æ“šåŒæ­¥
    sync_parser = subparsers.add_parser('sync', help='åŒæ­¥æ•¸æ“š')
    sync_parser.add_argument('--site-url', help='ç«™é» URL')
    sync_parser.add_argument('--site-id', type=int, help='ç«™é» ID')
    sync_parser.add_argument('--start-date', help='é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)')
    sync_parser.add_argument('--end-date', help='çµæŸæ—¥æœŸ (YYYY-MM-DD)')
    sync_parser.add_argument('--force', action='store_true', help='å¼·åˆ¶é‡å»ºæ•¸æ“š')
    sync_parser.add_argument('--all-sites', action='store_true', help='åŒæ­¥æ‰€æœ‰ç«™é»')

    # é€²åº¦ç›£æ§ (ç°¡åŒ–ç‰ˆ)
    subparsers.add_parser('progress', help='é¡¯ç¤ºæœ€è¿‘ä»»å‹™')

    # æ¯å°æ™‚æ•¸æ“šç›¸é—œå‘½ä»¤
    hourly_sync_parser = subparsers.add_parser('hourly-sync', help='åŒæ­¥æ¯å°æ™‚æ•¸æ“š')
    hourly_sync_parser.add_argument('--site-url', help='ç«™é» URL')
    hourly_sync_parser.add_argument('--start-date', help='é–‹å§‹æ—¥æœŸ (é»˜èªæ˜¨å¤©)')
    hourly_sync_parser.add_argument('--end-date', help='çµæŸæ—¥æœŸ (é»˜èªä»Šå¤©)')

    hourly_summary_parser = subparsers.add_parser(
        'hourly-summary', help='é¡¯ç¤ºæ¯å°æ™‚æ•¸æ“šç¸½çµ')
    hourly_summary_parser.add_argument('--site-id', type=int, help='ç«™é» ID')
    hourly_summary_parser.add_argument('--date', help='æ—¥æœŸ (å¯é¸)')

    hourly_coverage_parser = subparsers.add_parser(
        'hourly-coverage', help='é¡¯ç¤ºæ¯å°æ™‚æ•¸æ“šè¦†è“‹æƒ…æ³')
    hourly_coverage_parser.add_argument('--site-id', type=int, help='ç«™é» ID')

    # API ç‹€æ…‹
    subparsers.add_parser('api-status', help='é¡¯ç¤º API ä½¿ç”¨ç‹€æ…‹')

    # æ—¥èªŒæŸ¥çœ‹
    logs_parser = subparsers.add_parser('logs', help='æŸ¥çœ‹åŒæ­¥æ—¥èªŒ')
    logs_parser.add_argument(
        '--lines',
        type=int,
        default=50,
        help='é¡¯ç¤ºè¡Œæ•¸ (é»˜èª50è¡Œ)')
    logs_parser.add_argument(
        '--error-only',
        action='store_true',
        help='åªé¡¯ç¤ºéŒ¯èª¤æ—¥èªŒ')

    # æ•¸æ“šå¯è¦–åŒ–
    plot_parser = subparsers.add_parser('plot', help='ç¹ªè£½æ•¸æ“šåœ–è¡¨')
    plot_parser.add_argument('--site-id', type=int, help='ç«™é» ID')
    plot_parser.add_argument(
        '--type',
        choices=[
            'clicks',
            'rankings',
            'coverage'],
        default='clicks',
        help='åœ–è¡¨é¡å‹')
    plot_parser.add_argument(
        '--days',
        type=int,
        default=30,
        help='å¤©æ•¸ç¯„åœ (é»˜èª30å¤©)')
    plot_parser.add_argument('--save', help='ä¿å­˜åœ–ç‰‡è·¯å¾‘ (å¯é¸)')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ 'python search_console_manager.py' ä¾†å•Ÿå‹•äº’å‹•å¼é¸å–®")
        return

    try:
        if args.command == 'auth':
            handle_auth()
        elif args.command == 'sites':
            show_sites()
        elif args.command == 'add-site':
            add_site(args.site_url)
        elif args.command == 'coverage':
            show_coverage(args.site_id)
        elif args.command == 'sync':
            handle_sync(args)
        elif args.command == 'progress':
            show_progress()
        elif args.command == 'hourly-sync':
            handle_hourly_sync(args)
        elif args.command == 'hourly-summary':
            show_hourly_summary(args.site_id, args.date)
        elif args.command == 'hourly-coverage':
            show_hourly_coverage(args.site_id)
        elif args.command == 'api-status':
            show_api_status()
        elif args.command == 'logs':
            show_logs(args)
        elif args.command == 'plot':
            plot_data(args)
        else:
            parser.print_help()

    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—ï¼š{e}")
        logger.error(f"Command execution failed: {e}")


def handle_auth():
    """è™•ç†èªè­‰"""
    gsc_client = GSCClient()

    if gsc_client.is_authenticated():
        print("âœ… å·²ç¶“èªè­‰æˆåŠŸ")
        return

    auth_url = gsc_client.get_auth_url()
    print("ğŸ”— è«‹è¨ªå•ä»¥ä¸‹ URL é€²è¡Œèªè­‰ï¼š")
    print(auth_url)
    print("\nèªè­‰å®Œæˆå¾Œï¼Œè«‹é‡æ–°é‹è¡Œæ­¤å‘½ä»¤æª¢æŸ¥ç‹€æ…‹")


def show_sites():
    """é¡¯ç¤ºç«™é»åˆ—è¡¨"""
    try:
        gsc_client = GSCClient()

        # å¾ GSC ç²å–ç«™é»
        gsc_sites = gsc_client.get_sites()
        print(f"ğŸ“Š GSC ä¸­çš„ç«™é» ({len(gsc_sites)} å€‹)ï¼š")
        for i, site in enumerate(gsc_sites, 1):
            print(f"  {i}. {site}")

        # å¾æ•¸æ“šåº«ç²å–ç«™é»
        database = Database()
        db_sites = database.get_sites()
        print(f"\nğŸ’¾ æ•¸æ“šåº«ä¸­çš„ç«™é» ({len(db_sites)} å€‹)ï¼š")
        for site in db_sites:
            print(f"  ID: {site['id']} - {site['name']} ({site['domain']})")

    except Exception as e:
        print(f"âŒ ç²å–ç«™é»å¤±æ•—ï¼š{e}")


def add_site(site_url: Optional[str] = None):
    """æ·»åŠ ç«™é»"""
    try:
        # å¦‚æœæ²’æœ‰æä¾› URLï¼Œäº’å‹•å¼è©¢å•
        if not site_url:
            print("\nğŸŒ æ·»åŠ æ–°ç«™é»")
            print("è«‹è¼¸å…¥ç«™é» URL (ä¾‹å¦‚: https://example.com/ æˆ– sc-domain:example.com)")
            site_url = input("ç«™é» URL: ").strip()

            if not site_url:
                print("âŒ å¿…é ˆæä¾›ç«™é» URL")
                return

        database = Database()
        site_name = site_url.replace(
            'sc-domain:',
            '').replace(
            'https://',
            '').replace(
            'http://',
            '')
        site_id = database.add_site(site_url, site_name)
        print(f"âœ… ç«™é»å·²æ·»åŠ ï¼šID {site_id}, URL: {site_url}")

    except Exception as e:
        print(f"âŒ æ·»åŠ ç«™é»å¤±æ•—ï¼š{e}")


def show_coverage(site_id: Optional[int] = None):
    """é¡¯ç¤ºæ•¸æ“šè¦†è“‹æƒ…æ³"""
    try:
        database = Database()

        with database.get_connection() as conn:
            if site_id:
                # ç‰¹å®šç«™é»è¦†è“‹æƒ…æ³
                query = '''
                    SELECT
                        s.id, s.name, s.domain,
                        MIN(dr.date) as earliest_date,
                        MAX(dr.date) as latest_date,
                        COUNT(DISTINCT dr.date) as total_days,
                        COUNT(dr.id) as total_records
                    FROM sites s
                    LEFT JOIN daily_rankings dr ON s.id = dr.site_id
                    WHERE s.id = ?
                    GROUP BY s.id
                '''
                result = conn.execute(query, (site_id,)).fetchone()

                if result:
                    site = dict(result)
                    print(f"ğŸ“Š ç«™é»æ•¸æ“šè¦†è“‹æƒ…æ³ï¼š{site['name']}")
                    print("-" * 50)
                    print(f"ç«™é» IDï¼š{site['id']}")
                    print(f"åŸŸåï¼š{site['domain']}")
                    print(
                        f"æ•¸æ“šç¯„åœï¼š{site['earliest_date']} è‡³ {site['latest_date']}")
                    print(f"æ¶µè“‹å¤©æ•¸ï¼š{site['total_days']}")
                    print(f"ç¸½è¨˜éŒ„æ•¸ï¼š{site['total_records']:,}")
                else:
                    print(f"âŒ æ‰¾ä¸åˆ°ç«™é» ID {site_id}")
            else:
                # æ‰€æœ‰ç«™é»æ¦‚æ³
                query = '''
                    SELECT
                        s.id, s.name, s.domain,
                        MIN(dr.date) as earliest_date,
                        MAX(dr.date) as latest_date,
                        COUNT(DISTINCT dr.date) as total_days,
                        COUNT(dr.id) as total_records
                    FROM sites s
                    LEFT JOIN daily_rankings dr ON s.id = dr.site_id
                    GROUP BY s.id
                    ORDER BY s.name
                '''
                sites = [dict(row) for row in conn.execute(query).fetchall()]

                print_coverage_table(sites)

    except Exception as e:
        print(f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{e}")


def print_coverage_table(sites: list):
    """æ‰“å°è¦†è“‹æƒ…æ³è¡¨æ ¼ - æå–é‡è¤‡ä»£ç¢¼"""
    print("ğŸ“Š æ‰€æœ‰ç«™é»æ•¸æ“šè¦†è“‹æƒ…æ³ï¼š")
    print("-" * 80)
    print(f"{'ID':<4} {'ç«™é»åç¨±':<20} {'æœ€æ—©æ—¥æœŸ':<12} {'æœ€æ–°æ—¥æœŸ':<12} {'å¤©æ•¸':<6} {'è¨˜éŒ„æ•¸':<10}")
    print("-" * 80)

    for site in sites:
        records = f"{site['total_records']:,}" if site['total_records'] else "0"
        print(
            f"{site['id']:<4} {site['name'][:19]:<20} {site['earliest_date'] or 'N/A':<12} "
            f"{site['latest_date'] or 'N/A':<12} {site['total_days'] or 0:<6} {records:<10}")


def handle_sync(args):
    """è™•ç†æ•¸æ“šåŒæ­¥"""
    try:
        gsc_client = GSCClient()
        database = Database()

        # è¨­ç½®æ—¥æœŸç¯„åœ
        end_date = args.end_date or datetime.now().strftime('%Y-%m-%d')
        start_date = args.start_date or (
            datetime.now() -
            timedelta(
                days=7)).strftime('%Y-%m-%d')

        if args.all_sites:
            # åŒæ­¥æ‰€æœ‰ç«™é»
            sites = database.get_sites()
            for site in sites:
                print(f"ğŸ”„ åŒæ­¥ç«™é»ï¼š{site['name']}")
                result = gsc_client.sync_site_data_enhanced(
                    site['domain'], start_date, end_date)
                print(
                    f"âœ… å®Œæˆï¼šé—œéµå­— {result['keyword_count']} æ¢ï¼Œé é¢ {result['page_count']} æ¢")

        elif args.site_url:
            # åŒæ­¥æŒ‡å®š URL çš„ç«™é»
            print(f"ğŸ”„ åŒæ­¥ç«™é»ï¼š{args.site_url}")
            result = gsc_client.sync_site_data_enhanced(
                args.site_url, start_date, end_date)
            print(
                f"âœ… å®Œæˆï¼šé—œéµå­— {result['keyword_count']} æ¢ï¼Œé é¢ {result['page_count']} æ¢")

        elif args.site_id:
            # åŒæ­¥æŒ‡å®š ID çš„ç«™é»
            sites = database.get_sites()
            site = next((s for s in sites if s['id'] == args.site_id), None)
            if site:
                print(f"ğŸ”„ åŒæ­¥ç«™é»ï¼š{site['name']}")
                result = gsc_client.sync_site_data_enhanced(
                    site['domain'], start_date, end_date)
                print(
                    f"âœ… å®Œæˆï¼šé—œéµå­— {result['keyword_count']} æ¢ï¼Œé é¢ {result['page_count']} æ¢")
            else:
                print(f"âŒ æ‰¾ä¸åˆ°ç«™é» ID {args.site_id}")
        else:
            print("âŒ è«‹æŒ‡å®šè¦åŒæ­¥çš„ç«™é» (--site-url, --site-id, æˆ– --all-sites)")

    except Exception as e:
        print(f"âŒ åŒæ­¥å¤±æ•—ï¼š{e}")


def show_progress():
    """é¡¯ç¤ºæœ€è¿‘ä»»å‹™ (ç°¡åŒ–ç‰ˆ)"""
    try:
        database = Database()
        tasks = database.get_recent_tasks(10)

        if not tasks:
            print("ğŸ“Š æ²’æœ‰ä»»å‹™è¨˜éŒ„")
            return

        print("ğŸ“Š æœ€è¿‘ä»»å‹™ï¼š")
        print("-" * 80)
        print(f"{'ID':<4} {'ç«™é»':<20} {'ä»»å‹™é¡å‹':<15} {'ç‹€æ…‹':<10} {'è¨˜éŒ„æ•¸':<8} {'å‰µå»ºæ™‚é–“':<20}")
        print("-" * 80)

        for task in tasks:
            created_at = task['created_at'][:19] if task['created_at'] else 'N/A'
            site_name = task['site_name'][:19] if task['site_name'] else 'N/A'
            print(
                f"{task['id']:<4} {site_name:<20} {task['task_type']:<15} "
                f"{task['status']:<10} {task['total_records']:<8} {created_at:<20}")

    except Exception as e:
        print(f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{e}")


def handle_hourly_sync(args):
    """è™•ç†æ¯å°æ™‚æ•¸æ“šåŒæ­¥"""
    site_url = args.site_url
    start_date = args.start_date or (
        datetime.now() -
        timedelta(
            days=1)).strftime('%Y-%m-%d')
    end_date = args.end_date or datetime.now().strftime('%Y-%m-%d')

    if not site_url:
        print("âŒ éœ€è¦æŒ‡å®šç«™é» URL")
        return

    print(f"ğŸ“Š é–‹å§‹åŒæ­¥æ¯å°æ™‚æ•¸æ“šï¼š{site_url}")
    print(f"æ—¥æœŸç¯„åœï¼š{start_date} è‡³ {end_date}")
    print("âš ï¸  æ¯å°æ™‚æ•¸æ“šåƒ…æä¾›æœ€è¿‘ 10 å¤©çš„è³‡æ–™")

    try:
        gsc_client = GSCClient()
        result = gsc_client.sync_hourly_data(site_url, start_date, end_date)
        print("âœ… å®ŒæˆåŒæ­¥ï¼")
        print(f"   - ä¿å­˜è¨˜éŒ„æ•¸ï¼š{result['hourly_count']}")
        print(f"   - ç¨ç‰¹é …ç›®æ•¸ï¼š{result['unique_items']}")
    except Exception as e:
        print(f"âŒ åŒæ­¥å¤±æ•—ï¼š{e}")


def show_hourly_summary(site_id: Optional[int], date: Optional[str] = None):
    """é¡¯ç¤ºæ¯å°æ™‚æ•¸æ“šç¸½çµ"""
    if site_id is None:
        print("âŒ éœ€è¦æŒ‡å®šç«™é» ID (ä½¿ç”¨ sites å‘½ä»¤æŸ¥çœ‹)")
        return

    try:
        database = Database()
        summary = database.get_hourly_summary(site_id, date)

        if not summary:
            print("ğŸ“Š æ²’æœ‰æ‰¾åˆ°æ¯å°æ™‚æ•¸æ“š")
            return

        print(f"ğŸ“Š æ¯å°æ™‚æ•¸æ“šç¸½çµ (ç«™é» ID: {site_id})")
        if date:
            print(f"æ—¥æœŸï¼š{date}")
        else:
            print("é¡¯ç¤ºæœ€è¿‘ 3 å¤©çš„æ•¸æ“š")

        print_hourly_summary_table(summary)

    except Exception as e:
        print(f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{e}")


def print_hourly_summary_table(summary: list):
    """æ‰“å°æ¯å°æ™‚ç¸½çµè¡¨æ ¼ - æå–é‡è¤‡ä»£ç¢¼"""
    print("-" * 80)
    print(f"{'æ—¥æœŸ':<12} {'å°æ™‚':<4} {'æŸ¥è©¢æ•¸':<8} {'é»æ“Š':<8} {'å±•ç¤º':<10} {'å¹³å‡æ’å':<8} {'CTR':<8}")
    print("-" * 80)

    for row in summary:
        ctr_percent = f"{row['avg_ctr']*100:.2f}%" if row['avg_ctr'] else "0%"
        avg_pos = f"{row['avg_position']:.1f}" if row['avg_position'] else "N/A"

        print(f"{row['date']:<12} {row['hour']:>2}h {row['query_count']:>6} "
              f"{row['total_clicks']:>6} {row['total_impressions']:>8} "
              f"{avg_pos:>7} {ctr_percent:>7}")


def show_hourly_coverage(site_id: Optional[int]):
    """é¡¯ç¤ºæ¯å°æ™‚æ•¸æ“šè¦†è“‹æƒ…æ³"""
    if site_id is None:
        print("âŒ éœ€è¦æŒ‡å®šç«™é» ID (ä½¿ç”¨ sites å‘½ä»¤æŸ¥çœ‹)")
        return

    try:
        database = Database()
        coverage = database.get_hourly_coverage(site_id)

        print(f"ğŸ“Š æ¯å°æ™‚æ•¸æ“šè¦†è“‹æƒ…æ³ (ç«™é» ID: {site_id})")
        print("-" * 50)

        if coverage.get('total_records', 0) > 0:
            print_hourly_coverage_details(coverage)
        else:
            print("âŒ æ²’æœ‰æ¯å°æ™‚æ•¸æ“š")
            print("ğŸ’¡ ä½¿ç”¨ 'hourly-sync' å‘½ä»¤é–‹å§‹æ”¶é›†æ¯å°æ™‚æ•¸æ“š")

    except Exception as e:
        print(f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{e}")


def print_hourly_coverage_details(coverage: dict):
    """æ‰“å°æ¯å°æ™‚è¦†è“‹è©³æƒ… - æå–é‡è¤‡ä»£ç¢¼"""
    print(f"ç¸½è¨˜éŒ„æ•¸ï¼š{coverage['total_records']:,}")
    print(f"æ•¸æ“šç¯„åœï¼š{coverage['first_date']} è‡³ {coverage['last_date']}")
    print(f"æ¶µè“‹å¤©æ•¸ï¼š{coverage['unique_dates']}")
    print(f"æ¶µè“‹å°æ™‚æ•¸ï¼š{coverage['unique_hours']}")

    print("\næœ€è¿‘ 24 å°æ™‚çš„æ•¸æ“šé»ï¼š")
    print(f"{'æ—¥æœŸ':<12} {'å°æ™‚':<6} {'è¨˜éŒ„æ•¸':<8}")
    print("-" * 26)

    for data in coverage.get('recent_data', []):
        print(f"{data['date']:<12} {data['hour']:>2}h {data['records']:>6}")


def show_api_status():
    """é¡¯ç¤º API ä½¿ç”¨ç‹€æ…‹"""
    print("ğŸ“Š GSC API ä½¿ç”¨ç‹€æ…‹")
    print("-" * 40)
    
    try:
        gsc_client = GSCClient()
        stats = gsc_client.get_api_usage_stats()
        
        print("ğŸ“ˆ ç•¶å‰ä½¿ç”¨æƒ…æ³ï¼š")
        print(f"  â€¢ ä»Šæ—¥è«‹æ±‚ï¼š{stats['requests_today']:,} / {stats['daily_limit']:,} ({stats['daily_usage_percent']:.2f}%)")
        print(f"  â€¢ æœ¬åˆ†é˜è«‹æ±‚ï¼š{stats['requests_this_minute']} / {stats['minute_limit']} ({stats['minute_usage_percent']:.2f}%)")
        print(f"  â€¢ ä»Šæ—¥å‰©é¤˜ï¼š{stats['daily_remaining']:,} æ¬¡è«‹æ±‚")
        print(f"  â€¢ æœ¬åˆ†é˜å‰©é¤˜ï¼š{stats['minute_remaining']} æ¬¡è«‹æ±‚")
        print()
        
        # ä½¿ç”¨é‡è­¦å‘Š
        if stats['daily_usage_percent'] > 80:
            print("ğŸš¨ è­¦å‘Šï¼šä»Šæ—¥APIä½¿ç”¨é‡å·²è¶…é80%ï¼")
        elif stats['daily_usage_percent'] > 50:
            print("âš ï¸  æ³¨æ„ï¼šä»Šæ—¥APIä½¿ç”¨é‡å·²è¶…é50%")
        else:
            print("âœ… APIä½¿ç”¨é‡æ­£å¸¸")
        print()
        
        # é¡¯ç¤ºæœ€è¿‘7å¤©çš„ä½¿ç”¨çµ±è¨ˆ
        print("ğŸ“… æœ€è¿‘7å¤©ä½¿ç”¨çµ±è¨ˆï¼š")
        try:
            with gsc_client.database.get_connection() as conn:
                cursor = conn.execute('''
                    SELECT date, requests_count 
                    FROM api_usage_stats 
                    WHERE date >= date('now', '-7 days')
                    ORDER BY date DESC
                ''')
                history = cursor.fetchall()
                
                if history:
                    for row in history:
                        percentage = (row['requests_count'] / 100000) * 100
                        print(f"  â€¢ {row['date']}: {row['requests_count']:,} æ¬¡ ({percentage:.2f}%)")
                else:
                    print("  â€¢ æš«ç„¡æ­·å²è¨˜éŒ„")
        except Exception as e:
            print(f"  â€¢ ç„¡æ³•è¼‰å…¥æ­·å²çµ±è¨ˆ: {e}")
        print()
        
    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•ç²å–APIä½¿ç”¨çµ±è¨ˆï¼š{e}")
        print()
        
    print("ğŸ“‹ API é™åˆ¶ï¼š")
    print("  â€¢ æ¯æ—¥è«‹æ±‚ï¼š100,000 æ¬¡")
    print("  â€¢ æ¯åˆ†é˜è«‹æ±‚ï¼š1,200 æ¬¡")
    print("  â€¢ æ¯æ¬¡è«‹æ±‚æœ€å¤§è¡Œæ•¸ï¼š1,000 è¡Œ")
    print("  â€¢ æ•¸æ“šå»¶é²ï¼š2-3 å¤©")
    print("  â€¢ æ¯å°æ™‚æ•¸æ“šç¯„åœï¼šæœ€å¤š 10 å¤©")
    print()
    print("ğŸ’¡ æœ€ä½³å¯¦è¸ï¼š")
    print("  â€¢ é¿å…é‡è¤‡æŠ“å–ç›¸åŒæ•¸æ“š")
    print("  â€¢ ä½¿ç”¨å¢é‡åŒæ­¥")
    print("  â€¢ æ‰¹æ¬¡è™•ç†å¤§é‡è«‹æ±‚")
    print("  â€¢ æ¯å°æ™‚æ•¸æ“šå»ºè­°ç­‰å¾… 2-3 å¤©å¾ŒæŸ¥è©¢")


def show_logs(args):
    """é¡¯ç¤ºåŒæ­¥æ—¥èªŒ"""
    import os

    try:
        log_files = ['gsc_simple.log', 'app.log']
        found_logs = [f for f in log_files if os.path.exists(f)]

        if not found_logs:
            print("âŒ æ‰¾ä¸åˆ°æ—¥èªŒæ–‡ä»¶")
            print("ğŸ’¡ å¯ç”¨çš„æ—¥èªŒæ–‡ä»¶ï¼šgsc_simple.log, app.log")
            return

        log_file = found_logs[0]
        print(f"ğŸ“‹ æŸ¥çœ‹æ—¥èªŒæ–‡ä»¶ï¼š{log_file}")
        print("-" * 80)

        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # åªé¡¯ç¤ºéŒ¯èª¤æ—¥èªŒ
        if args.error_only:
            error_lines = []
            for line in lines:
                if any(
                    keyword in line for keyword in [
                        'ERROR',
                        'Failed',
                        'failed',
                        'âŒ']):
                    error_lines.append(line)

            if not error_lines:
                print("âœ… æ²’æœ‰ç™¼ç¾éŒ¯èª¤æ—¥èªŒ")
                return
            else:
                print(f"ğŸš¨ æ‰¾åˆ° {len(error_lines)} æ¢éŒ¯èª¤æ—¥èªŒï¼š")
                lines = error_lines

        # é¡¯ç¤ºæœ€å¾ŒNè¡Œ
        display_lines = lines[-args.lines:] if len(
            lines) > args.lines else lines

        for line in display_lines:
            print(line.rstrip())

        print("\nğŸ“Š æ—¥èªŒçµ±è¨ˆï¼š")
        if args.error_only:
            print(f"  â€¢ éŒ¯èª¤è¡Œæ•¸ï¼š{len(display_lines)}")
        else:
            print(f"  â€¢ ç¸½è¡Œæ•¸ï¼š{len(lines)}")
            error_count = len([line for line in lines if any(
                keyword in line for keyword in ['ERROR', 'Failed', 'failed'])])
            print(f"  â€¢ éŒ¯èª¤è¡Œæ•¸ï¼š{error_count}")
            print(f"  â€¢ é¡¯ç¤ºè¡Œæ•¸ï¼š{len(display_lines)}")

    except Exception as e:
        print(f"âŒ è®€å–æ—¥èªŒå¤±æ•—ï¼š{e}")


def plot_data(args):
    """ç¹ªè£½æ•¸æ“šåœ–è¡¨"""
    try:
        import matplotlib.pyplot as plt
        from datetime import datetime, timedelta

        # è¨­ç½®ä¸­æ–‡å­—é«”
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS',
                                           'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

        database = Database()
        site_id = args.site_id

        # å¦‚æœæ²’æœ‰æä¾›ç«™é» IDï¼Œäº’å‹•å¼é¸æ“‡
        if not site_id:
            sites = database.get_sites()
            if not sites:
                print("âŒ æ•¸æ“šåº«ä¸­æ²’æœ‰ç«™é»ï¼Œè«‹å…ˆæ·»åŠ ç«™é»")
                return

            print("\nğŸ“Š é¸æ“‡è¦åˆ†æçš„ç«™é»:")
            for i, site in enumerate(sites, 1):
                print(f"  {i}. {site['name']} (ID: {site['id']})")

            try:
                choice = int(input(f"è«‹é¸æ“‡ç«™é» (1-{len(sites)}): ").strip())
                if 1 <= choice <= len(sites):
                    site_id = sites[choice - 1]['id']
                else:
                    print("âŒ ç„¡æ•ˆé¸æ“‡")
                    return
            except ValueError:
                print("âŒ è«‹è¼¸å…¥æ•¸å­—")
                return

        # è¨ˆç®—æ—¥æœŸç¯„åœ
        end_date = datetime.now()
        start_date = end_date - timedelta(days=args.days)
        start_str = start_date.strftime('%Y-%m-%d')
        end_str = end_date.strftime('%Y-%m-%d')

        with database.get_connection() as conn:
            if args.type == 'clicks':
                plot_clicks_trend(conn, site_id, start_str, end_str, args.save)
            elif args.type == 'rankings':
                plot_rankings_trend(
                    conn, site_id, start_str, end_str, args.save)
            elif args.type == 'coverage':
                plot_data_coverage(conn, site_id, args.save)

    except ImportError:
        print("âŒ éœ€è¦å®‰è£ matplotlibï¼špip install matplotlib")
    except Exception as e:
        print(f"âŒ ç¹ªåœ–å¤±æ•—ï¼š{e}")


def plot_clicks_trend(conn, site_id, start_date, end_date, save_path=None):
    """ç¹ªè£½é»æ“Šé‡è¶¨å‹¢åœ–"""
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    from datetime import datetime

    # æŸ¥è©¢æ¯æ—¥é»æ“Šé‡æ•¸æ“š
    query = '''
        SELECT date, SUM(clicks) as total_clicks, SUM(impressions) as total_impressions
        FROM daily_rankings
        WHERE site_id = ? AND date BETWEEN ? AND ?
        GROUP BY date
        ORDER BY date
    '''

    results = conn.execute(query, (site_id, start_date, end_date)).fetchall()

    if not results:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æ•¸æ“š")
        return

    dates = [datetime.strptime(row[0], '%Y-%m-%d') for row in results]
    clicks = [row[1] for row in results]
    impressions = [row[2] for row in results]

    # å‰µå»ºåœ–è¡¨
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

    # é»æ“Šé‡è¶¨å‹¢
    ax1.plot(dates, clicks, marker='o', linewidth=2, color='#1f77b4')
    ax1.set_title(
        f'æ¯æ—¥é»æ“Šé‡è¶¨å‹¢ (ç«™é» ID: {site_id})',
        fontsize=14,
        fontweight='bold')
    ax1.set_ylabel('é»æ“Šé‡', fontsize=12)
    ax1.grid(True, alpha=0.3)
    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))

    # å±•ç¤ºé‡è¶¨å‹¢
    ax2.plot(dates, impressions, marker='s', linewidth=2, color='#ff7f0e')
    ax2.set_title('æ¯æ—¥å±•ç¤ºé‡è¶¨å‹¢', fontsize=14, fontweight='bold')
    ax2.set_xlabel('æ—¥æœŸ', fontsize=12)
    ax2.set_ylabel('å±•ç¤ºé‡', fontsize=12)
    ax2.grid(True, alpha=0.3)
    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… åœ–è¡¨å·²ä¿å­˜åˆ°ï¼š{save_path}")
    else:
        plt.show()


def plot_rankings_trend(conn, site_id, start_date, end_date, save_path=None):
    """ç¹ªè£½æ’åè¶¨å‹¢åœ–"""
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    from datetime import datetime

    # æŸ¥è©¢å¹³å‡æ’åæ•¸æ“š
    query = '''
        SELECT date, AVG(position) as avg_position, COUNT(*) as keyword_count
        FROM daily_rankings
        WHERE site_id = ? AND date BETWEEN ? AND ? AND position > 0
        GROUP BY date
        ORDER BY date
    '''

    results = conn.execute(query, (site_id, start_date, end_date)).fetchall()

    if not results:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æ’åæ•¸æ“š")
        return

    dates = [datetime.strptime(row[0], '%Y-%m-%d') for row in results]
    avg_positions = [row[1] for row in results]
    keyword_counts = [row[2] for row in results]

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

    # å¹³å‡æ’åè¶¨å‹¢ (æ’åè¶Šä½è¶Šå¥½ï¼Œæ‰€ä»¥Yè»¸åè½‰)
    ax1.plot(dates, avg_positions, marker='o', linewidth=2, color='#2ca02c')
    ax1.set_title(f'å¹³å‡æ’åè¶¨å‹¢ (ç«™é» ID: {site_id})', fontsize=14, fontweight='bold')
    ax1.set_ylabel('å¹³å‡æ’å', fontsize=12)
    ax1.invert_yaxis()  # åè½‰Yè»¸ï¼Œæ’å1åœ¨é ‚éƒ¨
    ax1.grid(True, alpha=0.3)
    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))

    # é—œéµå­—æ•¸é‡è¶¨å‹¢
    ax2.plot(dates, keyword_counts, marker='s', linewidth=2, color='#d62728')
    ax2.set_title('æ¯æ—¥é—œéµå­—æ•¸é‡', fontsize=14, fontweight='bold')
    ax2.set_xlabel('æ—¥æœŸ', fontsize=12)
    ax2.set_ylabel('é—œéµå­—æ•¸é‡', fontsize=12)
    ax2.grid(True, alpha=0.3)
    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… åœ–è¡¨å·²ä¿å­˜åˆ°ï¼š{save_path}")
    else:
        plt.show()


def plot_data_coverage(conn, site_id, save_path=None):
    """ç¹ªè£½æ•¸æ“šè¦†è“‹æƒ…æ³"""
    import matplotlib.pyplot as plt
    from datetime import datetime

    # æŸ¥è©¢æ•¸æ“šè¦†è“‹æƒ…æ³
    query = '''
        SELECT date, COUNT(*) as record_count
        FROM daily_rankings
        WHERE site_id = ?
        GROUP BY date
        ORDER BY date
    '''

    results = conn.execute(query, (site_id,)).fetchall()

    if not results:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æ•¸æ“š")
        return

    dates = [datetime.strptime(row[0], '%Y-%m-%d') for row in results]
    record_counts = [row[1] for row in results]

    plt.figure(figsize=(12, 6))
    plt.bar(dates, record_counts, alpha=0.7, color='#9467bd')
    plt.title(f'æ•¸æ“šè¦†è“‹æƒ…æ³ (ç«™é» ID: {site_id})', fontsize=14, fontweight='bold')
    plt.xlabel('æ—¥æœŸ', fontsize=12)
    plt.ylabel('è¨˜éŒ„æ•¸é‡', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.xticks(rotation=45)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… åœ–è¡¨å·²ä¿å­˜åˆ°ï¼š{save_path}")
    else:
        plt.show()


if __name__ == "__main__":
    main()
