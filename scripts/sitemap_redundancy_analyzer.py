#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¿«é€Ÿ Sitemap å†—é¤˜åˆ†æå·¥å…·

å°ˆç‚ºå¿«é€Ÿã€å®Œæ•´åœ°åˆ†æ sitemap URL å†—é¤˜æƒ…æ³è€Œè¨­è¨ˆï¼Œå„ªåŒ–äº†è³‡æ–™åº«æŸ¥è©¢å’Œçµæœå‘ˆç¾ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ™ºèƒ½ Sitemap ç™¼ç¾ï¼ˆå¯é¸ï¼Œæˆ–è‡ªå‹•æª¢æ¸¬ï¼‰ã€‚
2. é«˜æ•ˆç²å– Sitemap ä¸­çš„æ‰€æœ‰ URLï¼ˆæ”¯æŒ sitemap ç´¢å¼•ï¼Œä¸¦ç™¼è™•ç†ï¼‰ã€‚
3. ä¸€æ¬¡æ€§æŸ¥è©¢è³‡æ–™åº«ä¸­æ‰€æœ‰ç›¸é—œé é¢ï¼Œé¿å…å¤šæ¬¡æŸ¥è©¢ã€‚
4. æ¸…æ™°çš„æ•¸æ“šè¦†è“‹æƒ…æ³å’Œå†—é¤˜åˆ†æå ±å‘Šã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
# è‡ªå‹•ç™¼ç¾ sitemap ä¸¦é€²è¡Œåˆ†æ
poetry run python scripts/sitemap_redundancy_analyzer.py --site-id 14 --days 30

# æ‰‹å‹•æŒ‡å®š sitemap URL
poetry run python scripts/sitemap_redundancy_analyzer.py --site-id 14 --sitemap-url "https://example.com/sitemap.xml" --days 30
"""

import argparse
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urljoin

import requests
from lxml import etree
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, track
from rich.prompt import Prompt
from rich.table import Table

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„æ·»åŠ åˆ° sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ruff: noqa: E402
from src.containers import Container
from src.services.database import Database

console = Console()

SITEMAP_NS = {"ns": "http://www.sitemaps.org/schemas/sitemap/0.9"}
COMMON_SITEMAP_PATHS = ["/sitemap.xml", "/sitemap_index.xml", "/sitemaps.xml"]
REQUEST_TIMEOUT_SHORT = 5
REQUEST_TIMEOUT_LONG = 30
MAX_WORKERS = 10  # ä¸¦ç™¼ä¸‹è¼‰çš„ç·šç¨‹æ•¸


class SitemapAnalyzer:
    """å¿«é€Ÿå†—é¤˜åˆ†æå™¨"""

    def __init__(self, db: Database):
        self.db = db
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (compatible; SitemapAnalyzer/1.0; +https://github.com/user/repo)"
            }
        )

    def discover_and_select_sitemap(self, domain: str, auto_select: bool = True) -> Optional[str]:
        """å¾ robots.txt å’Œå¸¸è¦‹è·¯å¾‘ç™¼ç¾ä¸¦é¸æ“‡ sitemap"""
        if not domain.startswith(("http://", "https://")):
            domain = f"https://{domain}"

        console.print(Panel.fit(f"[bold blue]æ™ºèƒ½ Sitemap ç™¼ç¾[/bold blue]\nç›®æ¨™åŸŸå: {domain}"))

        # 1. å¾ robots.txt ç™¼ç¾
        robots_url = urljoin(domain, "/robots.txt")
        sitemaps = []
        try:
            response = self.session.get(robots_url, timeout=REQUEST_TIMEOUT_SHORT)
            if response.status_code == 200:
                sitemap_pattern = re.compile(r"sitemap:\s*(.+)", re.IGNORECASE)
                sitemaps.extend([match.strip() for match in sitemap_pattern.findall(response.text)])
        except requests.RequestException as e:
            console.log(f"[yellow]ç„¡æ³•è®€å– {robots_url}: {e}[/yellow]")

        # 2. æª¢æŸ¥å¸¸è¦‹è·¯å¾‘
        for path in COMMON_SITEMAP_PATHS:
            sitemaps.append(urljoin(domain, path))

        # 3. é©—è­‰ä¸¦é¸æ“‡
        validated_sitemaps = []
        for url in track(set(sitemaps), description="é©—è­‰ Sitemap..."):
            try:
                response = self.session.head(
                    url, timeout=REQUEST_TIMEOUT_SHORT, allow_redirects=True
                )
                if (
                    response.status_code == 200
                    and "xml" in response.headers.get("content-type", "").lower()
                ):
                    validated_sitemaps.append(str(response.url))  # ä½¿ç”¨é‡å®šå‘å¾Œçš„æœ€çµ‚ URL
                    console.print(f"âœ… æ‰¾åˆ°æœ‰æ•ˆ Sitemap: {response.url}")
            except requests.RequestException:
                continue

        if not validated_sitemaps:
            console.print("[bold red]âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ sitemap[/bold red]")
            return None

        if auto_select or len(validated_sitemaps) == 1:
            selected = validated_sitemaps[0]
            console.print(f"[bold green]è‡ªå‹•é¸æ“‡: {selected}[/bold green]")
            return selected

        # è®“ç”¨æˆ¶é¸æ“‡
        table = Table(title="ç™¼ç¾çš„æœ‰æ•ˆ Sitemap")
        table.add_column("åºè™Ÿ", style="cyan")
        table.add_column("URL", style="blue")
        for i, sitemap_url in enumerate(validated_sitemaps, 1):
            table.add_row(str(i), sitemap_url)
        console.print(table)

        choice = Prompt.ask(
            "é¸æ“‡è¦ä½¿ç”¨çš„ sitemap (è¼¸å…¥åºè™Ÿ)",
            choices=[str(i) for i in range(1, len(validated_sitemaps) + 1)],
            default="1",
        )
        return validated_sitemaps[int(choice) - 1]

    def _fetch_and_parse_single_sitemap(self, url: str) -> List[str]:
        """ç²å–ä¸¦è§£æå–®å€‹ sitemap æˆ– sitemap ç´¢å¼•ï¼Œè¿”å› URL åˆ—è¡¨"""
        try:
            response = self.session.get(url, timeout=REQUEST_TIMEOUT_LONG)
            response.raise_for_status()  # æ‹‹å‡º HTTP éŒ¯èª¤
            root = etree.fromstring(response.content, parser=etree.XMLParser(recover=True))

            # æª¢æŸ¥æ˜¯ sitemap ç´¢å¼•é‚„æ˜¯ url é›†åˆ
            if root.tag.endswith("sitemapindex"):
                xpath_expr = "//ns:sitemap/ns:loc"
            elif root.tag.endswith("urlset"):
                xpath_expr = "//ns:url/ns:loc"
            else:
                # å¯èƒ½æ˜¯æ²’æœ‰ namespace çš„ sitemap
                if root.tag == "sitemapindex":
                    xpath_expr = "//sitemap/loc"
                else:
                    xpath_expr = "//url/loc"

            # æå–æ–‡æœ¬ä¸¦éæ¿¾æ‰ None æˆ–ç©ºå­—ä¸²
            return [
                elem.text for elem in root.xpath(xpath_expr, namespaces=SITEMAP_NS) if elem.text
            ]

        except requests.HTTPError as e:
            console.log(f"[red]HTTP éŒ¯èª¤ {e.response.status_code} æ–¼ {url}[/red]")
        except Exception as e:
            console.log(f"[red]è™•ç† {url} å¤±æ•—: {e}[/red]")
        return []

    def fetch_sitemap_urls(self, sitemap_url: str) -> List[str]:
        """é«˜æ•ˆç²å–ä¸¦è§£æ Sitemapï¼Œæå–æ‰€æœ‰ URLï¼Œæ”¯æŒä¸¦ç™¼è™•ç†ç´¢å¼•"""
        console.print(f"\nğŸ” æ­£åœ¨ç²å– Sitemap: {sitemap_url}")

        initial_urls = self._fetch_and_parse_single_sitemap(sitemap_url)
        if not initial_urls:
            return []

        # åˆ¤æ–·æ˜¯ sitemap ç´¢å¼•é‚„æ˜¯æ™®é€šçš„ sitemap
        if initial_urls[0].endswith(".xml"):
            console.print(
                f"ğŸ“„ æª¢æ¸¬åˆ° Sitemap ç´¢å¼•ï¼ŒåŒ…å« {len(initial_urls)} å€‹å­ sitemapï¼Œé–‹å§‹ä¸¦ç™¼è™•ç†..."
            )
            all_urls = []
            with Progress(console=console) as progress:
                task = progress.add_task("[cyan]è§£æå­ Sitemap...", total=len(initial_urls))
                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    future_to_url = {
                        executor.submit(self._fetch_and_parse_single_sitemap, url): url
                        for url in initial_urls
                    }
                    for future in as_completed(future_to_url):
                        try:
                            urls_from_sub = future.result()
                            all_urls.extend(urls_from_sub)
                        except Exception as e:
                            url = future_to_url[future]
                            console.log(f"[red]ç²å–å­ sitemap {url} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}[/red]")
                        progress.update(task, advance=1)
            urls = all_urls
        else:
            urls = initial_urls

        console.print("\nğŸ“Š [bold]Sitemap URL æå–çµæœ[/bold]")
        console.print(f"   ğŸ¯ Sitemap ç¸½ URL æ•¸: {len(urls):,} å€‹")
        console.print(f"   ğŸ“„ ä¾†æº: {sitemap_url}")
        return urls

    def get_db_pages_and_coverage(self, site_id: int, days: Optional[int]) -> Tuple[Set[str], dict]:
        """ä¸€æ¬¡æ€§ç²å–è³‡æ–™åº«ä¸­çš„ä¸é‡è¤‡é é¢åŠæ•¸æ“šè¦†è“‹æƒ…æ³"""

        console.print("\nğŸ” æ­£åœ¨æŸ¥è©¢è³‡æ–™åº«...")

        date_clause = ""
        time_range_text = "å…¨éƒ¨æ™‚é–“"

        if days:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            date_clause = "AND date BETWEEN ? AND ?"
            params = [site_id, start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")]
            time_range_text = f"æœ€è¿‘ {days} å¤©"
        else:
            params = [site_id]

        # æŸ¥è©¢æ•¸æ“šè¦†è“‹æƒ…æ³
        coverage_query = f"""
            SELECT date, COUNT(*) FROM gsc_performance_data
            WHERE site_id = ? {date_clause}
            GROUP BY date ORDER BY date DESC
        """
        with self.db._lock:
            coverage_results = self.db._connection.execute(coverage_query, params).fetchall()

        actual_days = len(coverage_results)
        total_records = sum(row[1] for row in coverage_results)

        console.print(f"ğŸ“Š [bold]è³‡æ–™åº«æ•¸æ“šè¦†è“‹æƒ…æ³[/bold] (æŸ¥è©¢ç¯„åœ: {time_range_text})")
        if days and days > 0:
            console.print(
                f"   âœ… å¯¦éš›æœ‰æ•¸æ“šå¤©æ•¸: {actual_days} å¤© ({actual_days / days * 100:.1f}%)"
            )
        else:
            console.print(f"   âœ… å¯¦éš›æœ‰æ•¸æ“šå¤©æ•¸: {actual_days} å¤©")

        if actual_days > 0:
            console.print(
                f"   ğŸ“… æ•¸æ“šæ—¥æœŸç¯„åœ: {coverage_results[-1][0]} åˆ° {coverage_results[0][0]}"
            )
        console.print(f"   ğŸ“ˆ ç¸½æ•¸æ“šé»è¨˜éŒ„æ•¸: {total_records:,} ç­†")

        # æŸ¥è©¢ä¸é‡è¤‡é é¢
        pages_query = (
            f"SELECT DISTINCT page FROM gsc_performance_data WHERE site_id = ? {date_clause}"
        )
        with self.db._lock:
            page_results = self.db._connection.execute(pages_query, params).fetchall()

        db_pages = {row[0] for row in page_results}
        console.print(f"   ğŸ¯ æœ‰æ•¸æ“šçš„ç¨ç«‹é é¢URLæ•¸: {len(db_pages):,} å€‹")

        coverage_info = {
            "queried_days": time_range_text,
            "actual_days": actual_days,
            "db_unique_pages": len(db_pages),
        }
        return db_pages, coverage_info

    def analyze_and_display(
        self, sitemap_urls: List[str], db_pages: Set[str], site_info: Dict, coverage_info: Dict
    ):
        """åˆ†æå†—é¤˜ä¸¦é¡¯ç¤ºçµæœ"""
        console.print("\nğŸ” æ­£åœ¨é€²è¡Œå†—é¤˜åˆ†æ...")

        sitemap_set = set(sitemap_urls)

        urls_in_db = sitemap_set.intersection(db_pages)
        urls_not_in_db = sitemap_set - db_pages

        # å‰µå»ºçµæœé¢æ¿
        title = f"[bold]Sitemap å†—é¤˜åˆ†æå ±å‘Š for {site_info.get('name', 'æœªçŸ¥')}[/bold]"
        table = Table(title=title, show_header=False, box=None, padding=(0, 2))
        table.add_column(style="cyan")
        table.add_column(style="magenta")

        table.add_row("\n[bold]Sitemap çµ±è¨ˆ[/bold]", "")
        table.add_row("Sitemap ç¸½ URL æ•¸", f"{len(sitemap_urls):,}")
        table.add_row("å»é‡å¾Œç¨ç«‹ URL æ•¸", f"{len(sitemap_set):,}")

        table.add_row("\n[bold]è³‡æ–™åº«çµ±è¨ˆ[/bold]", f"({coverage_info['queried_days']})")
        table.add_row("æœ‰æ•¸æ“šçš„ç¨ç«‹ URL æ•¸", f"{coverage_info['db_unique_pages']:,}")

        table.add_row("\n[bold]å†—é¤˜åˆ†æ (Sitemap vs è³‡æ–™åº«)[/bold]", "")
        table.add_row("âœ… æœ‰æ•¸æ“šçš„ Sitemap URL", f"{len(urls_in_db):,}")
        table.add_row("âŒ ç„¡æ•¸æ“šçš„ Sitemap URL", f"{len(urls_not_in_db):,}")

        if len(sitemap_set) > 0:
            redundancy_rate = len(urls_not_in_db) / len(sitemap_set) * 100
            coverage_rate = len(urls_in_db) / len(sitemap_set) * 100

            # æ ¹æ“šå†—é¤˜ç‡é¡¯ç¤ºä¸åŒé¡è‰²çš„çµæœ
            color = "green"
            if redundancy_rate > 50:
                color = "red"
            elif redundancy_rate > 20:
                color = "yellow"

            table.add_row(
                f"[{color}]å†—é¤˜ç‡ (ç„¡æ•¸æ“šçš„ URL ä½”æ¯”)[/{color}]",
                f"[{color}]{redundancy_rate:.1f}%[/{color}]",
            )
            table.add_row(
                "[green]è¦†è“‹ç‡ (æœ‰æ•¸æ“šçš„ URL ä½”æ¯”)[/green]", f"[green]{coverage_rate:.1f}%[/green]"
            )

        console.print(Panel(table, expand=False))


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description="å¿«é€Ÿ Sitemap å†—é¤˜åˆ†æå·¥å…·")
    parser.add_argument("--site-id", type=int, required=True, help="è¦åˆ†æçš„ç¶²ç«™ ID")
    parser.add_argument(
        "--sitemap-url", type=str, action="append", help="æ‰‹å‹•æŒ‡å®š Sitemap URLï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰"
    )
    parser.add_argument("--days", type=int, help="æŸ¥è©¢å¤©æ•¸ç¯„åœï¼ˆå¯é¸ï¼Œé è¨­æŸ¥è©¢å…¨éƒ¨æ™‚é–“ï¼‰")
    parser.add_argument(
        "--interactive-discovery", action="store_true", help="å¼·åˆ¶é€²è¡Œäº¤äº’å¼ Sitemap é¸æ“‡"
    )

    args = parser.parse_args()

    console.print(Panel.fit("[bold blue]å¿«é€Ÿ Sitemap å†—é¤˜åˆ†æå·¥å…·[/bold blue]"))

    try:
        container = Container()
        db = container.database()
        console.print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
    except Exception as e:
        console.print(f"[bold red]éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥è³‡æ–™åº«: {e}[/bold red]")
        sys.exit(1)

    site_info = db.get_site_by_id(args.site_id)
    if not site_info:
        console.print(f"[bold red]éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° ID ç‚º {args.site_id} çš„ç«™é»[/bold red]")
        sys.exit(1)

    analyzer = SitemapAnalyzer(db)

    # ç²å– Sitemap URL
    sitemap_urls_to_fetch = args.sitemap_url
    if not sitemap_urls_to_fetch:
        domain = site_info["domain"].replace("sc-domain:", "")
        # å¦‚æœä½¿ç”¨ --interactive-discoveryï¼Œå‰‡ auto_select ç‚º False
        discovered_url = analyzer.discover_and_select_sitemap(
            domain, auto_select=not args.interactive_discovery
        )
        if discovered_url:
            sitemap_urls_to_fetch = [discovered_url]
        else:
            sys.exit(1)

    all_sitemap_urls = []
    for url in sitemap_urls_to_fetch:
        all_sitemap_urls.extend(analyzer.fetch_sitemap_urls(url))

    if not all_sitemap_urls:
        console.print("[bold yellow]æœªèƒ½å¾æŒ‡å®šçš„ Sitemap ä¸­æå–ä»»ä½• URLã€‚[/bold yellow]")
        sys.exit(0)

    db_pages, coverage_info = analyzer.get_db_pages_and_coverage(args.site_id, args.days)

    analyzer.analyze_and_display(all_sitemap_urls, db_pages, site_info, coverage_info)


if __name__ == "__main__":
    main()
