#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import concurrent.futures
import json
import logging
import os

# å°å…¥ç›¸é—œæ¨¡å¡Š
import sys
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import Flow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from .database import Database
from .hourly_data import HourlyDataHandler

sys.path.append(os.path.dirname(__file__))
import threading
import xml.etree.ElementTree as ET

import requests
import typer

from .. import config
from ..utils.rich_console import console

logger = logging.getLogger(__name__)


class GSCClient:
    def __init__(self, db: Database):
        """åˆå§‹åŒ– GSC å®¢æˆ¶ç«¯"""
        self.scopes = config.GSC_SCOPES
        self.credentials_path = str(config.get_credentials_path())
        self.client_config_path = str(config.CLIENT_SECRET_PATH)

        # åˆå§‹åŒ–ç‹€æ…‹
        self.credentials: Optional[Credentials] = None
        self.service: Optional[Any] = None
        self.database = db

        # API ä½¿ç”¨è¨ˆæ•¸å™¨
        self.api_requests_today = 0
        self.api_requests_this_minute = 0
        self.last_minute_reset = datetime.now().replace(second=0, microsecond=0)
        self.today = datetime.now().date()
        self._api_lock = threading.Lock()

        # å¾æ•¸æ“šåº«è¼‰å…¥ä»Šæ—¥APIä½¿ç”¨è¨ˆæ•¸
        self._load_api_usage_from_db()

        # å˜—è©¦åŠ è¼‰å·²æœ‰æ†‘è­‰ï¼Œå¦‚æœæˆåŠŸå‰‡åˆå§‹åŒ– service
        if self._load_credentials():
            self.service = build("searchconsole", "v1", credentials=self.credentials)

        # OAuth é…ç½®
        self.client_config = self._load_client_config()
        current_port = os.getenv("PORT", "8000")
        if "web" in self.client_config:
            self.client_config["web"]["redirect_uris"] = [
                f"http://localhost:{current_port}/auth/callback"
            ]

        # åˆå§‹åŒ–æ¯å°æ™‚æ•¸æ“šè™•ç†å™¨
        self.hourly_handler = None
        if self.service:
            self.hourly_handler = HourlyDataHandler(self.service, self.database)

    def authenticate(self) -> bool:
        """
        åŸ·è¡Œå®Œæ•´çš„èªè­‰æµç¨‹ã€‚

        1. æª¢æŸ¥ç¾æœ‰æ†‘è­‰æ˜¯å¦æœ‰æ•ˆã€‚
        2. å¦‚æœç„¡æ•ˆï¼Œå•Ÿå‹•æ§åˆ¶å° OAuth æµç¨‹ã€‚
        3. ç²å–æ–°çš„æ†‘è­‰ä¸¦ä¿å­˜ã€‚

        Returns:
            å¦‚æœæœ€çµ‚èªè­‰æˆåŠŸå‰‡è¿”å› Trueï¼Œå¦å‰‡è¿”å› Falseã€‚
        """
        if self.is_authenticated():
            console.print("âœ… å·²ä½¿ç”¨ç¾æœ‰æœ‰æ•ˆæ†‘è­‰é€²è¡Œèªè­‰ã€‚")
            return True

        console.print("ğŸš€ [bold yellow]éœ€è¦æ–°çš„èªè­‰ï¼Œå•Ÿå‹• OAuth2 æµç¨‹...[/bold yellow]")
        auth_url = self.get_auth_url()

        console.print("\n1. è«‹å°‡ä»¥ä¸‹ URL è¤‡è£½åˆ°æ‚¨çš„ç€è¦½å™¨ä¸­æ‰“é–‹ï¼Œä¸¦ç™»å…¥æ‚¨çš„ Google å¸³æˆ¶é€²è¡Œæˆæ¬Šï¼š")
        console.print(f"\n[link={auth_url}]{auth_url}[/link]\n")

        console.print(
            "2. æˆæ¬Šå¾Œï¼Œæ‚¨å°‡è¢«é‡å®šå‘åˆ°ä¸€å€‹ç„¡æ³•æ‰“é–‹çš„é é¢ (é€™æ˜¯æ­£å¸¸çš„)ã€‚"
            "è«‹å¾è©²é é¢çš„ç€è¦½å™¨åœ°å€æ¬„ä¸­ï¼Œè¤‡è£½ `code=` å¾Œé¢çš„æ‰€æœ‰å…§å®¹ã€‚"
        )

        auth_code = typer.prompt("3. è«‹åœ¨æ­¤è™•è²¼ä¸Šæ‚¨è¤‡è£½çš„æˆæ¬Šç¢¼ (code)")

        if not auth_code:
            console.print("[bold red]âŒ æœªæä¾›æˆæ¬Šç¢¼ï¼Œèªè­‰å·²å–æ¶ˆã€‚[/bold red]")
            return False

        console.print("\nâ³ [cyan]æ­£åœ¨ä½¿ç”¨æˆæ¬Šç¢¼æ›å–æ†‘è­‰...[/cyan]")
        if self.handle_oauth_callback(auth_code.strip()):
            console.print("[bold green]âœ… èªè­‰æˆåŠŸï¼æ†‘è­‰å·²ä¿å­˜ã€‚[/bold green]")
            return True
        else:
            console.print("[bold red]âŒ èªè­‰å¤±æ•—ã€‚è«‹æª¢æŸ¥æ‚¨çš„æˆæ¬Šç¢¼æˆ–é…ç½®ã€‚[/bold red]")
            return False

    def stream_site_data(self, site_url: str, start_date: str, end_date: str):
        """
        ä»¥æµå¼æ–¹å¼å¾ GSC API ç²å–æ•¸æ“šï¼Œä½œç‚ºä¸€å€‹ generatorã€‚
        å„ªåŒ–ï¼šéæ­· search_typeï¼Œä½†åœ¨å–®å€‹è«‹æ±‚ä¸­åŒ…å« device ç¶­åº¦ï¼Œä»¥æ¸›å°‘è«‹æ±‚ç¸½æ•¸ã€‚
        """
        if not self.is_authenticated() or not self.service:
            raise Exception("GSC service not initialized")

        search_types = ["web", "image", "video", "news", "discover", "googleNews"]

        for search_type in search_types:
            start_row = 0
            while True:
                try:
                    request_body = {
                        "startDate": start_date,
                        "endDate": end_date,
                        "dimensions": ["page", "query", "device"],
                        "rowLimit": 25000,
                        "startRow": start_row,
                        "type": search_type,
                    }

                    response = (
                        self.service.searchanalytics()
                        .query(siteUrl=site_url, body=request_body)
                        .execute()
                    )

                    self._track_api_request()

                    rows = response.get("rows", [])
                    if not rows:
                        break

                    device_chunks: Dict[str, List[Dict[str, Any]]] = {}
                    for row in rows:
                        dimensions = request_body["dimensions"]
                        row_keys = row["keys"]
                        # ç¢ºä¿é¡å‹æ­£ç¢º
                        keys = dict(zip(dimensions, row_keys)) if dimensions and row_keys else {}  # type: ignore  # type: ignore
                        device = keys.get("device", "N/A")

                        if device not in device_chunks:
                            device_chunks[device] = []

                        processed_row = {
                            "page": keys.get("page"),
                            "query": keys.get("query"),
                            "clicks": row.get("clicks"),
                            "impressions": row.get("impressions"),
                            "ctr": row.get("ctr"),
                            "position": row.get("position"),
                        }
                        device_chunks[device].append(processed_row)

                    for device, chunk in device_chunks.items():
                        yield device, search_type, chunk

                    if len(rows) < 25000:
                        break

                    start_row += 25000

                except HttpError as e:
                    if e.resp.status in [404, 403, 400]:
                        logger.warning(
                            f"No data or unsupported type for {site_url} with type "
                            f"{search_type} for date {start_date} (HTTP {e.resp.status}). Skipping."
                        )
                        break
                    logger.error(f"HTTP error for type {search_type}: {e}", exc_info=True)
                    raise
                except Exception as e:
                    logger.error(f"Unexpected error for type {search_type}: {e}", exc_info=True)
                    raise

    def stream_hourly_data(self, site_url: str, start_date: str, end_date: str):
        """
        ä»¥æµå¼æ–¹å¼å¾ GSC API ç²å–æ¯å°æ™‚çš„è©³ç´°æ•¸æ“šã€‚
        æ­¤æ–¹æ³•æ ¹æ“š 2025/04/09 çš„å®˜æ–¹æ–‡ä»¶é€²è¡Œäº†ä¿®æ­£ã€‚
        """
        if not self.is_authenticated() or not self.service:
            raise Exception("GSC service not initialized")

        start_row = 0
        while True:
            try:
                # æ ¹æ“šå®˜æ–¹æ–‡ä»¶ä¿®æ­£ request_body
                request_body = {
                    "startDate": start_date,
                    "endDate": end_date,
                    # é—œéµä¿®æ­£ 1: dimensions å¿…é ˆåŒ…å« 'HOUR'
                    "dimensions": ["HOUR", "query", "page", "device"],
                    # é—œéµä¿®æ­£ 2: dataState å¿…é ˆæ˜¯ 'HOURLY_ALL'
                    "dataState": "HOURLY_ALL",
                    "rowLimit": 25000,
                    "startRow": start_row,
                }

                response = (
                    self.service.searchanalytics()
                    .query(siteUrl=site_url, body=request_body)
                    .execute()
                )

                self._track_api_request()

                rows = response.get("rows", [])
                if not rows:
                    break

                for row in rows:
                    dimensions = request_body["dimensions"]
                    row_keys = row["keys"]
                    # ç¢ºä¿é¡å‹æ­£ç¢º  # type: ignore
                    keys = dict(zip(dimensions, row_keys)) if dimensions and row_keys else {}  # type: ignore
                    # API å›å‚³çš„ 'HOUR' éµæ˜¯ä¸€å€‹å®Œæ•´çš„ ISO 8601 æ™‚é–“æˆ³
                    dt_str = keys.get("HOUR", "")
                    if not dt_str:
                        continue

                    dt_obj = datetime.fromisoformat(dt_str.replace("Z", "+00:00"))

                    yield {
                        "query": keys.get("query"),
                        "page": keys.get("page"),
                        "device": keys.get("device"),
                        "date": dt_obj.strftime("%Y-%m-%d"),
                        "hour": dt_obj.hour,
                        "hour_timestamp": dt_obj.isoformat(),
                        "clicks": row.get("clicks", 0),
                        "impressions": row.get("impressions", 0),
                        "ctr": row.get("ctr", 0),
                        "position": row.get("position", 0),
                    }

                if len(rows) < 25000:
                    break

                start_row += 25000

            except HttpError as e:
                logger.error(f"ç²å–æ¯å°æ™‚æ•¸æ“šæ™‚ç™¼ç”Ÿ HTTP éŒ¯èª¤: {e}", exc_info=True)
                break
            except Exception as e:
                logger.error(f"ç²å–æ¯å°æ™‚æ•¸æ“šæ™‚ç™¼ç”Ÿæ„å¤–éŒ¯èª¤: {e}", exc_info=True)
                break

    def _load_client_config(self) -> Dict[str, Any]:
        """è¼‰å…¥ Google OAuth å®¢æˆ¶ç«¯é…ç½®"""
        try:
            with open(self.client_config_path, "r") as f:
                config_data = json.load(f)
                # ç¢ºä¿è¿”å›æ­£ç¢ºçš„é¡å‹
                return dict(config_data) if isinstance(config_data, dict) else {"web": {}}
        except (FileNotFoundError, json.JSONDecodeError) as e:
            logger.error(f"Failed to load client config from {self.client_config_path}: {e}")
            return {"web": {}}

    def get_auth_url(self) -> str:
        """ç²å–OAuthèªè­‰URL"""
        flow = Flow.from_client_config(
            self.client_config,
            scopes=self.scopes,
            redirect_uri=self.client_config["web"]["redirect_uris"][0],
        )

        auth_url, _ = flow.authorization_url(
            access_type="offline", include_granted_scopes="false", prompt="consent"
        )

        return str(auth_url)

    def handle_oauth_callback(self, code: str) -> bool:
        """è™•ç†OAuthå›èª¿"""
        try:
            flow = Flow.from_client_config(
                self.client_config,
                scopes=self.scopes,
                redirect_uri=self.client_config["web"]["redirect_uris"][0],
            )

            flow.fetch_token(code=code)
            if flow.credentials:
                self.credentials = flow.credentials  # type: ignore
                self._save_credentials()

                # åˆå§‹åŒ–æœå‹™å’Œæ¯å°æ™‚è™•ç†å™¨
                self.service = build("searchconsole", "v1", credentials=self.credentials)
                self.hourly_handler = HourlyDataHandler(self.service, self.database)

                logger.info("OAuth authentication successful")
                return True

        except Exception as e:
            logger.error(f"OAuth callback failed: {e}")

        return False

    def _save_credentials(self):
        """ä¿å­˜èªè­‰è³‡è¨Šåˆ°æ–‡ä»¶"""
        if self.credentials:
            creds_data = {
                "token": self.credentials.token,
                "refresh_token": self.credentials.refresh_token,
                "token_uri": self.credentials.token_uri,
                "client_id": self.credentials.client_id,
                "client_secret": self.credentials.client_secret,
                "scopes": self.credentials.scopes,
            }
            try:
                os.makedirs(os.path.dirname(self.credentials_path), exist_ok=True)
                with open(self.credentials_path, "w") as f:
                    json.dump(creds_data, f)
                logger.info(f"Credentials saved to {self.credentials_path}")
            except Exception as e:
                logger.error(f"Failed to save credentials to {self.credentials_path}: {e}")

    def _load_credentials(self) -> bool:
        """
        å¾æ–‡ä»¶åŠ è¼‰æ†‘è­‰ã€‚å¦‚æœæ†‘è­‰éæœŸä¸”æœ‰åˆ·æ–°ä»¤ç‰Œï¼Œå‰‡å˜—è©¦åˆ·æ–°ã€‚
        """
        if os.path.exists(self.credentials_path):
            try:
                with open(self.credentials_path, "r") as f:
                    creds_data = json.load(f)
                    self.credentials = Credentials.from_authorized_user_info(
                        creds_data, self.scopes
                    )

                # æª¢æŸ¥æ†‘è­‰æ˜¯å¦éæœŸï¼Œå¦‚æœéæœŸä¸”æœ‰åˆ·æ–°ä»¤ç‰Œï¼Œå‰‡åˆ·æ–°
                if self.credentials and self.credentials.expired and self.credentials.refresh_token:
                    logger.info("Credentials expired, attempting to refresh...")
                    self.credentials.refresh(Request())
                    self._save_credentials()  # ä¿å­˜åˆ·æ–°å¾Œçš„æ†‘è­‰
                    logger.info("Credentials refreshed successfully.")

                return self.credentials is not None and self.credentials.valid

            except Exception as e:
                logger.error(f"Failed to load or refresh credentials: {e}")
                self.credentials = None
                return False
        return False

    def is_authenticated(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦å·²èªè­‰"""
        if not self.credentials:
            self._load_credentials()

        if self.credentials:
            if self.credentials.expired and self.credentials.refresh_token:
                try:
                    self.credentials.refresh(Request())
                    self._save_credentials()
                    return True
                except Exception as e:
                    logger.error(f"Failed to refresh credentials: {e}")
                    return False

            return not self.credentials.expired

        return False

    def get_sites(self) -> List[str]:
        """ç²å–GSCä¸­çš„ç«™é»åˆ—è¡¨"""
        if not self.is_authenticated():
            raise Exception("Not authenticated with GSC")

        if self.service is None:
            raise Exception("GSC service not initialized")

        try:
            sites_response = self.service.sites().list().execute()
            sites = []

            for site in sites_response.get("siteEntry", []):
                if site["permissionLevel"] in ["siteFullUser", "siteOwner"]:
                    sites.append(site["siteUrl"])

            return sites

        except HttpError as e:
            logger.error(f"Failed to get sites: {e}")
            return []

    def get_sitemaps(self, site_url: str) -> List[Dict[str, Any]]:
        """ç²å–ç«™é»çš„ç´¢å¼•é é¢ï¼ˆsitemapï¼‰åˆ—è¡¨"""
        if not self.is_authenticated():
            raise Exception("Not authenticated with GSC")

        if self.service is None:
            raise Exception("GSC service not initialized")

        try:
            sitemaps_response = self.service.sitemaps().list(siteUrl=site_url).execute()
            sitemaps = sitemaps_response.get("sitemap", [])
            return list(sitemaps) if sitemaps else []

        except HttpError as e:
            logger.error(f"Failed to get sitemaps for {site_url}: {e}")
            return []

    def get_sitemap_details(self, site_url: str, feedpath: str) -> Dict[str, Any]:
        """ç²å–ç‰¹å®šç´¢å¼•é é¢çš„è©³ç´°ä¿¡æ¯"""
        if not self.is_authenticated():
            raise Exception("Not authenticated with GSC")

        if self.service is None:
            raise Exception("GSC service not initialized")

        try:
            sitemap_response = (
                self.service.sitemaps().get(siteUrl=site_url, feedpath=feedpath).execute()
            )
            return dict(sitemap_response) if sitemap_response else {}

        except HttpError as e:
            logger.error(f"Failed to get sitemap details for {site_url}/{feedpath}: {e}")
            return {}

    def get_all_sitemaps_for_sites(self, site_urls: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        """æ‰¹é‡ç²å–å¤šå€‹ç«™é»çš„ç´¢å¼•é é¢"""
        if not self.is_authenticated():
            raise Exception("Not authenticated with GSC")

        if self.service is None:
            raise Exception("GSC service not initialized")

        results = {}

        for site_url in site_urls:
            try:
                sitemaps = self.get_sitemaps(site_url)
                results[site_url] = sitemaps
                logger.info(f"Retrieved {len(sitemaps)} sitemaps for {site_url}")
            except Exception as e:
                logger.error(f"Failed to get sitemaps for {site_url}: {e}")
                results[site_url] = []

        return results

    def get_indexed_pages_count(self, site_url: str) -> Dict[str, Any]:
        """ç²å–ç«™é»çš„ç´¢å¼•é é¢çµ±è¨ˆä¿¡æ¯"""
        if not self.is_authenticated():
            raise Exception("Not authenticated with GSC")

        if self.service is None:
            raise Exception("GSC service not initialized")

        try:
            # ç²å– sitemap åˆ—è¡¨
            sitemaps = self.get_sitemaps(site_url)

            total_pages = 0
            sitemap_details = []

            for sitemap in sitemaps:
                feedpath = sitemap.get("path", "")
                if feedpath:
                    details = self.get_sitemap_details(site_url, feedpath)
                    if details:
                        contents = details.get("contents", [])
                        for content in contents:
                            if "submitted" in content:
                                submitted_count = content["submitted"]
                                if isinstance(submitted_count, (int, float)):
                                    total_pages += int(submitted_count)

                        sitemap_details.append(
                            {
                                "path": feedpath,
                                "lastSubmitted": sitemap.get("lastSubmitted"),
                                "contents": contents,
                            }
                        )

            return {
                "site_url": site_url,
                "total_sitemaps": len(sitemaps),
                "total_indexed_pages": total_pages,
                "sitemap_details": sitemap_details,
            }

        except Exception as e:
            logger.error(f"Failed to get indexed pages count for {site_url}: {e}")
            return {
                "site_url": site_url,
                "total_sitemaps": 0,
                "total_indexed_pages": 0,
                "sitemap_details": [],
            }

    def get_url_inspection(self, site_url: str, inspection_url: str) -> Dict[str, Any]:
        """ä½¿ç”¨ URL Inspection API æª¢æŸ¥ç‰¹å®š URL çš„ç´¢å¼•ç‹€æ…‹"""
        if not self.is_authenticated():
            raise Exception("Not authenticated with GSC")

        if self.service is None:
            raise Exception("GSC service not initialized")

        try:
            request = {"inspectionUrl": inspection_url, "siteUrl": site_url}

            response = self.service.urlInspection().index().inspect(request).execute()
            return dict(response) if response else {}

        except HttpError as e:
            logger.error(f"ç²å– URL æª¢æŸ¥çµæœæ™‚å‡ºéŒ¯: {e}")
            return {"error": str(e)}

    def get_sample_pages_from_analytics(
        self, site_url: str, start_date: str, end_date: str
    ) -> Dict[str, Any]:
        """
        [é‡æ§‹] å¾æœç´¢åˆ†æä¸­ç²å–é é¢æ¨£æœ¬æ•¸æ“šï¼Œä»¥è¿‘ä¼¼æ¨¡æ“¬ç´¢å¼•è¦†è“‹æƒ…æ³ã€‚
        åŸå get_index_coverage_reportï¼Œå·²é‡å‘½åä»¥æ›´æº–ç¢ºåœ°åæ˜ å…¶åŠŸèƒ½ã€‚
        """
        if not self.service:
            logger.error("GSC service is not initialized.")
            return {"error": "GSC service not initialized."}
        try:
            # ä½¿ç”¨ searchanalytics API ç²å–ä¸€æ®µæ™‚é–“å…§çš„é é¢æ•¸æ“š
            response = (
                self.service.searchanalytics()
                .query(
                    siteUrl=site_url,
                    body={
                        "startDate": start_date,
                        "endDate": end_date,
                        "dimensions": ["page"],
                        "rowLimit": 500,  # ç²å–ä¸€å€‹æ¨£æœ¬é‡
                    },
                )
                .execute()
            )
            self._track_api_request()
            return dict(response) if response else {}
        except Exception as e:
            logger.error(f"å¾æœç´¢åˆ†æç²å–é é¢æ¨£æœ¬æ™‚å‡ºéŒ¯: {e}")
            return {"error": str(e)}

    def get_crawl_stats(self, site_url: str) -> Dict[str, Any]:
        """
        [NEW] ç²å–æŒ‡å®šç«™é»çš„æŠ“å–çµ±è¨ˆä¿¡æ¯ã€‚
        """
        if not self.service:
            raise Exception("GSC service not initialized")

        try:
            response = self.service.urlcrawlerrorscounts().query(siteUrl=site_url).execute()
            self._track_api_request()
            return dict(response) if response else {}
        except HttpError as e:
            logger.error(f"Failed to get crawl stats for {site_url}: {e}")
            return {"error": str(e), "status": e.resp.status}
        except Exception as e:
            logger.error(f"An unexpected error occurred while fetching crawl stats: {e}")
            return {"error": str(e)}

    def get_indexed_pages_via_search_analytics(
        self, site_url: str, start_date: Optional[str], end_date: Optional[str]
    ) -> List[Dict[str, Any]]:
        """é€šéæœç´¢åˆ†ææ•¸æ“šç²å–ç´¢å¼•é é¢åˆ—è¡¨"""
        if not self.is_authenticated() or not self.service:
            raise Exception("GSC service not initialized")

        if not start_date:
            start_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        if not end_date:
            end_date = datetime.now().strftime("%Y-%m-%d")

        try:
            # ç²å–é é¢ç¶­åº¦çš„æ•¸æ“š
            pages_data = self.get_search_analytics_batch(
                site_url, start_date, end_date, dimensions=["page"]
            )

            indexed_pages = []
            for row in pages_data:
                page = row.get("keys", [""])[0]
                if page:
                    indexed_pages.append(
                        {
                            "page": page,
                            "clicks": row.get("clicks", 0),
                            "impressions": row.get("impressions", 0),
                            "ctr": row.get("ctr", 0),
                            "position": row.get("position", 0),
                        }
                    )

            return indexed_pages

        except HttpError as e:
            logger.error(f"Failed to get indexed pages via search analytics for {site_url}: {e}")
            return []

    def get_all_indexed_pages(
        self,
        site_url: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
    ) -> Dict[str, Any]:
        """ç²å–ç«™é»çš„æ‰€æœ‰ç´¢å¼•é é¢ä¿¡æ¯ï¼ˆç¶œåˆæ–¹æ³•ï¼‰"""

        # ç¢ºä¿æ—¥æœŸä¸ç‚º None
        final_start_date = start_date or (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        final_end_date = end_date or datetime.now().strftime("%Y-%m-%d")

        try:
            # 1. ç²å– sitemap ä¿¡æ¯
            sitemap_info = self.get_indexed_pages_count(site_url)

            # 2. ç²å–æœç´¢åˆ†æä¸­çš„ç´¢å¼•é é¢
            search_analytics_pages = self.get_indexed_pages_via_search_analytics(
                site_url, final_start_date, final_end_date
            )

            # 3. ç²å–çˆ¬èŸ²çµ±è¨ˆ
            crawl_stats = self.get_crawl_stats(site_url)

            return {
                "site_url": site_url,
                "date_range": f"{final_start_date} to {final_end_date}",
                "sitemap_info": sitemap_info,
                "search_analytics_pages": search_analytics_pages,
                "crawl_stats": crawl_stats,
                "total_pages_from_search": len(search_analytics_pages),
                "total_pages_from_sitemap": sitemap_info.get("total_indexed_pages", 0),
            }

        except Exception as e:
            logger.error(f"Failed to get all indexed pages for {site_url}: {e}")
            return {"site_url": site_url, "error": str(e)}

    def get_sitemap_urls_for_site(self, site_url: str) -> List[str]:
        """
        ç²å–ä¸€å€‹ç«™é»æ‰€æœ‰ sitemaps ä¸­çš„æ‰€æœ‰ URLã€‚
        æ­¤æ–¹æ³•ç¾åœ¨çµ±ä¸€ä½¿ç”¨ get_sitemap_urlsï¼Œä¸¦ç§»é™¤äº†å° parse_sitemap_xml çš„ä¾è³´ã€‚
        """
        all_urls = set()
        try:
            # é¦–å…ˆå¾ robots.txt ç²å– sitemaps
            sitemap_paths = self.get_sitemaps_from_robots(site_url)

            # å¦‚æœ robots.txt ä¸­æ²’æœ‰ï¼Œå‰‡å˜—è©¦å¾ GSC API ç²å–
            if not sitemap_paths:
                sitemaps = self.get_sitemaps(site_url)
                sitemap_paths = [s["path"] for s in sitemaps]

            # éæ­·æ‰€æœ‰æ‰¾åˆ°çš„ sitemap è·¯å¾‘
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future_to_url = {
                    executor.submit(self.get_sitemap_urls, path): path for path in sitemap_paths
                }
                for future in concurrent.futures.as_completed(future_to_url):
                    try:
                        urls = future.result()
                        all_urls.update(urls)
                    except Exception as exc:
                        logger.error(f"å¾ sitemap {future_to_url[future]} ç²å– URL æ™‚å‡ºéŒ¯: {exc}")

            return list(all_urls)
        except Exception as e:
            logger.error(f"ç‚ºç«™é» {site_url} ç²å– sitemap URLs æ™‚å‡ºéŒ¯: {e}")
            return []

    def get_sitemaps_from_robots(self, site_url: str) -> List[str]:
        """å¾ robots.txt æ–‡ä»¶ä¸­ç²å– sitemap URL"""

        import requests

        try:
            # æ§‹å»º robots.txt URL
            if site_url.startswith("sc-domain:"):
                domain = site_url.replace("sc-domain:", "")
                robots_url = f"https://{domain}/robots.txt"
            else:
                # ç¢ºä¿æœ‰å”è­°
                if not site_url.startswith(("http://", "https://")):
                    site_url = f"https://{site_url}"
                robots_url = f"{site_url.rstrip('/')}/robots.txt"

            # ç²å– robots.txt å…§å®¹
            response = requests.get(robots_url, timeout=30)
            response.raise_for_status()

            # è§£æ sitemap è¡Œ
            sitemaps = []
            for line in response.text.split("\n"):
                line = line.strip()
                if line.lower().startswith("sitemap:"):
                    sitemap_url = line[8:].strip()
                    if sitemap_url:
                        sitemaps.append(sitemap_url)

            return sitemaps

        except Exception as e:
            logger.warning(f"Failed to get sitemaps from robots.txt for {site_url}: {e}")
            return []

    def get_sitemap_urls(self, sitemap_path: str) -> List[str]:
        """
        éè¿´åœ°å¾ sitemapï¼ˆæˆ– sitemap ç´¢å¼•ï¼‰ä¸­æå–æ‰€æœ‰ URLã€‚
        é€™æ˜¯çµ±ä¸€çš„ã€å¥å£¯çš„ sitemap è§£æå™¨ã€‚
        """
        all_urls: List[str] = []

        try:
            logger.info(f"æ­£åœ¨è™•ç†: {sitemap_path}")
            response = requests.get(sitemap_path, timeout=30)
            response.raise_for_status()

            content = response.content
            root = ET.fromstring(content)

            namespace = ""
            if "}" in root.tag:
                namespace = root.tag.split("}")[0].strip("{")

            # æª¢æŸ¥æ˜¯ sitemap index é‚„æ˜¯ sitemap
            if root.tag.endswith("sitemapindex"):
                logger.info(f"{sitemap_path} æ˜¯ä¸€å€‹ç«™é»åœ°åœ–ç´¢å¼•ï¼Œæ­£åœ¨è§£æå­åœ°åœ–...")
                sitemap_links = [
                    elem.text
                    for elem in root.findall(f"{{{namespace}}}sitemap/{{{namespace}}}loc")
                    if elem.text
                ]
                for link in sitemap_links:
                    all_urls.extend(self.get_sitemap_urls(link))  # éè¿´èª¿ç”¨
            elif root.tag.endswith("urlset"):
                logger.info(f"{sitemap_path} æ˜¯ä¸€å€‹æ¨™æº–ç«™é»åœ°åœ–ï¼Œæ­£åœ¨è§£æ URL...")
                urls = [
                    elem.text
                    for elem in root.findall(f"{{{namespace}}}url/{{{namespace}}}loc")
                    if elem.text
                ]
                all_urls.extend(urls)
            else:
                logger.warning(f"æœªçŸ¥çš„æ ¹æ¨™ç±¤ '{root.tag}' åœ¨ {sitemap_path}")

        except requests.exceptions.RequestException as e:
            logger.error(f"ä¸‹è¼‰ç«™é»åœ°åœ– {sitemap_path} å¤±æ•—: {e}")
        except ET.ParseError as e:
            logger.error(f"è§£æç«™é»åœ°åœ– XML {sitemap_path} å¤±æ•—: {e}")
        except Exception as e:
            logger.error(f"è™•ç†ç«™é»åœ°åœ– {sitemap_path} æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")

        # åœ¨éè¿´çš„æœ€å¤–å±¤è¿”å›çµæœ
        # ç‚ºäº†é¿å…åœ¨éè¿´ä¸­æ‰“å°é‡è¤‡æ—¥èªŒï¼Œæˆ‘å€‘åªåœ¨æœ€é ‚å±¤èª¿ç”¨æ™‚æ‰“å°ç¸½æ•¸
        # é€™éœ€è¦ä¸€å€‹è¼”åŠ©å‡½æ•¸æˆ–ä¸€å€‹èª¿ç”¨æ·±åº¦æ¨™è¨˜ï¼Œæš«æ™‚ç°¡åŒ–è™•ç†
        return all_urls

    def compare_db_and_sitemap(self, site_url: str, site_id: int) -> Dict[str, Any]:
        """æ¯”è¼ƒæ•¸æ“šåº«ä¸­çš„ URL å’Œ sitemap ä¸­çš„ URL"""
        try:
            # 1. [REFACTORED] å¾ gsc_performance_data ç²å–æ•¸æ“šåº«ä¸­çš„ URL
            db_urls = set(self.database.get_all_pages_for_site(site_id=site_id))
            if not db_urls:
                logger.warning(f"åœ¨æ•¸æ“šåº«ä¸­æ‰¾ä¸åˆ°ç«™é» ID {site_id} çš„ä»»ä½•é é¢æ•¸æ“šã€‚")

            # 2. ç²å– sitemap ä¸­çš„ URL
            sitemap_urls = set(self.get_sitemap_urls_for_site(site_url))
            if not sitemap_urls:
                logger.warning(f"ç„¡æ³•å¾ {site_url} çš„ sitemap ä¸­ç²å–ä»»ä½• URLã€‚")

            # 3. æ¯”è¼ƒå·®ç•°
            only_in_db = sorted(list(db_urls - sitemap_urls))
            only_in_sitemap = sorted(list(sitemap_urls - db_urls))
            in_both = sorted(list(db_urls & sitemap_urls))

            return {
                "site_url": site_url,
                "db_url_count": len(db_urls),
                "sitemap_url_count": len(sitemap_urls),
                "common_url_count": len(in_both),
                "only_in_db": only_in_db,
                "only_in_sitemap": only_in_sitemap,
                "in_both": in_both,
            }
        except Exception as e:
            logger.error(f"æ¯”è¼ƒæ•¸æ“šåº«å’Œ sitemap æ™‚å‡ºéŒ¯: {e}", exc_info=True)
            return {"error": str(e)}

    def get_search_analytics(
        self,
        site_url: str,
        start_date: str,
        end_date: str,
        dimensions: Optional[List[str]] = None,
        row_limit: int = 1000,
    ) -> List[Dict[str, Any]]:
        """ç²å–æœç´¢åˆ†ææ•¸æ“š"""
        if not self.is_authenticated():
            raise Exception("Not authenticated with GSC")

        if self.service is None:
            raise Exception("GSC service not initialized")

        if dimensions is None:
            dimensions = ["query"]

        try:
            request = {
                "startDate": start_date,
                "endDate": end_date,
                "dimensions": dimensions,
                "rowLimit": row_limit,
                "startRow": 0,
            }

            response = (
                self.service.searchanalytics().query(siteUrl=site_url, body=request).execute()
            )

            self._track_api_request()  # è¿½è¹¤APIä½¿ç”¨
            rows = response.get("rows", [])
            return list(rows) if rows else []

        except HttpError as e:
            logger.error(f"Failed to get search analytics for {site_url}: {e}")
            return []

    def get_search_analytics_batch(
        self,
        site_url: str,
        start_date: str,
        end_date: str,
        dimensions: Optional[List[str]] = None,
        device_filter: Optional[str] = None,
        max_total_rows: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """æ‰¹æ¬¡ç²å–æœç´¢åˆ†ææ•¸æ“š"""
        if not self.is_authenticated():
            raise Exception("Not authenticated with GSC")

        if self.service is None:
            raise Exception("GSC service not initialized")

        if dimensions is None:
            dimensions = ["query"]

        all_rows: List[Dict[str, Any]] = []
        max_rows_per_request = 25000
        start_row = 0

        try:
            while max_total_rows is None or len(all_rows) < max_total_rows:
                # è¨ˆç®—é€™æ¬¡è«‹æ±‚çš„è¡Œæ•¸é™åˆ¶
                if max_total_rows is None:
                    row_limit = max_rows_per_request
                else:
                    row_limit = min(max_rows_per_request, max_total_rows - len(all_rows))

                logger.info(
                    f"Requesting batch: startRow={start_row}, rowLimit={row_limit}, "
                    f"total_so_far={len(all_rows)}"
                )

                request = {
                    "startDate": start_date,
                    "endDate": end_date,
                    "dimensions": dimensions,
                    "rowLimit": row_limit,
                    "startRow": start_row,
                }

                if device_filter:
                    request["dimensionFilterGroups"] = [
                        {
                            "filters": [
                                {
                                    "dimension": "device",
                                    "operator": "equals",
                                    "expression": device_filter,
                                }
                            ]
                        }
                    ]

                response = (
                    self.service.searchanalytics().query(siteUrl=site_url, body=request).execute()
                )

                self._track_api_request()  # è¿½è¹¤APIä½¿ç”¨
                rows = response.get("rows", [])
                logger.info(f"Received {len(rows)} rows from GSC API")

                if not rows:
                    logger.info("No more rows returned, breaking")
                    break

                all_rows.extend(rows)
                start_row += len(rows)

                # ä¸è¦å› ç‚ºè¿”å›è¡Œæ•¸å°‘å°±åœæ­¢ï¼GSC API å¯èƒ½æœ‰éš±è—é™åˆ¶
                # åªæœ‰ç•¶çœŸçš„æ²’æœ‰æ•¸æ“šæ™‚æ‰åœæ­¢
                if len(rows) == 0:
                    logger.info("No rows returned, stopping")
                    break

                # å¦‚æœè¿”å›è¡Œæ•¸å°‘æ–¼è«‹æ±‚ï¼Œä½†å¤§æ–¼ 1000ï¼Œç¹¼çºŒå˜—è©¦ä¸‹ä¸€é 
                if len(rows) < max_rows_per_request and len(rows) < 1000:
                    logger.info(f"Received only {len(rows)} rows (< 1000), likely end of data")
                    break

        except HttpError as e:
            logger.error(f"Failed to get search analytics batch for {site_url}: {e}")

        return all_rows

    def get_keywords_for_site(self, site_url: str, limit: int = 100) -> List[str]:
        """ç²å–ç«™é»çš„ç†±é–€é—œéµå­—"""
        try:
            data = self.get_search_analytics_batch(
                site_url,
                (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d"),
                datetime.now().strftime("%Y-%m-%d"),
                dimensions=["query"],
                max_total_rows=limit,
            )

            # æŒ‰é»æ“Šæ•¸æ’åº
            sorted_data = sorted(data, key=lambda x: x.get("clicks", 0), reverse=True)
            return [row["keys"][0] for row in sorted_data[:limit]]

        except Exception as e:
            logger.error(f"Failed to get keywords for {site_url}: {e}")
            return []

    def _track_api_request(self):
        """è¿½è¹¤APIè«‹æ±‚ (ç·šç¨‹å®‰å…¨)"""
        with self._api_lock:
            now = datetime.now()
            current_minute = now.replace(second=0, microsecond=0)
            current_date = now.date()

            # é‡ç½®æ¯æ—¥è¨ˆæ•¸å™¨
            if current_date != self.today:
                self.api_requests_today = 0
                self.today = current_date
                # ç‚ºæ–°çš„ä¸€å¤©å‰µå»ºæ•¸æ“šåº«è¨˜éŒ„
                self._load_api_usage_from_db()

            # é‡ç½®æ¯åˆ†é˜è¨ˆæ•¸å™¨
            if current_minute != self.last_minute_reset:
                self.api_requests_this_minute = 0
                self.last_minute_reset = current_minute

            self.api_requests_today += 1
            self.api_requests_this_minute += 1

            # ä¿å­˜åˆ°æ•¸æ“šåº«
            self._save_api_usage_to_db()

            # è¨˜éŒ„åˆ°æ—¥èªŒ
            logger.info(
                f"APIè«‹æ±‚è¨ˆæ•¸: ä»Šæ—¥ {self.api_requests_today}, "
                f"æœ¬åˆ†é˜ {self.api_requests_this_minute}"
            )

    def get_api_usage_stats(self) -> Dict[str, Any]:
        """ç²å–APIä½¿ç”¨çµ±è¨ˆ"""
        with self._api_lock:
            return {
                "requests_today": self.api_requests_today,
                "requests_this_minute": self.api_requests_this_minute,
                "daily_limit": 100000,
                "minute_limit": 1200,
                "daily_remaining": 100000 - self.api_requests_today,
                "minute_remaining": 1200 - self.api_requests_this_minute,
                "daily_usage_percent": (self.api_requests_today / 100000) * 100,
                "minute_usage_percent": (self.api_requests_this_minute / 1200) * 100,
            }

    def _load_api_usage_from_db(self):
        """å¾æ•¸æ“šåº«è¼‰å…¥ä»Šæ—¥APIä½¿ç”¨è¨ˆæ•¸"""
        try:
            self.database.init_api_usage_table()
            # è¼‰å…¥ä»Šæ—¥çš„è¨ˆæ•¸
            today_str = self.today.strftime("%Y-%m-%d")
            self.api_requests_today = self.database.get_api_usage(today_str)
            logger.info(f"è¼‰å…¥ä»Šæ—¥APIä½¿ç”¨è¨ˆæ•¸: {self.api_requests_today}")

        except Exception as e:
            logger.error(f"è¼‰å…¥APIä½¿ç”¨çµ±è¨ˆå¤±æ•—: {e}")

    def _save_api_usage_to_db(self):
        """ä¿å­˜ä»Šæ—¥APIä½¿ç”¨è¨ˆæ•¸åˆ°æ•¸æ“šåº«"""
        today_str = self.today.strftime("%Y-%m-%d")
        self.database.update_api_usage(today_str, self.api_requests_today)
