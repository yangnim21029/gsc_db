#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import concurrent.futures
import logging
import os
import ssl

# å°å…¥ç›¸é—œæ¨¡å¡Š
import sys
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from .database import Database
from .hourly_data import HourlyDataHandler

sys.path.append(os.path.dirname(__file__))
import threading
import time
import xml.etree.ElementTree as ET

import requests

from .. import config
from ..utils.rich_console import console

logger = logging.getLogger(__name__)


class GSCClient:
    _api_lock = threading.Lock()
    api_requests_this_minute = 0
    last_minute_reset = datetime.now().replace(second=0, microsecond=0)

    def __init__(self, db: Database):
        """åˆå§‹åŒ– GSC å®¢æˆ¶ç«¯"""
        self.scopes = config.GSC_SCOPES
        self.credentials_path = str(config.get_credentials_path())
        self.client_config_path = str(config.CLIENT_SECRET_PATH)

        # åˆå§‹åŒ–ç‹€æ…‹
        self.credentials: Optional[Credentials] = None
        self.service: Optional[Any] = None
        self.database = db

        # API ä½¿ç”¨è¨ˆæ•¸å™¨
        self.api_requests_today = 0
        self.api_requests_this_minute = 0
        self.last_minute_reset = datetime.now().replace(second=0, microsecond=0)
        self.today = datetime.now().date()
        self._api_lock = threading.Lock()

        # å¾æ•¸æ“šåº«è¼‰å…¥ä»Šæ—¥APIä½¿ç”¨è¨ˆæ•¸
        self._load_api_usage_from_db()

    def authenticate(self):
        """
        åŸ·è¡Œæˆ–é©—è­‰GSCèªè­‰ã€‚å¦‚æœæ†‘è­‰æœ‰æ•ˆå‰‡ç›´æ¥ä½¿ç”¨ï¼Œå¦å‰‡å•Ÿå‹•OAuthæµç¨‹ã€‚
        æ­¤æ–¹æ³•ç¢ºä¿ self.service åœ¨çµæŸæ™‚æ˜¯å¯ç”¨çš„ã€‚
        """
        # å¦‚æœ service ç‰©ä»¶å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œå‰‡ç›´æ¥è¿”å›ï¼Œé¿å…é‡è¤‡èªè­‰
        if self.service and self.credentials and self.credentials.valid:
            return

        creds: Optional[Credentials] = None

        # 1. å˜—è©¦å¾ token æ–‡ä»¶åŠ è¼‰
        if os.path.exists(self.credentials_path):
            try:
                # é¦–å…ˆåŠ è¼‰ï¼Œé¡å‹å°šä¸ç¢ºå®š
                loaded_creds = Credentials.from_authorized_user_file(
                    self.credentials_path, self.scopes
                )
                # é¡¯å¼æª¢æŸ¥é¡å‹ï¼Œç¢ºä¿é¡å‹å®‰å…¨
                if isinstance(loaded_creds, Credentials):
                    creds = loaded_creds
                else:
                    logger.warning(
                        f"å¾ {self.credentials_path} åŠ è¼‰çš„æ†‘è­‰é¡å‹ä¸æ­£ç¢ºï¼Œå°‡å¿½ç•¥ä¸¦é‡æ–°èªè­‰ã€‚"
                    )
            except Exception as e:
                logger.warning(f"åŠ è¼‰ {self.credentials_path} å¤±æ•—: {e}ï¼Œå°‡å˜—è©¦é‡æ–°èªè­‰ã€‚")
                creds = None

        # 2. å¦‚æœæ†‘è­‰ç„¡æ•ˆæˆ–éœ€è¦åˆ·æ–°ï¼Œå‰‡è™•ç†
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                try:
                    creds.refresh(Request())
                except Exception as e:
                    logger.error(f"åˆ·æ–° token å¤±æ•—ï¼Œå°‡é‡æ–°å•Ÿå‹•èªè­‰æµç¨‹: {e}")
                    creds = None  # å¼·åˆ¶é‡æ–°èªè­‰
            else:
                # 3. å¦‚æœæ²’æœ‰å¯ç”¨æ†‘è­‰ï¼Œå•Ÿå‹• InstalledAppFlow æµç¨‹
                try:
                    console.print("ğŸš€ [bold yellow]éœ€è¦æ–°çš„èªè­‰ï¼Œå•Ÿå‹• OAuth2 æµç¨‹...[/bold yellow]")
                    console.print("æ‚¨çš„ç€è¦½å™¨å°‡æœƒè‡ªå‹•æ‰“é–‹ï¼Œè«‹ç™»å…¥ä¸¦æˆæ¬Šã€‚")
                    flow = InstalledAppFlow.from_client_secrets_file(
                        self.client_config_path, self.scopes
                    )

                    # flow.run_local_server() è¿”å›çš„æ˜¯ Credentials
                    flow_creds = flow.run_local_server(port=0)
                    if isinstance(flow_creds, Credentials):
                        creds = flow_creds

                except FileNotFoundError:
                    console.print(
                        f"[bold red]éŒ¯èª¤: æ‰¾ä¸åˆ° OAuth ç”¨æˆ¶ç«¯å¯†é‘°æª”æ¡ˆ: {self.client_config_path}ã€‚[/bold red]"
                    )
                    console.print(
                        "è«‹å¾ Google Cloud Console ä¸‹è¼‰ 'é›»è…¦ç‰ˆæ‡‰ç”¨ç¨‹å¼' çš„æ†‘è­‰ï¼Œä¸¦å°‡å…¶å‘½åç‚º client_secret.json æ”¾åœ¨ cred/ ç›®éŒ„ä¸‹ã€‚"
                    )
                    raise
                except Exception as e:
                    logger.error(f"OAuth æµç¨‹å‡ºéŒ¯: {e}")
                    raise

        # 4. ä¿å­˜æ–°çš„æ†‘è­‰
        if creds and isinstance(creds, Credentials):
            try:
                with open(self.credentials_path, "w") as token:
                    token.write(creds.to_json())
            except Exception as e:
                logger.error(f"ä¿å­˜ token å¤±æ•—: {e}")

        # 5. æœ€çµ‚è¨­ç½® class å±¬æ€§
        if creds and isinstance(creds, Credentials):
            self.credentials = creds
            self.service = build("searchconsole", "v1", credentials=self.credentials)
            self.hourly_handler = HourlyDataHandler(self.service, self.database)
        else:
            raise ConnectionRefusedError("ç„¡æ³•ç²å–æœ‰æ•ˆçš„ Google èªè­‰æ†‘è­‰ã€‚")

    def _rate_limit_check(self):
        """å¯¦æ–½ API é€Ÿç‡é™åˆ¶æª¢æŸ¥"""
        with self._api_lock:
            now = datetime.now()
            current_minute = now.replace(second=0, microsecond=0)

            # é‡ç½®æ¯åˆ†é˜è¨ˆæ•¸å™¨
            if current_minute > self.last_minute_reset:
                self.api_requests_this_minute = 0
                self.last_minute_reset = current_minute

            # æª¢æŸ¥æ¯åˆ†é˜é™åˆ¶ (ä¿å®ˆä¼°è¨ˆ 100 requests/minute)
            if self.api_requests_this_minute >= 100:
                sleep_time = 60 - now.second
                logger.info(f"é”åˆ°æ¯åˆ†é˜é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {sleep_time} ç§’...")
                time.sleep(sleep_time)
                self.api_requests_this_minute = 0
                self.last_minute_reset = datetime.now().replace(second=0, microsecond=0)

    def stream_site_data_optimized(self, site_url: str, start_date: str, end_date: str):
        """
        å„ªåŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ API æœ€ä½³å¯¦è¸çš„æ•¸æ“šæµå¼ç²å–
        - æ¸›å°‘ä½µç™¼è«‹æ±‚
        - ä½¿ç”¨ gzip å£“ç¸®
        - ä½¿ç”¨ fields åƒæ•¸
        - éµå¾ªæ¯æ—¥æŸ¥è©¢æ¨¡å¼
        """
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        search_types = ["web", "image", "video", "news", "discover", "googleNews"]

        # æŒ‰æ—¥æœŸåˆ†çµ„ï¼Œæ¯å¤©è™•ç†ä¸€æ¬¡
        current_date = datetime.strptime(start_date, "%Y-%m-%d")
        end_date_obj = datetime.strptime(end_date, "%Y-%m-%d")

        while current_date <= end_date_obj:
            date_str = current_date.strftime("%Y-%m-%d")

            # å°æ¯å€‹æœç´¢é¡å‹é †åºè™•ç†ï¼ˆé¿å…éåº¦ä½µç™¼ï¼‰
            for search_type in search_types:
                try:
                    self._rate_limit_check()

                    # ä½¿ç”¨ API æ¨è–¦çš„æŸ¥è©¢æ¨¡å¼
                    request_body = {
                        "startDate": date_str,
                        "endDate": date_str,  # æ¯å¤©æŸ¥è©¢ä¸€å¤©çš„æ•¸æ“š
                        "dimensions": ["page", "query", "device"],
                        "rowLimit": 25000,
                        "startRow": 0,
                        "type": search_type,
                    }

                    all_rows = []
                    start_row = 0

                    while True:
                        request_body["startRow"] = start_row

                        response = (
                            self.service.searchanalytics()
                            .query(siteUrl=site_url, body=request_body)
                            .execute()
                        )

                        self._track_api_request()

                        rows = response.get("rows", [])
                        if not rows:
                            break

                        all_rows.extend(rows)

                        if len(rows) < 25000:
                            break

                        start_row += 25000

                    # æŒ‰è¨­å‚™åˆ†çµ„æ•¸æ“š
                    if all_rows:
                        device_chunks: Dict[str, List[Dict[str, Any]]] = {}
                        for row in all_rows:
                            dimensions = request_body["dimensions"]
                            row_keys = row["keys"]
                            keys = (
                                dict(zip(dimensions, row_keys)) if dimensions and row_keys else {}  # type: ignore
                            )
                            device = keys.get("device", "N/A")

                            if device not in device_chunks:
                                device_chunks[device] = []

                            processed_row = {
                                "page": keys.get("page"),
                                "query": keys.get("query"),
                                "clicks": row.get("clicks"),
                                "impressions": row.get("impressions"),
                                "ctr": row.get("ctr"),
                                "position": row.get("position"),
                            }
                            device_chunks[device].append(processed_row)

                        for device, chunk in device_chunks.items():
                            yield device, search_type, chunk

                except HttpError as e:
                    if e.resp.status in [404, 403, 400]:
                        logger.warning(
                            f"No data for {site_url} with type {search_type} "
                            f"on {date_str} (HTTP {e.resp.status}). Skipping."
                        )
                        continue
                    elif e.resp.status == 429:
                        logger.warning("Rate limit exceeded, waiting 60 seconds...")
                        time.sleep(60)
                        continue
                    logger.error(f"HTTP error for type {search_type}: {e}", exc_info=True)
                    raise
                except ssl.SSLError as e:
                    logger.warning(
                        f"SSL error for type {search_type}: {e}. "
                        "This will be retried by the caller."
                    )
                    raise
                except requests.exceptions.RequestException as e:
                    logger.warning(
                        f"Network error for type {search_type}: {e}. "
                        "This will be retried by the caller."
                    )
                    raise

                # åœ¨ä¸åŒæœç´¢é¡å‹ä¹‹é–“æ·»åŠ æ›´é•·çš„å»¶é²ä¾†æ¸›å°‘ SSL éŒ¯èª¤
                time.sleep(1.0)  # å¢åŠ åˆ° 1.0 ç§’ï¼Œé€²ä¸€æ­¥æ¸›å°‘ SSL éŒ¯èª¤

            current_date += timedelta(days=1)

    def stream_site_data(self, site_url: str, start_date: str, end_date: str):
        """
        ä¿æŒå‘å¾Œå…¼å®¹æ€§ï¼Œä½†å…§éƒ¨ä½¿ç”¨å„ªåŒ–ç‰ˆæœ¬
        """
        return self.stream_site_data_optimized(site_url, start_date, end_date)

    def stream_hourly_data(self, site_url: str, start_date: str, end_date: str):
        """
        ä»¥æµå¼æ–¹å¼å¾ GSC API ç²å–æ¯å°æ™‚çš„è©³ç´°æ•¸æ“šã€‚
        æ­¤æ–¹æ³•æ ¹æ“š 2025/04/09 çš„å®˜æ–¹æ–‡ä»¶é€²è¡Œäº†ä¿®æ­£ã€‚
        """
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        start_row = 0
        while True:
            try:
                # æ ¹æ“šå®˜æ–¹æ–‡ä»¶ä¿®æ­£ request_body
                request_body = {
                    "startDate": start_date,
                    "endDate": end_date,
                    # é—œéµä¿®æ­£ 1: dimensions å¿…é ˆåŒ…å« 'HOUR'
                    "dimensions": ["HOUR", "query", "page", "device"],
                    # é—œéµä¿®æ­£ 2: dataState å¿…é ˆæ˜¯ 'HOURLY_ALL'
                    "dataState": "HOURLY_ALL",
                    "rowLimit": 25000,
                    "startRow": start_row,
                }

                response = (
                    self.service.searchanalytics()
                    .query(siteUrl=site_url, body=request_body)
                    .execute()
                )

                self._track_api_request()

                rows = response.get("rows", [])
                if not rows:
                    break

                for row in rows:
                    dimensions = request_body["dimensions"]
                    row_keys = row["keys"]
                    # ç¢ºä¿é¡å‹æ­£ç¢º  # type: ignore
                    keys = dict(zip(dimensions, row_keys)) if dimensions and row_keys else {}  # type: ignore
                    # API å›å‚³çš„ 'HOUR' éµæ˜¯ä¸€å€‹å®Œæ•´çš„ ISO 8601 æ™‚é–“æˆ³
                    dt_str = keys.get("HOUR", "")
                    if not dt_str:
                        continue

                    dt_obj = datetime.fromisoformat(dt_str.replace("Z", "+00:00"))

                    yield {
                        "query": keys.get("query"),
                        "page": keys.get("page"),
                        "device": keys.get("device"),
                        "date": dt_obj.strftime("%Y-%m-%d"),
                        "hour": dt_obj.hour,
                        "hour_timestamp": dt_obj.isoformat(),
                        "clicks": row.get("clicks", 0),
                        "impressions": row.get("impressions", 0),
                        "ctr": row.get("ctr", 0),
                        "position": row.get("position", 0),
                    }

                if len(rows) < 25000:
                    break

                start_row += 25000

            except HttpError as e:
                logger.error(f"ç²å–æ¯å°æ™‚æ•¸æ“šæ™‚ç™¼ç”Ÿ HTTP éŒ¯èª¤: {e}", exc_info=True)
                break
            except Exception as e:
                logger.error(f"ç²å–æ¯å°æ™‚æ•¸æ“šæ™‚ç™¼ç”Ÿæ„å¤–éŒ¯èª¤: {e}", exc_info=True)
                break

    def get_sites(self) -> List[str]:
        """ç²å–å¸³æˆ¶ä¸­çš„æ‰€æœ‰ç«™é»"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        site_list = []
        try:
            site_list = self.service.sites().list().execute()
            return [s["siteUrl"] for s in site_list.get("siteEntry", [])]
        except HttpError as e:
            logger.error(f"ç²å–ç¶²ç«™åˆ—è¡¨æ™‚å‡ºéŒ¯: {e}")
            return []

    def get_sitemaps(self, site_url: str) -> List[Dict[str, Any]]:
        """ç²å–ç«™é»çš„ç´¢å¼•é é¢ï¼ˆsitemapï¼‰åˆ—è¡¨"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        try:
            sitemaps_response = self.service.sitemaps().list(siteUrl=site_url).execute()  # type: ignore
            sitemaps = sitemaps_response.get("sitemap", [])
            return list(sitemaps) if sitemaps else []

        except HttpError as e:
            logger.error(f"Failed to get sitemaps for {site_url}: {e}")
            return []

    def get_sitemap_details(self, site_url: str, feedpath: str) -> Dict[str, Any]:
        """ç²å–ç‰¹å®šç´¢å¼•é é¢çš„è©³ç´°ä¿¡æ¯"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        try:
            sitemap_response = (
                self.service.sitemaps().get(siteUrl=site_url, feedpath=feedpath).execute()  # type: ignore
            )
            return dict(sitemap_response) if sitemap_response else {}

        except HttpError as e:
            logger.error(f"Failed to get sitemap details for {site_url}/{feedpath}: {e}")
            return {}

    def get_all_sitemaps_for_sites(self, site_urls: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        """æ‰¹é‡ç²å–å¤šå€‹ç«™é»çš„ç´¢å¼•é é¢"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        results = {}

        for site_url in site_urls:
            try:
                sitemaps = self.get_sitemaps(site_url)
                results[site_url] = sitemaps
                logger.info(f"Retrieved {len(sitemaps)} sitemaps for {site_url}")
            except Exception as e:
                logger.error(f"Failed to get sitemaps for {site_url}: {e}")
                results[site_url] = []

        return results

    def get_indexed_pages_count(self, site_url: str) -> Dict[str, Any]:
        """ç²å–ç«™é»çš„ç´¢å¼•é é¢çµ±è¨ˆä¿¡æ¯"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        try:
            # ç²å– sitemap åˆ—è¡¨
            sitemaps = self.get_sitemaps(site_url)

            total_pages = 0
            sitemap_details = []

            for sitemap in sitemaps:
                feedpath = sitemap.get("path", "")
                if feedpath:
                    details = self.get_sitemap_details(site_url, feedpath)
                    if details:
                        contents = details.get("contents", [])
                        for content in contents:
                            if "submitted" in content:
                                submitted_count = content["submitted"]
                                if isinstance(submitted_count, (int, float)):
                                    total_pages += int(submitted_count)

                        sitemap_details.append(
                            {
                                "path": feedpath,
                                "lastSubmitted": sitemap.get("lastSubmitted"),
                                "contents": contents,
                            }
                        )

            return {
                "site_url": site_url,
                "total_sitemaps": len(sitemaps),
                "total_indexed_pages": total_pages,
                "sitemap_details": sitemap_details,
            }

        except Exception as e:
            logger.error(f"Failed to get indexed pages count for {site_url}: {e}")
            return {
                "site_url": site_url,
                "total_sitemaps": 0,
                "total_indexed_pages": 0,
                "sitemap_details": [],
            }

    def get_url_inspection(self, site_url: str, inspection_url: str) -> Dict[str, Any]:
        """ä½¿ç”¨ URL Inspection API æª¢æŸ¥ç‰¹å®š URL çš„ç´¢å¼•ç‹€æ…‹"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        try:
            request = {"inspectionUrl": inspection_url, "siteUrl": site_url}

            response = self.service.urlInspection().index().inspect(request).execute()  # type: ignore
            return dict(response) if response else {}

        except HttpError as e:
            logger.error(f"ç²å– URL æª¢æŸ¥çµæœæ™‚å‡ºéŒ¯: {e}")
            return {"error": str(e)}

    def get_sample_pages_from_analytics(
        self, site_url: str, start_date: str, end_date: str
    ) -> Dict[str, Any]:
        """
        [é‡æ§‹] å¾æœç´¢åˆ†æä¸­ç²å–é é¢æ¨£æœ¬æ•¸æ“šï¼Œä»¥è¿‘ä¼¼æ¨¡æ“¬ç´¢å¼•è¦†è“‹æƒ…æ³ã€‚
        åŸå get_index_coverage_reportï¼Œå·²é‡å‘½åä»¥æ›´æº–ç¢ºåœ°åæ˜ å…¶åŠŸèƒ½ã€‚
        """
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")
        try:
            # ä½¿ç”¨ searchAnalytics.query API ç²å–æ•¸æ“š
            response = (
                self.service.searchanalytics()
                .query(
                    siteUrl=site_url,
                    body={
                        "startDate": start_date,
                        "endDate": end_date,
                        "dimensions": ["page"],
                        "rowLimit": 500,  # ç²å–ä¸€å€‹æ¨£æœ¬é‡
                    },
                )
                .execute()
            )
            self._track_api_request()
            return dict(response) if response else {}
        except Exception as e:
            logger.error(f"å¾æœç´¢åˆ†æç²å–é é¢æ¨£æœ¬æ™‚å‡ºéŒ¯: {e}")
            return {"error": str(e)}

    def get_crawl_stats(self, site_url: str) -> Dict[str, Any]:
        """
        [NEW] ç²å–æŒ‡å®šç«™é»çš„æŠ“å–çµ±è¨ˆä¿¡æ¯ã€‚
        """
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")
        try:
            response = self.service.urlcrawlerrorscounts().query(siteUrl=site_url).execute()  # type: ignore
            self._track_api_request()
            return dict(response) if response else {}
        except HttpError as e:
            logger.error(f"Failed to get crawl stats for {site_url}: {e}")
            return {"error": str(e), "status": e.resp.status}
        except Exception as e:
            logger.error(f"An unexpected error occurred while fetching crawl stats: {e}")
            return {"error": str(e)}

    def get_indexed_pages_via_search_analytics(
        self, site_url: str, start_date: Optional[str], end_date: Optional[str]
    ) -> List[Dict[str, Any]]:
        """é€šé Search Analytics API ç²å–ç´¢å¼•é é¢åˆ—è¡¨ï¼Œå¸¶æœ‰é‡è©¦æ©Ÿåˆ¶"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        if not start_date:
            start_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        if not end_date:
            end_date = datetime.now().strftime("%Y-%m-%d")

        logger.info(f"æ­£åœ¨å¾ GSC ç²å– {site_url} å¾ {start_date} åˆ° {end_date} çš„æ•¸æ“š...")

        try:
            # ç²å–é é¢ç¶­åº¦çš„æ•¸æ“š
            pages_data = self.get_search_analytics_batch(
                site_url, start_date, end_date, dimensions=["page"]
            )

            indexed_pages = []
            for row in pages_data:
                page = row.get("keys", [""])[0]
                if page:
                    indexed_pages.append(
                        {
                            "page": page,
                            "clicks": row.get("clicks", 0),
                            "impressions": row.get("impressions", 0),
                            "ctr": row.get("ctr", 0),
                            "position": row.get("position", 0),
                        }
                    )

            return indexed_pages

        except HttpError as e:
            logger.error(f"Failed to get indexed pages via search analytics for {site_url}: {e}")
            return []

    def get_all_indexed_pages(
        self,
        site_url: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
    ) -> Dict[str, Any]:
        """[æ ¸å¿ƒåŠŸèƒ½] ç²å–æŒ‡å®šç«™é»çš„æ‰€æœ‰å·²ç´¢å¼•é é¢ï¼Œä¸¦è¿”å›è©³ç´°è³‡è¨Šå’Œçµ±è¨ˆæ•¸æ“šã€‚"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        # ç¢ºä¿æ—¥æœŸä¸ç‚º None
        if not start_date or not end_date:
            today = datetime.now()
            start_date = (today - timedelta(days=30)).strftime("%Y-%m-%d")
            end_date = today.strftime("%Y-%m-%d")

        try:
            # ä½¿ç”¨ ThreadPoolExecutor ä¸¦è¡Œè™•ç† Sitemap å’Œ Analytics æ•¸æ“š
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                # æäº¤ Sitemap URL ç²å–ä»»å‹™
                future_sitemap_urls = executor.submit(self.get_sitemap_urls_for_site, site_url)
                # æäº¤ Analytics Page ç²å–ä»»å‹™
                future_analytics_pages = executor.submit(
                    self.get_sample_pages_from_analytics, site_url, start_date, end_date
                )

                # ç­‰å¾…çµæœ
                sitemap_urls = set(future_sitemap_urls.result())
                analytics_result = future_analytics_pages.result()

            analytics_pages = set(analytics_result.get("pages", []))
            total_pages_in_analytics = analytics_result.get("total_pages", 0)

            # é€²è¡Œæ¯”è¼ƒ
            common_urls = sitemap_urls.intersection(analytics_pages)
            sitemap_only_urls = sitemap_urls.difference(analytics_pages)
            analytics_only_urls = analytics_pages.difference(sitemap_urls)

            # è¨ˆç®—è¦†è“‹ç‡å’Œå†—é¤˜ç‡
            coverage_rate = len(common_urls) / len(sitemap_urls) if len(sitemap_urls) > 0 else 0
            redundancy_rate = (
                len(sitemap_only_urls) / len(sitemap_urls) if len(sitemap_urls) > 0 else 0
            )

            return {
                "sitemap_urls_count": len(sitemap_urls),
                "analytics_pages_count": len(analytics_pages),
                "total_pages_in_analytics": total_pages_in_analytics,
                "common_urls_count": len(common_urls),
                "sitemap_only_urls_count": len(sitemap_only_urls),
                "analytics_only_urls_count": len(analytics_only_urls),
                "coverage_rate": coverage_rate,
                "redundancy_rate": redundancy_rate,
                # "sitemap_only_urls": list(sitemap_only_urls), # å¯é¸è¿”å›
                # "analytics_only_urls": list(analytics_only_urls), # å¯é¸è¿”å›
            }

        except Exception as e:
            logger.error(f"åœ¨ get_all_indexed_pages ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return {"error": str(e)}

    def get_sitemap_urls_for_site(self, site_url: str) -> List[str]:
        """
        å¾çµ¦å®šçš„ç«™é» URL ä¸­æå–æ‰€æœ‰ Sitemap ä¸­åŒ…å«çš„ URLã€‚
        æ­¤æ–¹æ³•ç¾åœ¨çµ±ä¸€ä½¿ç”¨ get_sitemap_urlsï¼Œä¸¦ç§»é™¤äº†å° parse_sitemap_xml çš„ä¾è³´ã€‚
        """
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        all_sitemap_urls = set()

        try:
            # 1. å¾ robots.txt ç²å– Sitemaps
            sitemap_paths = self.get_sitemaps_from_robots(site_url)

            # 2. å¦‚æœ robots.txt ä¸­æ²’æœ‰ï¼Œå‰‡å¾ GSC API ç²å–
            if not sitemap_paths:
                sitemap_list = self.get_sitemaps(site_url)
                sitemap_paths = [sitemap["path"] for sitemap in sitemap_list]

            # 3. ä¸¦è¡Œè§£ææ‰€æœ‰ Sitemap æ–‡ä»¶
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future_to_url = {
                    executor.submit(self.get_sitemap_urls, path): path for path in sitemap_paths
                }
                for future in concurrent.futures.as_completed(future_to_url):
                    try:
                        urls = future.result()
                        all_sitemap_urls.update(urls)
                    except Exception as exc:
                        logger.error(f"è§£æ sitemap æ™‚å‡ºéŒ¯: {exc}")

            return list(all_sitemap_urls)

        except Exception as e:
            logger.error(f"ç„¡æ³•ç²å–ç«™é» {site_url} çš„ sitemap URL: {e}")
            return []

    def get_sitemaps_from_robots(self, site_url: str) -> List[str]:
        """å¾ robots.txt æ–‡ä»¶ä¸­è§£æå‡º sitemap çš„ URL"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        robots_url = f"{site_url.rstrip('/')}/robots.txt"
        sitemap_urls = []
        try:
            response = requests.get(robots_url, timeout=10)
            response.raise_for_status()
            for line in response.text.splitlines():
                if line.lower().startswith("sitemap:"):
                    sitemap_urls.append(line.split(":", 1)[1].strip())
        except requests.RequestException as e:
            logger.warning(f"ç„¡æ³•å¾ {robots_url} ç²å– robots.txt: {e}")
        return sitemap_urls

    def get_sitemap_urls(self, sitemap_path: str) -> List[str]:
        """
        [REFACTORED] è§£æå–®å€‹ Sitemap æ–‡ä»¶ï¼ˆæˆ–ç´¢å¼•æ–‡ä»¶ï¼‰ï¼Œè¿”å›æ‰€æœ‰ URL åˆ—è¡¨ã€‚
        æ­¤ç‰ˆæœ¬æ”¯æŒéæ­¸è§£æ Sitemap ç´¢å¼•æ–‡ä»¶ã€‚
        """
        urls = []
        try:
            response = requests.get(sitemap_path, timeout=30)
            response.raise_for_status()

            # é¿å… XML ç‚¸å½ˆ
            if len(response.content) > 10 * 1024 * 1024:  # 10MB
                logger.error(f"Sitemap file {sitemap_path} is too large, skipping.")
                return []

            root = ET.fromstring(response.content)
            namespace = {"sm": "http://www.sitemaps.org/schemas/sitemap/0.9"}

            # æª¢æŸ¥æ˜¯ Sitemap ç´¢å¼•é‚„æ˜¯ URL Set
            if root.tag.endswith("sitemapindex"):
                # å¦‚æœæ˜¯ Sitemap ç´¢å¼•ï¼Œéæ­¸èª¿ç”¨
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    child_sitemaps = [
                        loc.text for loc in root.findall("sm:sitemap/sm:loc", namespace) if loc.text
                    ]
                    future_to_url = {
                        executor.submit(self.get_sitemap_urls, child_path): child_path
                        for child_path in child_sitemaps
                    }
                    for future in concurrent.futures.as_completed(future_to_url):
                        try:
                            urls.extend(future.result())
                        except Exception as exc:
                            logger.error(f"è§£æå­ sitemap æ™‚å‡ºéŒ¯: {exc}")

            elif root.tag.endswith("urlset"):
                # å¦‚æœæ˜¯æ™®é€šçš„ URL Set
                urls.extend(
                    [loc.text for loc in root.findall("sm:url/sm:loc", namespace) if loc.text]
                )
        except requests.RequestException as e:
            logger.error(f"ç„¡æ³•ç²å– sitemap '{sitemap_path}': {e}")
        except ET.ParseError as e:
            logger.error(f"ç„¡æ³•è§£æ sitemap XML '{sitemap_path}': {e}")
        except Exception as e:
            logger.error(f"è™•ç† sitemap '{sitemap_path}' æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")

        return urls

    def compare_db_and_sitemap(self, site_url: str, site_id: int) -> Dict[str, Any]:
        """æ¯”è¼ƒè³‡æ–™åº«ä¸­çš„é é¢èˆ‡ Sitemap ä¸­çš„ URLï¼Œæ‰¾å‡ºå†—é¤˜å’Œç¼ºå¤±ã€‚"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        # 1. å¾è³‡æ–™åº«ç²å–æ‰€æœ‰ç¨ç«‹é é¢
        try:
            db_pages = set(self.database.get_distinct_pages_for_site(site_id))
            if not db_pages:
                logger.warning(f"åœ¨æ•¸æ“šåº«ä¸­æ‰¾ä¸åˆ°ç«™é» ID {site_id} çš„ä»»ä½•é é¢æ•¸æ“šã€‚")

            # 2. ç²å– sitemap ä¸­çš„æ‰€æœ‰ URL
            sitemap_urls = set(self.get_sitemap_urls_for_site(site_url))
            if not sitemap_urls:
                logger.warning(f"å¾ {site_url} çš„ sitemap ä¸­æ‰¾ä¸åˆ°ä»»ä½• URLã€‚")

            # 3. æ¯”è¼ƒå·®ç•°
            only_in_db = sorted(list(db_pages - sitemap_urls))
            only_in_sitemap = sorted(list(sitemap_urls - db_pages))
            in_both = sorted(list(db_pages & sitemap_urls))

            return {
                "site_url": site_url,
                "db_url_count": len(db_pages),
                "sitemap_url_count": len(sitemap_urls),
                "common_url_count": len(in_both),
                "redundant_urls_in_db_count": len(only_in_db),
                "missing_urls_in_sitemap_count": len(only_in_sitemap),
                "redundant_urls_in_db": only_in_db[:20],  # åƒ…é¡¯ç¤ºå‰ 20 æ¢ä»¥ä½œæ¨£æœ¬
                "missing_urls_in_sitemap": only_in_sitemap[:20],
            }
        except Exception as e:
            logger.error(f"æ¯”è¼ƒæ•¸æ“šåº«å’Œ sitemap æ™‚å‡ºéŒ¯: {e}")
            return {"error": str(e)}

    def get_search_analytics(
        self,
        site_url: str,
        start_date: str,
        end_date: str,
        dimensions: Optional[List[str]] = None,
        row_limit: int = 1000,
    ) -> List[Dict[str, Any]]:
        """ç²å–æœç´¢åˆ†ææ•¸æ“š"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        if dimensions is None:
            dimensions = ["page", "query"]

        try:
            request = {
                "startDate": start_date,
                "endDate": end_date,
                "dimensions": dimensions,
                "rowLimit": row_limit,
            }
            response = (
                self.service.searchanalytics().query(siteUrl=site_url, body=request).execute()
            )
            self._track_api_request()
            rows: List[Dict[str, Any]] = response.get("rows", [])
            return rows
        except HttpError as e:
            logger.error(f"Search analytics API error for {site_url}: {e}")
            return []
        except Exception as e:
            logger.error(f"Unexpected error in get_search_analytics: {e}")
            return []

    def get_search_analytics_batch(
        self,
        site_url: str,
        start_date: str,
        end_date: str,
        dimensions: Optional[List[str]] = None,
        device_filter: Optional[str] = None,
        max_total_rows: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """æ‰¹æ¬¡ç²å–æœç´¢åˆ†ææ•¸æ“š"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        if dimensions is None:
            dimensions = ["page"]

        all_rows: List[Dict[str, Any]] = []
        start_row = 0
        row_limit = 25000  # API æœ€å¤§å€¼

        while True:
            try:
                request = {
                    "startDate": start_date,
                    "endDate": end_date,
                    "dimensions": dimensions,
                    "startRow": start_row,
                    "rowLimit": row_limit,
                }
                if device_filter:
                    request["dimensionFilterGroups"] = [
                        {
                            "filters": [
                                {
                                    "dimension": "device",
                                    "operator": "equals",
                                    "expression": device_filter,
                                }
                            ]
                        }
                    ]

                self._rate_limit_check()
                response = (
                    self.service.searchanalytics().query(siteUrl=site_url, body=request).execute()
                )
                self._track_api_request()

                rows = response.get("rows", [])
                if not rows:
                    break  # æ²’æœ‰æ›´å¤šæ•¸æ“š

                all_rows.extend(rows)

                # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°è«‹æ±‚çš„ç¸½è¡Œæ•¸é™åˆ¶
                if max_total_rows is not None and len(all_rows) >= max_total_rows:
                    return all_rows[:max_total_rows]

                # å¦‚æœè¿”å›çš„è¡Œæ•¸å°æ–¼è«‹æ±‚çš„è¡Œæ•¸ï¼Œèªªæ˜å·²ç¶“æ˜¯æœ€å¾Œä¸€é 
                if len(rows) < row_limit:
                    break

                start_row += row_limit

            except HttpError as e:
                logger.error(f"Search analytics API error for {site_url}: {e}")
                break  # ç™¼ç”ŸéŒ¯èª¤æ™‚çµ‚æ­¢å¾ªç’°
            except Exception as e:
                logger.error(f"Unexpected error in get_search_analytics_batch: {e}")
                break

        return all_rows

    def get_keywords_for_site(self, site_url: str, limit: int = 100) -> List[str]:
        """ç²å–ç«™é»çš„ç†±é–€é—œéµå­—"""
        if not self.service:
            self.authenticate()
        if not self.service:
            raise Exception("GSC service not initialized")

        try:
            analytics_data = self.get_search_analytics(
                site_url=site_url,
                start_date=(datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d"),
                end_date=datetime.now().strftime("%Y-%m-%d"),
                dimensions=["query"],
                row_limit=limit,
            )

            # æŒ‰é»æ“Šæ•¸æ’åº
            sorted_data = sorted(analytics_data, key=lambda x: x.get("clicks", 0), reverse=True)
            return [row["keys"][0] for row in sorted_data[:limit] if "keys" in row]

        except Exception as e:
            logger.error(f"ç²å–é—œéµå­—æ™‚å‡ºéŒ¯: {e}")
            return []

    def _track_api_request(self):
        """
        è·Ÿè¹¤ API è«‹æ±‚è¨ˆæ•¸ï¼Œç”¨æ–¼é€Ÿç‡é™åˆ¶å’Œæ¯æ—¥é…é¡ã€‚
        æ­¤æ–¹æ³•éœ€è¦æ˜¯ç·šç¨‹å®‰å…¨çš„ã€‚
        """
        with self._api_lock:
            now = datetime.now()
            today = now.date()

            # æª¢æŸ¥æ˜¯å¦æ–°çš„ä¸€å¤©ï¼Œå¦‚æœæ˜¯ï¼Œé‡ç½®æ¯æ—¥è¨ˆæ•¸å™¨
            if today != self.today:
                logger.info(f"æ–°çš„ä¸€å¤© ({today})ï¼Œé‡ç½®æ¯æ—¥ API ä½¿ç”¨è¨ˆæ•¸ã€‚")
                self._save_api_usage_to_db()  # ä¿å­˜å‰ä¸€å¤©çš„è¨ˆæ•¸
                self.api_requests_today = 0
                self.today = today

            # å¢åŠ è¨ˆæ•¸
            self.api_requests_this_minute += 1
            self.api_requests_today += 1

            # æ¯ 10 æ¬¡è«‹æ±‚å¾Œï¼Œå°‡ç•¶å‰è¨ˆæ•¸ç•°æ­¥ä¿å­˜åˆ°æ•¸æ“šåº«
            if self.api_requests_today % 10 == 0:
                # ä½¿ç”¨ç·šç¨‹ä»¥é¿å…é˜»å¡ä¸»æµç¨‹
                save_thread = threading.Thread(target=self._save_api_usage_to_db)
                save_thread.start()

    def get_api_usage_stats(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰çš„ API ä½¿ç”¨çµ±è¨ˆä¿¡æ¯"""
        return {
            "today": self.today.strftime("%Y-%m-%d"),
            "requests_today": self.api_requests_today,
            "requests_this_minute": self.api_requests_this_minute,
        }

    def _load_api_usage_from_db(self):
        """å¾æ•¸æ“šåº«åŠ è¼‰ä»Šå¤©çš„ API ä½¿ç”¨è¨ˆæ•¸"""
        today_str = self.today.strftime("%Y-%m-%d")
        count = self.database.get_api_usage(today_str)
        self.api_requests_today = count
        logger.info(f"å¾æ•¸æ“šåº«åŠ è¼‰äº†ä»Šå¤©çš„ API ä½¿ç”¨æ¬¡æ•¸: {count}")

    def _save_api_usage_to_db(self):
        """å°‡ç•¶å‰çš„ API ä½¿ç”¨è¨ˆæ•¸ä¿å­˜åˆ°æ•¸æ“šåº«"""
        today_str = self.today.strftime("%Y-%m-%d")
        try:
            self.database.update_api_usage(today_str, self.api_requests_today)
        except Exception as e:
            logger.error(f"ç„¡æ³•å°‡ API ä½¿ç”¨æƒ…æ³ä¿å­˜åˆ°æ•¸æ“šåº«: {e}")
